# utils_service_availability/recommendation_service_availability/business_impact.py
import streamlit as st
import plotly.express as px
import pandas as pd
import numpy as np

# ============================
# Company visual theme
# ============================
px.defaults.template = "plotly_white"
PRIMARY_BLUE = "#004C99"
SECONDARY_BLUE = "#007ACC"

# ============================================================
# Helper for CIO tables
# ============================================================
def render_cio_tables(title, cio):
    st.subheader(title)
    with st.expander(" Cost Reduction"):
        st.markdown(cio["cost"], unsafe_allow_html=True)
    with st.expander(" Performance Improvement"):
        st.markdown(cio["performance"], unsafe_allow_html=True)
    with st.expander(" Customer Satisfaction Improvement"):
        st.markdown(cio["satisfaction"], unsafe_allow_html=True)

def _to_num(s):
    return pd.to_numeric(s, errors="coerce")

# ============================================================
# Target ‚Äì Business Impact
# ============================================================
def business_impact(df: pd.DataFrame):

    # Prepare common numeric helpers (if columns exist)
    df_num = df.copy()
    if "downtime_minutes" in df_num.columns:
        df_num["downtime_minutes"] = _to_num(df_num["downtime_minutes"])
    if "estimated_cost_downtime" in df_num.columns:
        df_num["estimated_cost_downtime"] = _to_num(df_num["estimated_cost_downtime"])

    avg_rm_per_min = np.nan
    if {"downtime_minutes", "estimated_cost_downtime"}.issubset(df_num.columns):
        total_cost_all = float(df_num["estimated_cost_downtime"].sum(skipna=True))
        total_min_all = float(df_num["downtime_minutes"].sum(skipna=True))
        if total_min_all > 0:
            avg_rm_per_min = total_cost_all / total_min_all  # RM per downtime minute (derived from your data)

    # --------------------------------------------------------
    # A. Total Estimated Downtime Cost per Service
    # --------------------------------------------------------
    with st.expander("üìå Estimated Cost of Downtime per Service"):
        need = ["service_name", "downtime_minutes", "estimated_cost_downtime"]
        if not set(need).issubset(df.columns):
            st.warning(f"‚ö†Ô∏è Missing required columns: {set(need) - set(df.columns)}")
        else:
            # include minutes so we can use them in cost calcs
            df_cost = (
                df_num.groupby("service_name", as_index=False)
                .agg({
                    "downtime_minutes": "sum",
                    "estimated_cost_downtime": "sum"
                })
                .sort_values("estimated_cost_downtime", ascending=False)
            )

            fig = px.bar(
                df_cost,
                x="service_name",
                y="estimated_cost_downtime",
                text="estimated_cost_downtime",
                title="Total Estimated Downtime Cost by Service",
                labels={"service_name": "Service", "estimated_cost_downtime": "Estimated Cost (RM)"},
                color_discrete_sequence=[PRIMARY_BLUE],
            )
            fig.update_traces(texttemplate="RM %{text:,.0f}", textposition="outside", cliponaxis=False)
            fig.update_layout(xaxis_tickangle=-15)
            st.plotly_chart(fig, use_container_width=True)

            highest = df_cost.iloc[0]
            lowest = df_cost.iloc[-1]
            total_cost = float(df_cost["estimated_cost_downtime"].sum())
            median_cost = float(df_cost["estimated_cost_downtime"].median())
            # Data-derived potential saving: normalize top service down to median
            potential_rm_if_normalised = max(0.0, float(highest["estimated_cost_downtime"] - median_cost))

            st.markdown("### Analysis ‚Äì Estimated Downtime Cost")
            st.write(
f"""**What this graph is:** A **bar chart** showing **total estimated downtime cost (RM)** by service.  
- **X-axis:** Service name.  
- **Y-axis:** Total estimated downtime cost.

**What it shows in your data:**  
- **Highest cost impact:** **{highest['service_name']}** at **RM {highest['estimated_cost_downtime']:,.0f}**.  
- **Lowest cost impact:** **{lowest['service_name']}** at **RM {lowest['estimated_cost_downtime']:,.0f}**.  
- **Total downtime cost across services:** **RM {total_cost:,.0f}**.

**How to read it operationally:**  
1) Focus mitigation on the **left-most/tallest bars**‚Äîthese drive most losses.  
2) Compare each bar to the **median** to spot outliers where fixes return the most ROI.  
3) Pair this view with **downtime minutes** to separate costly incidents from inexpensive but frequent ones.

**Why this matters:** Concentrating fixes on the **few services** that create the **majority of cost** delivers the **fastest payback** and improves overall reliability."""
            )

            # Real values used in tables
            rm_min_txt = f"Avg RM/min‚âà**RM {avg_rm_per_min:,.2f}**" if avg_rm_per_min == avg_rm_per_min else "RM/min from dataset when both minutes & cost exist"
            # Top service minutes
            top_minutes = float(highest["downtime_minutes"]) if "downtime_minutes" in df_cost.columns else np.nan
            # Top-3 sum (for ROI bucket)
            top3 = df_cost.head(3)
            top3_cost_sum = float(top3["estimated_cost_downtime"].sum()) if len(top3) >= 1 else 0.0
            top3_minutes_sum = float(top3["downtime_minutes"].sum()) if "downtime_minutes" in top3.columns else np.nan

            # CIO tables ‚Äî phased, detailed, data-backed
            cio_a = {
                "cost": f"""
| Recommendation | Explanation (Phased) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| Normalize **{highest['service_name']}** to median | **Phase 1 ‚Äì Diagnose:** Identify the top three failure modes for **{highest['service_name']}** that are driving very high downtime cost compared to other services. Explain these patterns in clear business language so both technical and non technical stakeholders can understand why this service is riskier than the rest.<br><br>**Phase 2 ‚Äì Fix and guardrail:** Implement targeted patches, configuration changes, and canary or rollback mechanisms that directly address these failure modes. Make sure each fix has clear success criteria and monitoring so the team can confirm quickly whether the risk is actually going down.<br><br>**Phase 3 ‚Äì Verify:** Compare the cost of **{highest['service_name']}** against the median service cost of **RM {median_cost:,.0f}** across at least two reporting cycles. Confirm that the bar in the chart is consistently moving closer to the median instead of bouncing back after the first round of improvements. | - Reduces the oversized cost contribution of a single service so the overall portfolio becomes more balanced and predictable.<br><br>- Frees financial capacity that can be redirected into resilience work, innovation initiatives, or other strategic priorities instead of repeat firefighting.<br><br>- Lowers the likelihood of large one off penalty events because the riskiest service is brought closer to the normal performance band of other services.<br><br> | **Savings = RM {highest['estimated_cost_downtime']:,.0f} ‚àí RM {median_cost:,.0f} = RM {potential_rm_if_normalised:,.0f}** (from chart). | Tallest bar is **{highest['service_name']}** at **RM {highest['estimated_cost_downtime']:,.0f}**; median bar ‚âà **RM {median_cost:,.0f}**. |
| Attack the **Top-3** cost services | **Phase 1 ‚Äì Form a tiger team:** Assign a small cross functional team that clearly owns the top three services by cost and understands their architecture and failure modes. Ensure they have authority to change configurations, schedule maintenance, and coordinate with vendors where required.<br><br>**Phase 2 ‚Äì Execute sprints:** Run short improvement sprints to remove single points of failure, improve restore procedures, and shift maintenance into safer windows for these services. Document every change in simple language so that results can be linked back to reduced cost and downtime.<br><br>**Phase 3 ‚Äì Reassess:** After one or two cycles, compare the heights of the top three bars in the chart with the previous period. Check whether cost is falling in a sustained way or only moving temporarily before bouncing back. | - Concentrates effort on the small number of services that create the majority of downtime cost so each fix has higher financial impact.<br><br>- Delivers visible early wins that can help secure more support and budget for longer term reliability improvements.<br><br>- Reduces the overall volatility of downtime cost so forecasting and planning become easier for finance and operations teams.<br><br> | **Target RM avoided ‚âà RM {top3_cost_sum:,.0f} ‚àí (Top-3 post-fix total)**; if minutes available: **Œîminutes √ó {rm_min_txt}** (Top-3 minutes currently **{top3_minutes_sum:,.0f}**). | Left-skewed head (top-3 bars) visually concentrate the portfolio cost. |
| Reduce **high-value minutes** first | **Phase 1 ‚Äì Map RM per minute windows:** Use the relationship between downtime minutes and cost to identify time windows and services where each minute of downtime is more expensive than the average. Translate this into a simple view so non technical stakeholders can see which minutes matter the most.<br><br>**Phase 2 ‚Äì Rebase work:** Move planned work and risky changes away from those high value windows and shorten steps that run during them. Align maintenance planning with revenue and usage patterns so disruption is pushed into lower value periods.<br><br>**Phase 3 ‚Äì Monitor:** Review high value windows weekly or monthly and check whether total minutes are actually dropping. Adjust planning rules if the expected reduction in costly minutes does not appear in the charts. | - Focuses improvement on minutes that hurt the business the most so every minute reduced generates more financial benefit.<br><br>- Helps operations teams schedule changes in a way that protects key revenue periods instead of treating all minutes as equal.<br><br>- Reduces the shock of large monthly cost spikes because the most sensitive windows are better protected and more predictable.<br><br> | **Savings = Œî(high-value minutes) √ó {rm_min_txt}**. | Cost bars + available minutes indicate non-uniform value per minute across services. |
| Failover for **{highest['service_name']}** | **Phase 1 ‚Äì Assess:** Review recent incidents and confirm whether downtime for **{highest['service_name']}** is usually a full outage with no working alternative. Map out the current architecture to identify where an additional standby or secondary path could sit.<br><br>**Phase 2 ‚Äì Implement:** Design and deploy a warm standby or active passive configuration with health checks and automated cutover. Make sure the operating team understands exactly how the failover behaves in normal and degraded states.<br><br>**Phase 3 ‚Äì Validate:** Run planned failover drills and track how much downtime and cost are avoided compared to previous incidents without failover. Confirm that both the incident charts and the cost chart show a clear reduction after the change. | - Converts long and painful outages into short and controlled interruptions so customers experience less disruption.<br><br>- Reduces the number of major incidents that require emergency coordination calls and complex manual recovery steps.<br><br>- Protects revenue and contractual commitments because key services stay available or recover much faster when something goes wrong.<br><br> | **Benefit = Œîminutes avoided √ó {rm_min_txt}** (top service minutes currently **{top_minutes:,.0f}**). | Tallest bar suggests SPOF; failover shrinks visible impact rapidly. |
| Owner KPIs tied to RM avoided | **Phase 1 ‚Äì Instrument:** Attribute downtime cost at service level and map each service to an accountable owner or squad. Present the numbers in a simple scoreboard so everyone can see where money is being lost.<br><br>**Phase 2 ‚Äì Incentivize:** Set clear goals for each owner based on RM avoided rather than just technical metrics and agree on timelines to hit these goals. Build this into performance reviews or team objectives so it stays visible.<br><br>**Phase 3 ‚Äì Review:** Run quarterly governance sessions where owners explain what actions reduced cost and what is planned next. Use these discussions to share ideas that worked well across other teams. | - Links technical reliability work directly to financial outcomes so teams understand how their actions change real money for the business.<br><br>- Keeps cost control on the agenda over time instead of being a one off improvement project that fades away.<br><br>- Encourages healthy competition and collaboration across teams to find the most effective reliability improvements that reduce cost in the chart.<br><br> | **Quarterly RM avoided = Baseline cost ‚àí Current cost** per service. | Chart bar deltas provide an auditable RM-avoided trail. |
""",
                "performance": f"""
| Recommendation | Explanation (Phased) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| Root-cause eradication for top bars | **Phase 1 ‚Äì Cluster incidents:** Group incidents for the most expensive services and identify repeated technical or process patterns behind them. Make sure the clusters are described in simple language that both engineers and business owners can understand.<br><br>**Phase 2 ‚Äì Engineer fixes:** Design and implement configuration changes, code fixes, or platform changes that remove or strongly reduce those patterns. Document clearly which root causes each change is expected to eliminate.<br><br>**Phase 3 ‚Äì Lock in improvements:** Build regression tests, monitoring checks, and change procedures that ensure these root causes do not come back silently over time. | - Reduces the number of incidents on the highest cost services so the overall stability trend improves where it matters most.<br><br>- Improves recovery quality because teams spend less time repeating the same fix and more time preventing new issues.<br><br>- Makes technical debt more visible and easier to justify since each eradicated root cause is tied directly to cost reduction in the chart.<br><br> | **ŒîIncidents √ó ŒîDuration √ó {rm_min_txt}** within top-cost services. | Cost head implies repeatable technical drivers. |
| Golden-path recoveries | **Phase 1 ‚Äì Standardize:** For each top cost service, document a simple and repeatable recovery path that covers detection, triage, fix, verification, and rollback. Ensure the runbook is written in clear steps that any on call engineer can follow.<br><br>**Phase 2 ‚Äì Automate:** Convert the most repetitive runbook steps into scripts or tool driven commands so human error is removed and recovery is faster. Keep manual steps only where human judgment is truly required.<br><br>**Phase 3 ‚Äì Drill:** Run periodic game day exercises that force teams to use the runbooks under time pressure and adjust them based on what actually happens. | - Reduces average recovery time for incidents on the most important services because there is less confusion and fewer ad hoc decisions.<br><br>- Improves consistency of outcomes across shifts and teams so customers get a similar level of service even when different people are on call.<br><br>- Reduces the cognitive load on engineers during stressful incidents because the path to recovery is already defined and automated where possible.<br><br> | **ŒîMTTR √ó incidents √ó {rm_min_txt}** for each top service. | High bars often correlate with long, inconsistent restores. |
| Cost-aware alerting and routing | **Phase 1 ‚Äì Weight alerts:** Adjust monitoring to tag alerts from high cost services or components with higher priority. Make sure dashboards clearly show which alerts are tied to the most expensive services from the chart.<br><br>**Phase 2 ‚Äì Route to SMEs:** Configure queues so that alerts from these high impact services are routed directly to the right subject matter experts instead of general queues. Define clear ownership rules so there is no confusion.<br><br>**Phase 3 ‚Äì Audit:** Regularly review handoffs and response times to see whether high priority alerts are being handled quickly and with minimal bouncing between teams. | - Ensures that attention and effort are directed first to issues that have the biggest financial and business impact.<br><br>- Reduces wasted time from misrouted or ignored alerts so engineers spend more of their time fixing the right problems.<br><br>- Improves reliability of critical services, which in turn reduces the height of the most expensive bars in the chart over time.<br><br> | **Minutes saved/incident √ó incidents √ó {rm_min_txt}**. | Bars identify where speed yields outsized impact. |
| Pre-change risk gates on top bars | **Phase 1 ‚Äì Gate high-risk changes:** Require additional checks, approvals, or test coverage for changes touching the highest cost services. Make sure risk criteria are documented so everyone understands why some changes need more scrutiny.<br><br>**Phase 2 ‚Äì Freeze risky windows:** Avoid making disruptive changes to these services during the busiest business periods identified from usage data. Communicate these freeze windows widely so teams can plan ahead.<br><br>**Phase 3 ‚Äì Review:** After every major release for these services, review whether any incidents or cost spikes occurred and adjust gating rules based on what you learned. | - Reduces the chance that a change on a critical service will trigger a major incident with high cost and user impact.<br><br>- Creates clearer expectations for when teams can safely perform work on sensitive services, which improves planning discipline.<br><br>- Aligns technical risk handling with business risk, so the most important services receive the strongest pre change protection.<br><br> | **Major incidents avoided √ó average cost (from chart)**. | The tallest bars represent services least tolerant to risky changes. |
| Capacity guardrails | **Phase 1 ‚Äì Detect overload:** Use metrics such as CPU, memory, throughput, and queue depth to identify when top cost services are close to saturation. Translate these signals into simple thresholds for operations teams and business stakeholders.<br><br>**Phase 2 ‚Äì Throttle or scale:** When thresholds are reached, either add capacity where possible or temporarily throttle non critical traffic to keep critical journeys running. Document the exact triggers and actions so they can be applied consistently.<br><br>**Phase 3 ‚Äì Tune controls:** Refine thresholds and automated responses based on real incidents and feedback so the guardrails remain effective without being too sensitive. | - Prevents cascading failures that turn small load issues into large, costly outages on the most valuable services.<br><br>- Keeps critical user journeys available even under stress, which directly protects revenue and key SLAs.<br><br>- Provides a clear safety mechanism for both engineering and business teams so they know what happens when load gets too high.<br><br> | **Overload minutes avoided √ó {rm_min_txt}**. | Cost surges often align with peak usage windows. |
""",
                "satisfaction": f"""
| Recommendation | Explanation (Phased) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| Reliability roadmap (customer-facing) | **Phase 1 ‚Äì Publish:** Translate the chart of top cost services into a simple reliability roadmap that names the services, the issues being addressed, and the planned milestones. Use plain language so customers and non technical stakeholders can follow the story.<br><br>**Phase 2 ‚Äì Update:** Provide short monthly updates showing which bars are coming down and which actions were completed. Keep the format consistent so changes in the chart are easy to compare over time.<br><br>**Phase 3 ‚Äì Close:** When a major improvement target is achieved, clearly show the before and after state and explain what has changed for customers. | - Gives customers evidence that reliability problems are being addressed in a structured way instead of being handled reactively.<br><br>- Reduces ad hoc questions and escalations because stakeholders can see progress and future plans in one place.<br><br>- Strengthens the relationship with key customers because they feel informed and involved in how reliability is improving over time.<br><br> | **Ticket deflection √ó handling cost**; **Retention uplift** post-improvements. | Chart bar movements map directly to items in the roadmap. |
| Proactive comms for top services | **Phase 1 ‚Äì Alerts:** During incidents on high cost services, publish timely status updates that include impact, current actions, and expected next update time. Use channels that customers already rely on such as email or a status page.<br><br>**Phase 2 ‚Äì Workarounds:** Share simple temporary steps that users can take to keep working while the service is degraded. Make sure these instructions are easy to follow and tested with support teams.<br><br>**Phase 3 ‚Äì Debrief:** After resolution, send a short summary of what happened, what was fixed, and what will be done to prevent similar events. | - Reduces frustration because users know that the issue is acknowledged and that someone is actively working on it.<br><br>- Cuts down on duplicate tickets and calls because many users can get answers directly from the proactive updates.<br><br>- Improves long term trust in the service provider, even when issues occur, because communication feels structured and transparent.<br><br> | **Complaints avoided √ó cost/complaint**. | Users feel outages most on highest-cost services (leftmost bars). |
| Business-impact labels on status page | **Phase 1 ‚Äì Tag services:** Define simple business impact labels for services on the status page such as Critical, High, Medium, and Low. Map each service in the chart to one of these labels and explain the meaning in user friendly terms.<br><br>**Phase 2 ‚Äì Calibrate cadence:** Use stricter and more frequent update cadences for services marked as Critical or High, and lighter cadences for lower impact services. Make sure these rules are written down and shared internally.<br><br>**Phase 3 ‚Äì Review:** Periodically review whether the labels and update cadences still reflect real business impact and adjust them if usage or dependency patterns change. | - Helps users instantly understand how serious an incident or maintenance event is in terms of business impact.<br><br>- Reduces the need for stakeholders to ask follow up questions because the status page already encodes how important the affected service is.<br><br>- Ensures communication effort is focused on the services that matter most, which supports better customer satisfaction at lower cost.<br><br> | **Repeat contacts reduced √ó handling cost**. | The cost ranking justifies tiered comms intensity. |
| VIP notification track | **Phase 1 ‚Äì Identify:** Link key accounts and VIP users to the services that appear at the top of the cost chart. Maintain an up to date list of contacts and preferred communication channels for these users.<br><br>**Phase 2 ‚Äì Notify:** When one of these services has a major incident, send tailored updates to VIP users with an emphasis on business impact, mitigation steps, and specific commitments. Coordinate closely with account managers so messaging is consistent.<br><br>**Phase 3 ‚Äì Validate:** After major events, review feedback and satisfaction scores from VIP customers to confirm that the communication met their expectations and adjust the approach if needed. | - Reduces the risk that high value customers feel ignored or surprised during serious incidents on critical services.<br><br>- Supports account managers in protecting renewals and upsell opportunities because they can show that the situation was handled professionally.<br><br>- Helps preserve long term revenue by maintaining trust even when the environment is imperfect and incidents still happen.<br><br> | **Churn avoided √ó ACV**. | Highest bars likely map to VIP-relevant journeys. |
| Quarterly ‚ÄúRM avoided‚Äù wins | **Phase 1 ‚Äì Compute:** Compare the current chart of downtime cost per service to a baseline period and calculate RM avoided for each bar that has gone down. Summarize this in a simple table that business stakeholders can understand at a glance.<br><br>**Phase 2 ‚Äì Share:** Present these results to customers and internal leaders as part of a quarterly reliability or service review, highlighting the biggest improvements and the work that enabled them.<br><br>**Phase 3 ‚Äì Plan:** Use the same view to agree on the next set of high impact reliability initiatives and to show where future bar reductions are expected to come from. | - Turns reliability work into a clear financial story that is easy to communicate to executives and customers.<br><br>- Reinforces the value of ongoing investment in resilience because the benefits are expressed in actual RM instead of only technical metrics.<br><br>- Creates a feedback loop where improvements lead to more support for further reliability work, which then drives additional cost reductions in the chart.<br><br> | **RM avoided = Baseline ‚àí Current** (chart deltas). | Falling bar heights provide visual evidence. |
"""
            }
            render_cio_tables("CIO ‚Äì Estimated Downtime Cost", cio_a)

    # --------------------------------------------------------
    # B. Business Impact Category Analysis
    # --------------------------------------------------------
    with st.expander("üìå Business Impact Level Distribution"):
        need = ["service_name", "business_impact"]
        if not set(need).issubset(df.columns):
            st.warning(f"‚ö†Ô∏è Missing required columns: {set(need) - set(df.columns)}")
        else:
            impact_count = df.copy().groupby("business_impact").size().reset_index(name="count")
            impact_count = impact_count.sort_values("count", ascending=False)

            fig2 = px.pie(
                impact_count,
                names="business_impact",
                values="count",
                title="Distribution of Business Impact Levels",
                hole=0.4,
                color_discrete_sequence=[PRIMARY_BLUE, SECONDARY_BLUE, "#A8C7F0", "#6FA8DC", "#B4C7E7"],
            )
            st.plotly_chart(fig2, use_container_width=True)

            high_impact = impact_count.iloc[0]
            # Optional cost per impact group
            has_cost = "estimated_cost_downtime" in df_num.columns
            impact_cost = None
            if has_cost:
                impact_cost = (
                    df_num.groupby("business_impact", as_index=False)
                    .agg(avg_cost=("estimated_cost_downtime", "mean"),
                         total_cost=("estimated_cost_downtime", "sum"),
                         count=("estimated_cost_downtime", "size"))
                    .sort_values("total_cost", ascending=False)
                )

            st.markdown("### Analysis ‚Äì Business Impact Levels")
            if impact_cost is not None:
                top_row = impact_cost.iloc[0]
                st.write(
f"""**What this graph is:** A **donut chart** showing **how many records fall into each business impact level**.  
- **Slice (category):** Impact level (e.g., Critical, High, Medium‚Ä¶).  
- **Size:** Number of records in that level.

**What it shows in your data:**  
- **Most frequent impact level:** **{high_impact['business_impact']}** (**{high_impact['count']}** records).  
- **Highest total cost impact level (from data):** **{top_row['business_impact'] if 'business_impact' in top_row else 'N/A'}** at **RM {top_row['total_cost']:,.0f}** total (**{int(top_row['count'])}** records; **Avg RM/case ‚âà RM {top_row['avg_cost']:,.0f}**).

**How to read it operationally:**  
1) **Larger slices** mean more events/users are affected at that impact.  
2) Combine this with **cost per level** to target where impact and money intersect.  
3) If ‚ÄúCritical/High‚Äù dominates, enforce **faster response and stronger prevention** there first.

**Why this matters:** Concentrating reliability work where **business impact is highest** reduces **risk and cost** while improving **customer confidence**."""
                )
            else:
                st.write(
f"""**What this graph is:** A **donut chart** showing **how many records fall into each business impact level**.  
- **Slice (category):** Impact level (e.g., Critical, High, Medium‚Ä¶).  
- **Size:** Number of records in that level.

**What it shows in your data:**  
- **Most frequent impact level:** **{high_impact['business_impact']}** (**{high_impact['count']}** records).

**How to read it operationally:**  
1) **Larger slices** indicate where incidents could be most disruptive to operations.  
2) Prioritize **prevention and faster recovery** on the dominant levels.  
3) Tie staffing and escalation to impact tiers to protect the business first.

**Why this matters:** Aligning effort with **business-critical areas** protects revenue and reputation."""
                )

            # Build CIO with data-derived numbers where available
            if impact_cost is not None and "business_impact" in impact_cost.columns:
                top_imp = impact_cost.iloc[0]
                second_imp = impact_cost.iloc[1] if len(impact_cost) > 1 else None
                avg_top = float(top_imp["avg_cost"])
                total_top = float(top_imp["total_cost"])
                count_top = int(top_imp["count"])
                gap_to_next = (avg_top - float(second_imp["avg_cost"])) if second_imp is not None else 0.0
                
                cio_b = {
                    "cost": f"""
| Recommendation | Explanation (Phased) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| Reduce exposure in **{top_imp['business_impact']}** tier | **Phase 1 ‚Äì Controls:** Introduce stricter change gates, validation steps, and approval flows for incidents and changes that could land in the **{top_imp['business_impact']}** impact tier. Explain the rationale in business terms so teams understand why this tier is treated differently.<br><br>**Phase 2 ‚Äì Recovery:** Design faster recovery paths and pre approved actions that can be used when a case in this tier happens so teams can move quickly without waiting for long decision cycles.<br><br>**Phase 3 ‚Äì Track:** Monitor the total RM in this tier every month and check whether the bar is trending down after the new controls are introduced. | - Directly targets the impact level that currently creates the largest financial risk so improvements here generate strong returns.<br><br>- Reduces the number of large and painful incidents that surprise leadership and customers because more control is applied up front.<br><br>- Improves predictability of losses, which helps both risk management and budgeting disciplines in the organisation.<br><br> | **Target RM avoided = RM {total_top:,.0f} ‚àí New total after controls**; per-case benefit ‚âà **Œîminutes √ó Avg RM/min** if minutes available. | Impact-level totals show **{top_imp['business_impact']}** is the cost leader (**RM {total_top:,.0f}**, **{count_top}** cases, **Avg‚âàRM {avg_top:,.0f}/case**). |
| Shift cases down one tier | **Phase 1 ‚Äì Criteria:** Define clear operational and business criteria that distinguish cases in **{top_imp['business_impact']}** from cases in the next lower tier. Make sure these criteria are documented in language that engineers and business owners can both use.<br><br>**Phase 2 ‚Äì Playbooks:** Build mitigation playbooks that can keep incidents within the lower tier by acting early with partial workarounds, throttling, or quick configuration changes. Train teams on these playbooks.<br><br>**Phase 3 ‚Äì Monitor:** Track how many cases are prevented from escalating into the top tier and review any that still escalate to refine the playbooks. | - Reduces the average cost per case because fewer events reach the most damaging impact level.<br><br>- Helps teams think in terms of early containment and mitigation instead of waiting until an incident is already critical.<br><br>- Improves user perception because fewer events reach the level of disruption that is most visible and most painful.<br><br> | **Savings ‚âà #shifted √ó (Avg cost {top_imp['business_impact']} ‚àí Avg cost next tier {('‚âà RM ' + format(float(second_imp['avg_cost']), ',.0f')) if second_imp is not None else 'N/A'})**. | Tier cost gap (donut + totals) evidences economic benefit of de-escalation. |
| Pre-approved rapid fixes for top tier | **Phase 1 ‚Äì Catalogue:** Identify rapid fixes and mitigation actions that are known to be safe and effective for typical cases in the **{top_imp['business_impact']}** tier. Document exactly when each can be used and what the side effects are.<br><br>**Phase 2 ‚Äì Approve:** Obtain pre approval from leadership and risk teams so that these fixes can be executed immediately during real incidents without waiting for extra sign off.<br><br>**Phase 3 ‚Äì Drill:** Run practice scenarios where teams apply these rapid fixes under time pressure and update the catalog to reflect what works best. | - Shortens the time between detection and action for the most serious business impact cases, which directly cuts downtime and cost.<br><br>- Reduces confusion and disagreement during high stress events because teams know which actions are already approved and safe to execute.<br><br>- Increases confidence from customers and stakeholders because they see faster and more consistent responses when major issues occur.<br><br> | **RM saved = Œîminutes at top tier √ó Avg RM/min** (use dataset RM/min). | Top-tier total **RM {total_top:,.0f}** validates urgency of speed at this tier. |
| Impact-tier runbooks | **Phase 1 ‚Äì Tailor:** Create clear runbooks that are specific to each business impact tier, explaining how response, communication, and escalation differ between Critical, High, Medium, and Low. Use examples from past incidents to illustrate the differences.<br><br>**Phase 2 ‚Äì Enforce:** Integrate these runbooks into incident tooling and require responders to select the correct tier and follow the associated actions during live incidents.<br><br>**Phase 3 ‚Äì Refresh:** Review the runbooks every quarter based on actual incident outcomes and adjust instructions where they did not work well. | - Ensures that response actions are aligned with actual business risk so high impact incidents receive stronger and faster treatment than low impact ones.<br><br>- Reduces inconsistency across teams because everyone uses the same tier specific playbook instead of ad hoc decisions.<br><br>- Helps non technical stakeholders understand what to expect when an incident is classified in each impact level.<br><br> | **ŒîMTTR √ó incidents in tier √ó Avg RM/min**. | Tier concentration justifies bespoke playbooks. |
| Quarterly re-segmentation by loss data | **Phase 1 ‚Äì Recompute:** Use the last quarter of incident and loss data to reassess which services and cases belong in each business impact tier. Update the definitions where the reality no longer matches the current labels.<br><br>**Phase 2 ‚Äì Retarget:** Adjust controls, staffing, and escalation paths so that they line up with the new tier boundaries and current loss patterns. Communicate the changes so teams know what has moved.<br><br>**Phase 3 ‚Äì Publish:** Share a simple impact and loss scorecard with leadership that shows the updated tier distribution and the main changes from the previous quarter. | - Keeps impact tiers grounded in real financial data instead of static assumptions that may become outdated.<br><br>- Ensures that control effort and response capacity are always focused on the parts of the estate that are currently generating the most loss.<br><br>- Provides a clear narrative for leadership about how risk is changing over time and how the organisation is adapting.<br><br> | **RM avoided from re-targeting = Old plan loss ‚àí New plan loss**. | Tier totals provide empirical basis for re-segmenting. |
""",
                    "performance": f"""
| Recommendation | Explanation (Phased) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| Tiered SLAs | **Phase 1 ‚Äì Define:** Set recovery time targets and response expectations that are stricter for Critical and High impact tiers than for lower tiers. Make sure the definitions are documented and easy to read for both technical and business teams.<br><br>**Phase 2 ‚Äì Staff:** Align on call coverage, skills, and escalation paths so that higher tiers can realistically meet the tougher targets. Validate that key skills are available when needed.<br><br>**Phase 3 ‚Äì Measure:** Track SLA performance by tier and report which tiers meet or miss their targets, then adjust actions where gaps persist. | - Ensures that the most important business services get faster and more consistent attention when things go wrong.<br><br>- Makes performance management clearer because SLA results can be viewed by impact level instead of as a single blended number.<br><br>- Supports better prioritisation of engineering work because teams can see where SLA breaches create the most risk to the business.<br><br> | **Breach reduction √ó Avg minutes over √ó Avg RM/min**. | Tier distribution indicates where speed matters most. |
| Priority routing by tier | **Phase 1 ‚Äì Map:** Configure routing rules so that incidents in higher impact tiers are sent directly to more experienced engineers or specialised teams. Make sure fall back routing is defined for off hours or absences.<br><br>**Phase 2 ‚Äì Automate:** Implement these rules in ticketing, paging, or chat tools so priority routing happens automatically instead of relying on manual triage.<br><br>**Phase 3 ‚Äì Audit:** Periodically review tickets and incidents to confirm that high impact cases are not getting stuck in generic queues or bounced between teams. | - Reduces handling delays for the incidents that can damage the business the most.<br><br>- Decreases the number of handoffs and ownership confusion for critical cases, which helps reduce recovery time.<br><br>- Improves confidence among stakeholders because the most serious issues are handled by the people best equipped to fix them.<br><br> | **ŒîMTTR √ó top-tier cases √ó Avg RM/min**. | High-impact tiers dominate user-visible risk. |
| Pre-fault detection on top tier | **Phase 1 ‚Äì Signals:** Identify key metrics, logs, and business indicators that often change shortly before high impact incidents occur. Document these as pre fault signals for the affected tiers.<br><br>**Phase 2 ‚Äì Act:** Build automation or runbooks that trigger early mitigations when these signals cross thresholds, such as throttling, traffic shifting, or quick configuration adjustments.<br><br>**Phase 3 ‚Äì Tune:** Adjust thresholds and actions over time so that pre fault detection stays useful without creating unnecessary noise or false alarms. | - Turns some high impact incidents into smaller and shorter events by catching them earlier in their lifecycle.<br><br>- Allows teams to be more proactive and data driven instead of reacting only after customers are already affected.<br><br>- Improves overall stability in the most sensitive areas of the business because early intervention becomes part of normal operations.<br><br> | **Incidents avoided √ó Avg cost per top tier case (‚âà RM {avg_top:,.0f})**. | Cost totals by tier show where interception pays off. |
| Cross-functional war rooms | **Phase 1 ‚Äì Mobilize:** Define a clear set of roles and participants from different teams that must join a war room when a high tier incident is declared. Include engineering, operations, support, and business stakeholders where appropriate.<br><br>**Phase 2 ‚Äì Templates:** Prepare decision trees, standard agendas, and communication templates so war rooms can run in a consistent and efficient way. Make sure responsibilities and reporting lines are explicit.<br><br>**Phase 3 ‚Äì Review:** After each war room, run a short review focused on delays, communication gaps, and unclear decisions, then adjust the template to improve next time. | - Reduces wasted time during high impact incidents because decisions are made in one place with the right people present.<br><br>- Improves quality of communication inside and outside the technical team during stressful events.<br><br>- Builds shared understanding between technical and business stakeholders about what it takes to handle serious incidents effectively.<br><br> | **Coordination minutes saved √ó case count √ó Avg RM/min**. | Critical tier cases typically span multiple teams. |
| Impact drills | **Phase 1 ‚Äì Simulate:** Design realistic simulations of high tier incidents that mirror real architecture and business impact. Run these drills with the actual people and tools used in production incidents.<br><br>**Phase 2 ‚Äì Measure:** Track key metrics during drills such as time to detect, time to assemble the team, time to mitigate, and time to fully recover. Compare these metrics against your desired targets.<br><br>**Phase 3 ‚Äì Fix:** Use drill results to update runbooks, tooling, staffing plans, and communication patterns so that real incidents are handled better over time. | - Builds confidence and muscle memory in the team so real high impact incidents feel more manageable and controlled.<br><br>- Exposes hidden weaknesses in processes, tooling, or communication before real customers are affected.<br><br>- Provides concrete data for leadership on how prepared the organisation actually is for serious events.<br><br> | **ŒîBreach rate √ó penalty** or **ŒîMTTR √ó cases √ó Avg RM/min**. | Tier frequency and totals warrant rehearsal. |
""",
                    "satisfaction": f"""
| Recommendation | Explanation (Phased) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| Impact-based comms cadence | **Phase 1 ‚Äì Cadence:** Define how often updates must be sent for each impact tier, with more frequent updates for Critical and High and less frequent updates for lower tiers. Write down these rules so they are easy to apply during an incident.<br><br>**Phase 2 ‚Äì Content:** Provide guidance on what each update should contain for each tier, such as business impact, technical details, workarounds, and next update time. Train incident commanders to follow this guidance.<br><br>**Phase 3 ‚Äì Debrief:** After incidents, review whether the planned cadence and content met user expectations and adjust the rules where they were not sufficient. | - Keeps customers and internal stakeholders informed at a level that matches the seriousness of the incident.<br><br>- Reduces inbound noise from people repeatedly asking for status because they know when to expect the next update.<br><br>- Improves perceived professionalism and care, especially for the most severe incidents that attract executive attention.<br><br> | **Ticket deflection √ó handling cost**; **Complaint rate decrease** across high tiers. | High-impact tiers correlate with user-visible pain. |
| Workaround catalog for top tier | **Phase 1 ‚Äì Document:** Create clear step by step workaround guides for the most critical business journeys that are affected when top tier incidents occur. Use plain language and screenshots where helpful.<br><br>**Phase 2 ‚Äì Surface:** Publish these guides in portals, chatbots, and support tools so they are easy to find during real incidents. Make sure support teams are trained to use them with customers.<br><br>**Phase 3 ‚Äì Track:** Measure how often workarounds are used and how much they reduce support contacts and downtime for users. | - Allows customers to continue doing at least part of their work while a major technical fix is underway.<br><br>- Eases load on support teams because they can quickly share a ready made guide instead of inventing solutions on the spot.<br><br>- Demonstrates that the provider understands real user workflows and has prepared practical alternatives for difficult times.<br><br> | **Visible minutes avoided √ó value/min**. | Tier totals identify where workarounds matter most. |
| Executive brief on top tier | **Phase 1 ‚Äì Notify:** When a top tier incident opens, send a simple summary to executives and account owners covering impact, scope, and immediate actions. Keep the language focused on business outcomes rather than deep technical detail.<br><br>**Phase 2 ‚Äì Align:** Use short, focused calls or messages to agree on any trade offs such as temporary feature disables or customer promises so technical and business teams move together.<br><br>**Phase 3 ‚Äì Close:** After resolution, provide a short follow up that outlines what was fixed, what will change to prevent recurrence, and what customers will see. | - Reduces confusion and rumour at senior levels because leaders receive a clear and timely view of the situation.<br><br>- Speeds up decisions during critical events because business and technical leaders are aligned on options and priorities.<br><br>- Supports better external messaging because sales and account teams receive consistent information to share with customers.<br><br> | **Escalation load avoided**; **Retention uplift** on key accounts. | Cost concentration amplifies exec visibility needs. |
| Customer review after major cases | **Phase 1 ‚Äì Share:** For selected major high tier incidents, hold a short review with affected customers where you explain the cause, the fix, and the prevention plan in plain language. Provide supportive visual summaries if helpful.<br><br>**Phase 2 ‚Äì Commit:** Capture agreed follow up actions, dates, and owners, and send a written summary so customers can see the commitments clearly.<br><br>**Phase 3 ‚Äì Measure:** Track whether customer satisfaction and renewal signals improve after these reviews compared to similar incidents without structured follow up. | - Shows customers that the provider takes high impact incidents seriously and is willing to be transparent about what went wrong.<br><br>- Helps reset trust after difficult events because there is a clear plan and visible accountability.<br><br>- Gives the provider concrete feedback that can be used to improve both technology and process in a targeted way.<br><br> | **Retention uplift**; **Complaint recurrence decrease** after follow ups. | High-tier events are reputation moments. |
| Quarterly impact scorecard | **Phase 1 ‚Äì Publish:** Build a simple quarterly scorecard that shows impact tiers, total cost per tier, and the main actions taken to reduce risk. Share this with customers and internal stakeholders.<br><br>**Phase 2 ‚Äì Compare:** Show how the scorecard has changed compared to previous quarters in terms of tier distribution and total cost so people can see progress or new risks.<br><br>**Phase 3 ‚Äì Iterate:** Use the scorecard as a basis to select the next set of initiatives, focusing on tiers or services where impact remains high. | - Provides everyone a clear, repeatable view of how risk and business impact are evolving over time.<br><br>- Supports more structured planning, because priorities can be tied directly to tier and cost data instead of only opinions.<br><br>- Increases transparency and can reduce anxiety for stakeholders who want to know whether things are improving or worsening.<br><br> | **Ticket deflection √ó handling cost**; **Complaint recurrence decrease** across reporting periods. | Donut view plus totals roll up cleanly for execs. |
"""
                }

            else:
                cio_b = {
                    "cost": """
| Recommendation | Explanation (Phased) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| Focus on dominant impact tier | **Phase 1 ‚Äì Identify:** Use the donut chart to find the impact level with the largest slice and describe in simple terms what business areas it represents. Make sure everyone agrees that this is currently the most important tier to address.<br><br>**Phase 2 ‚Äì Control:** Introduce stronger process and technical controls for this tier, such as stricter approvals, better monitoring, and clearer recovery paths. Explain these controls to teams in practical language.<br><br>**Phase 3 ‚Äì Track:** Monitor how many records fall into this tier over time and check whether the slice is shrinking after changes are applied. | - Concentrates effort on the impact tier that currently affects the largest number of cases so improvements here help the widest audience.<br><br>- Reduces the likelihood that many events will reach a disruptive level of impact because stronger controls are applied where they are most needed.<br><br>- Creates a simple and visual way to explain risk focus to non technical stakeholders using the donut chart slices.<br><br> | **RM avoided = Records √ó Minutes √ó RM/min** (use dataset values). | Largest slice visually dominates impact exposure. |
| Prevent escalation between tiers | **Phase 1 ‚Äì Criteria:** Define clear rules that describe when a case should move from a lower impact tier to a higher one so teams can recognise escalation early. Write the rules in language that can be applied quickly during an incident.<br><br>**Phase 2 ‚Äì Actions:** Provide early mitigation actions and temporary workarounds that can be used to keep cases from moving up to a more severe tier. Train support and operations teams on how to use these actions.<br><br>**Phase 3 ‚Äì Review:** Regularly review cases that escalated between tiers and update criteria or actions where they failed to prevent escalation. | - Reduces the number of incidents that grow into very disruptive events because more issues are contained at lower impact levels.<br><br>- Helps teams act earlier and more consistently when they see risk increasing instead of waiting until the problem is already severe.<br><br>- Improves customer experience because fewer events will reach the highest disruption categories shown in the chart.<br><br> | **ŒîCount √ó (Avg cost higher ‚àí Avg cost lower)**. | Distribution implies escalation risk. |
| Pre-approved fixes for top tier | **Phase 1 ‚Äì Catalogue:** Identify a short list of safe and proven actions that can be used to quickly stabilise incidents in the highest impact tier. Describe exactly when they apply and what side effects they may have.<br><br>**Phase 2 ‚Äì Approve:** Gain agreement from leadership and risk stakeholders that these actions can be executed immediately when needed without additional approval steps.<br><br>**Phase 3 ‚Äì Drill:** Practise applying these fixes in simulations so teams become comfortable and confident using them during real high stress situations. | - Shortens the time to mitigation for the most serious incidents because teams can act immediately with pre agreed fixes.<br><br>- Reduces confusion during crises because responders already know which actions are trusted and allowed.<br><br>- Gives customers a better experience during high impact events because stability is restored more quickly and predictably.<br><br> | **ŒîMinutes √ó RM/min** using dataset rates. | Big slice = big target for speed. |
| Impact-based maintenance windows | **Phase 1 ‚Äì Map:** Review which services and business processes fall into higher impact tiers and identify their critical business hours. Document these windows in a way that planners can easily use.<br><br>**Phase 2 ‚Äì Rebase:** Schedule disruptive maintenance for these high impact areas outside of their critical windows wherever possible. Communicate clearly when exceptions are needed and why they are justified.<br><br>**Phase 3 ‚Äì Measure:** Compare incident counts and user feedback before and after changing maintenance windows to confirm that impact has been reduced. | - Reduces the visible effect of downtime because work is moved away from times when users depend most on the service.<br><br>- Lowers stress on support teams and operations during business hours because fewer planned activities clash with peak demand.<br><br>- Supports better relationships with business teams who can see that their critical times are being respected in planning.<br><br> | **ŒîPeak minutes √ó RM/min**. | Tiering informs scheduling choices. |
| Quarterly re-tiering | **Phase 1 ‚Äì Recompute:** Review incident and business data each quarter to see whether the current tier definitions still reflect real impact. Update the mapping between services, processes, and tiers if needed.<br><br>**Phase 2 ‚Äì Retarget:** Adjust control strength, staffing focus, and response rules so they align with the updated tiers. Make sure documentation and training materials are revised accordingly.<br><br>**Phase 3 ‚Äì Publish:** Share a refreshed view of tier distribution and main changes with stakeholders so they understand how focus areas have shifted. | - Keeps the impact model aligned with reality as the business, architecture, and usage patterns change over time.<br><br>- Helps avoid wasting effort on areas that are no longer the biggest sources of risk while under investing in newly important areas.<br><br>- Makes it easier to explain changing priorities using a consistent visual and tier based story.<br><br> | **RM avoided vs prior tiering**. | Distribution evolves over time. |
""",
                    "performance": """
| Recommendation | Explanation (Phased) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| Tiered SLAs and routing | **Phase 1 ‚Äì Define:** Set response and recovery targets that become stricter as the impact tier increases. Ensure these targets are written in clear terms that can be used in operations and support playbooks.<br><br>**Phase 2 ‚Äì Route:** Configure routing rules so that high tier incidents go directly to teams or individuals with the right skills rather than general queues. Document fall back paths for off hours.<br><br>**Phase 3 ‚Äì Audit:** Review whether actual routing and SLA performance align with the plan and adjust rules if high tier cases are still being delayed. | - Ensures that the most critical incidents receive the fastest and most skilled attention instead of being handled the same as minor issues.<br><br>- Improves reliability of important services because their incidents are less likely to be stuck or misrouted.<br><br>- Creates clearer expectations for teams and stakeholders about how quickly different types of incidents should be handled.<br><br> | **SLA uplift √ó incidents**. | Higher tiers demand priority. |
| Early warning monitors | **Phase 1 ‚Äì Add:** Identify metrics and business signals that move before impact level rises, such as error rates, slowdowns, or partial feature failures, and add them as early warnings for higher tiers.<br><br>**Phase 2 ‚Äì Tune:** Adjust thresholds so early warnings are sensitive enough to detect problems but not so noisy that teams start to ignore them.<br><br>**Phase 3 ‚Äì Review:** Compare early warning behaviour with actual incidents and refine rules when there are misses or false positives. | - Enables teams to act before an issue becomes a full blown high tier incident, which protects users from severe disruption.<br><br>- Reduces reliance on user complaints as the main trigger for response because systems can detect trouble earlier.<br><br>- Encourages a proactive culture where teams respond to leading indicators instead of waiting for outages.<br><br> | **Incidents avoided √ó Avg minutes**. | Tiers show where to watch closely. |
| Cross-team drills | **Phase 1 ‚Äì Simulate:** Run drills that mimic realistic high tier incidents and involve all relevant teams such as engineering, operations, support, and business stakeholders. Use real tools and processes where possible.<br><br>**Phase 2 ‚Äì Measure:** Track how long it takes to detect, assemble the team, make key decisions, and restore service during drills and compare to target values.<br><br>**Phase 3 ‚Äì Fix:** Use lessons from drills to update runbooks, escalation paths, and communication methods so the next real incident runs more smoothly. | - Improves coordination across teams during real incidents because roles and expectations have been practised.<br><br>- Exposes hidden process weaknesses before customers are affected, giving time to fix them safely.<br><br>- Builds confidence in the organisation‚Äôs ability to handle serious situations, which helps both staff and stakeholders stay calmer during real events.<br><br> | **ŒîMTTR √ó cases**. | Tiers with large slices justify rehearsal. |
| Playbooks by tier | **Phase 1 ‚Äì Build:** Create playbooks that explain how to handle incidents differently at each impact level, including who to inform, what to prioritise, and how to balance speed and safety. Use concrete examples from past events.<br><br>**Phase 2 ‚Äì Enforce:** Integrate these playbooks into incident management tools so responders are prompted to follow them when they set the impact tier.<br><br>**Phase 3 ‚Äì Update:** Review playbooks regularly and incorporate feedback from people who used them during real incidents. | - Ensures that responses are proportional to the impact level instead of being inconsistent across similar events.<br><br>- Reduces decision fatigue for responders because they have a clear starting point for what to do at each tier.<br><br>- Provides an easy training tool for new staff who need to learn how the organisation handles different levels of impact.<br><br> | **Defects avoided √ó minutes**. | Tiering leads naturally to bespoke steps. |
| Capacity guardrails | **Phase 1 ‚Äì Detect:** Identify capacity indicators, such as saturation or backlog, that are linked to high impact incidents and track them closely for services associated with higher tiers.<br><br>**Phase 2 ‚Äì Throttle or scale:** When guardrail thresholds are breached, automatically reduce non critical usage or scale resources to protect core functions.<br><br>**Phase 3 ‚Äì Remove:** Return systems to normal once stability is restored and review whether thresholds were appropriate. | - Helps maintain service performance under load so fewer incidents escalate into higher impact tiers.<br><br>- Protects the most important user journeys when the system is stressed by prioritising critical traffic.<br><br>- Provides teams with a clear and repeatable way to respond when capacity is under pressure instead of improvising each time.<br><br> | **Minutes saved √ó RM/min**. | High tiers are often associated with load sensitivity. |
""",
                    "satisfaction": """
| Recommendation | Explanation (Phased) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| Impact-based comms cadence | **Phase 1 ‚Äì Set:** Decide how often to communicate for each impact tier and document it clearly, with faster cadences for higher impact levels. Share this schedule with incident commanders and support teams.<br><br>**Phase 2 ‚Äì Content:** Define standard elements that each update should include such as status, user impact, expected resolution time, and next update time. Provide examples so messages are consistent.<br><br>**Phase 3 ‚Äì Review:** After major incidents, gather feedback on the clarity and timing of communications and refine the cadence and content rules. | - Gives users predictable visibility into what is happening so they do not feel left in the dark during outages.<br><br>- Reduces repeated questions and complaints because people know when they will hear from the team again.<br><br>- Improves overall confidence that the service provider is in control even when things go wrong.<br><br> | **Ticket deflection √ó cost**. | Users feel higher tiers more intensely in the donut distribution. |
| Workarounds library | **Phase 1 ‚Äì Document:** Build a library of simple workaround guides that map to each impact tier and the common business tasks affected. Make the instructions clear, step based, and easy to follow.<br><br>**Phase 2 ‚Äì Surface:** Integrate this library into self service portals, chatbots, and support tools so both users and agents can access it quickly during incidents.<br><br>**Phase 3 ‚Äì Measure:** Track which workarounds are used and how much they reduce contact volume and task blockage during incidents. | - Enables users to maintain some level of productivity instead of stopping work completely while waiting for a fix.<br><br>- Reduces pressure on support teams because many users can help themselves with documented steps.<br><br>- Builds trust that the provider understands real user workflows and has prepared practical alternatives for difficult times.<br><br> | **Visible minutes avoided √ó value/min**. | High tiers block core workflows that benefit from alternatives. |
| Executive updates | **Phase 1 ‚Äì Notify:** Provide concise executive level updates for high impact incidents that explain what is happening, which services and customers are affected, and what the current plan is. Avoid deep technical jargon.<br><br>**Phase 2 ‚Äì Align:** Use these updates to align on any business decisions such as temporary service restrictions or customer messaging so technical and commercial teams move in the same direction.<br><br>**Phase 3 ‚Äì Close:** After resolution, share a brief summary of the outcome, key learnings, and next steps so leaders can see that the incident was handled responsibly. | - Reduces confusion and speculation at senior levels by giving a clear and controlled narrative of events.<br><br>- Speeds up necessary decisions because leaders have the context they need without chasing multiple people for information.<br><br>- Supports more consistent communication to customers and partners because internal teams are aligned on facts and messages.<br><br> | **Escalation cost avoided**. | High tiers in the donut naturally drive executive interest. |
| Post-incident notes | **Phase 1 ‚Äì Share:** Write short, understandable summaries after impactful incidents that cover cause, fix, and prevention in language that is accessible to non technical audiences. Publish them in a central place.<br><br>**Phase 2 ‚Äì Timeline:** Include a simple timeline of key events so users can see how quickly the team responded and how the situation evolved.<br><br>**Phase 3 ‚Äì Learn:** Invite feedback on the notes and incorporate suggestions to make future explanations even clearer. | - Demonstrates transparency and accountability, which helps rebuild trust after service disruptions.<br><br>- Reduces repeated queries about what happened because users can refer to a clear and official explanation.<br><br>- Provides useful training material for internal teams who want to learn from past incidents.<br><br> | **Retention uplift**. | Transparency is especially important for high tier slices. |
| Scorecards by tier | **Phase 1 ‚Äì Publish:** Create simple scorecards that show volumes and trends of incidents by impact tier and share them quarterly with stakeholders. Use the same colours and labels as the donut chart to keep things consistent.<br><br>**Phase 2 ‚Äì Compare:** Show changes over time to highlight where risk is increasing or decreasing and relate these changes to completed actions.<br><br>**Phase 3 ‚Äì Iterate:** Use these scorecards to set new goals and track progress on reliability and user experience improvements. | - Provides stakeholders an easy way to understand risk and reliability without reading complex technical reports.<br><br>- Supports more effective planning because everyone can see which tiers need more focus in upcoming quarters.<br><br>- Promotes a shared understanding between technical and business teams about where the service is improving and where work is still needed.<br><br> | **Complaint recurrence decrease**. | Donut view plus scorecards together give a clear high level picture. |
"""
                }

            render_cio_tables("CIO ‚Äì Business Impact Analysis", cio_b)

    # --------------------------------------------------------
    # C. Recovery Performance vs Target
    # --------------------------------------------------------
    with st.expander("üìå Recovery Time vs RTO Target"):
        need = ["service_name", "recovery_time_minutes", "rto_target_minutes"]
        if not set(need).issubset(df.columns):
            st.warning(f"‚ö†Ô∏è Missing required columns: {set(need) - set(df.columns)}")
        else:
            df_rto = df_num.copy()
            df_rto["recovery_time_minutes"] = _to_num(df_rto["recovery_time_minutes"])
            df_rto["rto_target_minutes"] = _to_num(df_rto["rto_target_minutes"])
            df_rto["recovery_gap"] = df_rto["recovery_time_minutes"] - df_rto["rto_target_minutes"]

            # Total overage minutes (purely data-derived)
            overage_minutes_total = float(df_rto.loc[df_rto["recovery_gap"] > 0, "recovery_gap"].sum())

            fig3 = px.bar(
                df_rto,
                x="service_name",
                y="recovery_gap",
                color=(df_rto["recovery_gap"] > 0),
                title="Recovery Time vs Target (RTO Gap)",
                labels={"service_name": "Service", "recovery_gap": "Minutes Over Target"},
                color_discrete_map={True: SECONDARY_BLUE, False: PRIMARY_BLUE},
            )
            fig3.update_traces(cliponaxis=False)
            fig3.update_layout(xaxis_tickangle=-15)
            st.plotly_chart(fig3, use_container_width=True)

            breaches = df_rto[df_rto["recovery_gap"] > 0]
            breach_rate = (len(breaches) / len(df_rto) * 100) if len(df_rto) > 0 else 0
            best_row = df_rto.loc[df_rto["recovery_gap"].idxmin()] if len(df_rto) else None

            best_name = best_row["service_name"] if best_row is not None else "N/A"
            best_gap = float(best_row["recovery_gap"]) if best_row is not None else 0.0

            # Worst over-target service for evidence/cost
            worst_over = df_rto[df_rto["recovery_gap"] > 0]
            worst_service_name = worst_over.loc[worst_over["recovery_gap"].idxmax(), "service_name"] if not worst_over.empty else "N/A"
            worst_service_gap = float(worst_over["recovery_gap"].max()) if not worst_over.empty else 0.0
            worst_count = int((df_rto["service_name"] == worst_service_name).sum()) if worst_service_name != "N/A" else 0

            st.markdown("### Analysis ‚Äì Recovery Performance vs Target")
            st.write(
f"""**What this graph is:** A **bar chart** showing **per-service gap** between actual recovery time and the RTO target.  
- **X-axis:** Service name.  
- **Y-axis:** **Minutes over target** (negative values mean faster than target).

**What it shows in your data:**  
- **Services breaching RTO (gap > 0):** **{len(breaches)}** (**{breach_rate:.1f}%** of services).  
- **Best performer:** **{best_name}** (**{best_gap:.1f} mins** vs target, negative is better).  
- **Total minutes over target (all services):** **{overage_minutes_total:,.1f} mins**.

**How to read it operationally:**  
1) Tackle **largest positive bars** first to claw back overage minutes.  
2) Convert saved minutes to **RM** using the **RM/min** rate derived from your data.  
3) Embed pre-breach alerts and rollback to prevent future spikes.

**Why this matters:** Every **over-target minute** is **avoidable loss**‚Äîreducing it delivers **instant cost and SLA gains**."""
            )

            # Data-derived cost estimate for overage removal
            overage_rm = overage_minutes_total * avg_rm_per_min if avg_rm_per_min == avg_rm_per_min else np.nan
            overage_rm_txt = f"‚âà **RM {overage_rm:,.0f}** (Avg RM/min‚âàRM {avg_rm_per_min:,.2f})" if overage_rm == overage_rm else "derived from RM/min if available"

            cio_c = {
                "cost": f"""
| Recommendation | Explanation (Phased) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| Eliminate over-target minutes | **Phase 1 ‚Äì Rank:** Sort all services by **recovery_gap** from highest positive value to lowest and build a simple list that shows where the biggest over target delays occur. Share this ranking with incident and service owners so they see where they stand.<br><br>**Phase 2 ‚Äì Fix:** Work with the owners of the worst services to streamline their recovery steps, remove unnecessary waits, and automate repetitive tasks so the gap shrinks. Document which actions are applied to which services.<br><br>**Phase 3 ‚Äì Verify:** After changes are implemented, compare the new bar chart against the previous one and confirm that the positive bars for those services are significantly smaller or have moved into negative territory. | - Directly reduces the total number of minutes that services spend above the agreed RTO targets, which lowers real business impact.<br><br>- Creates a transparent prioritisation list so everyone understands which services must be improved first to get the biggest benefit.<br><br>- Encourages continuous improvement because progress is visible in the shrinking positive bars over time.<br><br> | **Portfolio savings ‚âà {overage_rm_txt}** based on **{overage_minutes_total:,.1f} mins** over target in the chart. | Positive bars quantify overage; worst service **{worst_service_name}** at **{worst_service_gap:.1f} mins** over target. |
| Pre-breach alerting and auto-escalation | **Phase 1 ‚Äì Timer:** For each service, set a threshold at around seventy to eighty percent of the RTO target and configure alerts that fire when active recovery time approaches this limit. Communicate clearly what crossing this line means.<br><br>**Phase 2 ‚Äì Page:** When the threshold is crossed, automatically page an incident commander or subject matter expert to accelerate decisions and actions before the breach actually occurs. Ensure escalation paths are unambiguous.<br><br>**Phase 3 ‚Äì Tune:** Review alerts after incidents to check for false positives or missed cases and adjust thresholds and escalation rules until they strike the right balance. | - Prevents many avoidable RTO breaches because the team is mobilised before the target is exceeded instead of after.<br><br>- Reduces the number of incidents that drift into long over target recoveries without urgent attention.<br><br>- Supports better SLA performance and fewer contractual penalties by catching slow recoveries early.<br><br> | **Breaches avoided √ó Avg minutes over √ó Avg RM/min** (use dataset minutes_over). | Number of breaching services: **{len(breaches)}** (**{breach_rate:.1f}%** of services). |
| Fast rollback timeboxes | **Phase 1 ‚Äì Define:** Decide and document a maximum amount of time that the team will spend trying to push a fix forward before rolling back when an incident is linked to a risky change. Make this timebox different by severity if needed.<br><br>**Phase 2 ‚Äì Enforce:** During incidents, ensure that the timebox is actively monitored and that the pre agreed rollback action triggers once the limit is reached. Train incident commanders to apply this rule consistently.<br><br>**Phase 3 ‚Äì Audit:** After each change related incident, review whether the timebox was respected and whether rollback occurred at the right moment, then adjust durations if necessary. | - Caps the worst case duration of change related incidents so they do not run far beyond the RTO target.<br><br>- Reduces the number of situations where teams keep trying risky fixes and accidentally extend downtime instead of restoring service quickly.<br><br>- Builds a culture where protecting users and SLAs is prioritised over pushing a particular change at any cost.<br><br> | **Overage minutes reduced √ó Avg RM/min** from the chart‚Äôs positive bars. | Large positive gaps signal stalled restores needing rollback discipline. |
| Prep artifacts for fast recovery | **Phase 1 ‚Äì Stage:** Identify the most common items that slow down recovery such as missing images, credentials, scripts, or configuration templates and prepare them in advance for the worst performing services.<br><br>**Phase 2 ‚Äì Cache:** Store these artifacts in well known and secure locations that responders can access quickly during incidents without waiting on other teams.<br><br>**Phase 3 ‚Äì Drill:** Run tests that simulate recovery while using the staged artifacts and measure how much faster the process becomes compared to previous incidents. | - Reduces the dead time at the start of an incident where teams are searching for tools or access rather than solving the problem.<br><br>- Lowers stress on responders because they know that the critical building blocks for recovery are already ready to use.<br><br>- Helps more incidents complete within the RTO target because recovery work starts earlier and flows more smoothly.<br><br> | **ŒîStart delay √ó incident count √ó Avg RM/min**. | Overage often front-loads in initialization; evidenced by persistent positive bars. |
| Owner scorecards on overage | **Phase 1 ‚Äì Attribute:** Allocate over target minutes and estimated RM impact to each service owner and present this as a simple scorecard. Ensure that the data is accurate and shared in advance of governance meetings.<br><br>**Phase 2 ‚Äì Goal:** Agree on realistic quarterly reduction targets for each owner based on their starting position and the opportunities they have to make improvements.<br><br>**Phase 3 ‚Äì Review:** Hold regular governance sessions where owners present progress, explain any challenges, and share actions that worked well so others can copy them. | - Builds clear accountability for reducing RTO breaches instead of treating them as a shared but vague problem.<br><br>- Encourages owners to compete in a healthy way to reduce their over target minutes and improve their position on the scorecard.<br><br>- Helps leadership see where improvement is happening and where additional support or pressure may be required.<br><br> | **Quarterly RM overage drop** vs baseline derived from chart. | Bar trends provide before/after overage evidence. |
""",
                "performance": f"""
| Recommendation | Explanation (Phased) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| Parallelize restore tasks | **Phase 1 ‚Äì Map:** Break down the recovery process for the worst performing services into individual steps and identify which steps truly depend on others and which can run in parallel. Document this as a simple flow.<br><br>**Phase 2 ‚Äì Execute:** Adjust runbooks and tooling so independent steps are executed at the same time rather than sequentially. Train responders to follow the new pattern during real incidents.<br><br>**Phase 3 ‚Äì Validate:** After changes, compare actual recovery times to previous incidents and confirm that the total duration has dropped without introducing new errors. | - Reduces total recovery time by removing unnecessary waiting between steps that do not really depend on each other.<br><br>- Helps teams use their resources more efficiently because different people and systems can work at the same time instead of in a long chain.<br><br>- Increases the likelihood that services will recover within their RTO target, especially for complex multi step scenarios.<br><br> | **ŒîMTTR √ó incidents** for worst services. | Wide positive gaps imply sequential bottlenecks. |
| RTO rehearsal calendar | **Phase 1 ‚Äì Select:** Choose a small set of services with the largest positive gaps and schedule regular recovery rehearsals for them, such as once per quarter. Make sure the scope and objectives of each rehearsal are clear.<br><br>**Phase 2 ‚Äì Measure:** During rehearsals, capture key metrics such as actual recovery time, steps that took longest, and coordination delays, then compare them against target RTO values.<br><br>**Phase 3 ‚Äì Scale:** Once rehearsals show strong improvements for these services, extend the practice to additional services while keeping the focus on those with the biggest gaps. | - Improves real incident performance because teams get repeated practice recovering the most problematic services in a controlled environment.<br><br>- Reveals hidden weaknesses in procedures or tooling that may not be visible from charts alone.<br><br>- Builds a culture of continuous learning where RTO is treated as a skill to be practised rather than a number on a slide.<br><br> | **ŒîBreach rate √ó penalty** or **Œîminutes over √ó Avg RM/min**. | Concentrated positives identify drill candidates. |
| Dependency resolution upfront | **Phase 1 ‚Äì Trace:** For services with large gaps, map their dependencies such as upstream systems, external vendors, and shared infrastructure to identify where restores are being slowed down by other components.<br><br>**Phase 2 ‚Äì Fix:** Work with owners of these dependencies to improve their availability, response time, and participation during incidents. Capture new agreements in clear support arrangements.<br><br>**Phase 3 ‚Äì Re-test:** Trigger test incidents or planned failovers to confirm that the improved dependencies now support faster recovery for the original service. | - Removes external bottlenecks that can block recovery even when the primary team works efficiently.<br><br>- Improves collaboration between teams and vendors because expectations and responsibilities become explicit.<br><br>- Reduces overall repair time for complex incidents where multiple systems must work together to restore service.<br><br> | **ŒîWaiting time √ó incidents √ó Avg RM/min**. | Persistent over-target bars suggest dependency latency. |
| Live ‚Äúminutes to RTO‚Äù dashboard | **Phase 1 ‚Äì Expose:** Build a simple dashboard that shows for each open incident how many minutes remain until the RTO target is reached and whether the incident is on track or at risk. Make this visible to responders and leaders.<br><br>**Phase 2 ‚Äì Alert:** Configure visual and possibly audible alerts when incidents approach the RTO limit so the team can react before a breach occurs. Clarify how these alerts should trigger action.<br><br>**Phase 3 ‚Äì Improve:** After incidents, review how the dashboard and alerts influenced behaviour and refine them to become more helpful and less noisy. | - Keeps time pressure visible in a clear and objective way so teams do not lose track of RTO commitments during busy incidents.<br><br>- Helps incident commanders prioritise attention and resources on the cases that are closest to breaching targets.<br><br>- Provides leadership with a real time view of risk rather than only after the fact reporting.<br><br> | **Breaches avoided √ó Avg minutes over** from data. | Over-target bars reflect poor time awareness during incidents. |
| Post-breach RCA within 48h | **Phase 1 ‚Äì Capture:** For each RTO breach, hold a focused review within forty eight hours to document what caused the delay and which parts of the process or technology contributed the most to the overage.<br><br>**Phase 2 ‚Äì Fix:** Define concrete actions that remove or reduce these causes, such as new automation, clearer decision criteria, or improved access to tools, and assign owners and due dates.<br><br>**Phase 3 ‚Äì Verify:** Check subsequent incidents for the same service to confirm that the same pattern no longer produces over target recovery times. | - Prevents repeated RTO breaches for the same reasons and turns individual failures into learning opportunities.<br><br>- Increases the quality of recovery processes over time because each breach drives targeted improvements instead of generic recommendations.<br><br>- Reassures stakeholders that breaches are taken seriously and that specific steps are being taken to reduce them in future.<br><br> | **Repeat overage decrease √ó Avg RM/min**. | Patterns across services show systemic causes. |
""",
                "satisfaction": f"""
| Recommendation | Explanation (Phased) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| User updates tied to RTO clock | **Phase 1 ‚Äì Communicate:** During incidents, clearly explain to users how the RTO target applies and how close the incident is to that limit. Use simple language instead of internal jargon.<br><br>**Phase 2 ‚Äì Frequency:** Increase update frequency as the clock approaches the target so users feel better informed when risk is highest. Make sure each update mentions expected next communication time.<br><br>**Phase 3 ‚Äì Close:** After recovery, provide a final message that explains whether the incident stayed within the RTO and what is being done to reduce the risk of future breaches. | - Reduces anxiety and frustration by giving users a clear sense of timing rather than leaving them to guess how long the outage might last.<br><br>- Cuts down duplicate contacts and complaints because users understand that the situation is under control and monitored against a clear target.<br><br>- Increases trust in the reliability process because the RTO figure feels real and visible instead of a hidden internal metric.<br><br> | **Ticket deflection √ó handling cost**; impact is higher where gaps are largest. | Positive bars (e.g., **{worst_service_name}** at **{worst_service_gap:.1f} mins**) drive anxiety without updates. |
| Workarounds for gap-heavy services | **Phase 1 ‚Äì Publish:** For services with large positive gaps, identify critical user journeys and document temporary workarounds that allow at least partial completion of those journeys during outages. Use clear instructions and examples.<br><br>**Phase 2 ‚Äì Train:** Teach support agents and customer facing teams how to guide users through these workarounds when recovery is taking longer than expected.<br><br>**Phase 3 ‚Äì Measure:** Track usage of workarounds and how they influence user satisfaction scores and contact volumes during long incidents. | - Allows users to keep working even when full recovery is delayed, which reduces the felt impact of long incidents.<br><br>- Supports support teams by giving them practical tools to help customers instead of only asking them to wait.<br><br>- Demonstrates that the provider understands and cares about real user tasks, not just about internal technical metrics.<br><br> | **Visible minutes avoided √ó value/min** on worst services. | The tallest positive gaps mark where productivity suffers most. |
| VIP comms for critical services | **Phase 1 ‚Äì Tag:** Identify VIP customers and link them to the services that show the largest RTO gaps so that the impact on them is well understood. Keep this mapping current as accounts change.<br><br>**Phase 2 ‚Äì Notify:** When those services breach or are at risk of breaching RTO, send tailored updates to VIP stakeholders explaining status, risk, and any special support being offered.<br><br>**Phase 3 ‚Äì Review:** After major incidents, review VIP feedback and adjust communication style and content to better fit their expectations. | - Reduces the risk that high value customers feel ignored during prolonged outages on critical services.<br><br>- Helps account teams manage difficult conversations because they have timely and structured information to share.<br><br>- Supports retention and renewal by showing VIP customers that they receive special attention when things go wrong.<br><br> | **Churn avoided √ó ACV**. | Worst bars typically touch flagship or VIP journeys. |
| Closure notes with gap reduction | **Phase 1 ‚Äì Share:** After resolving incidents with notable RTO gaps, write short closure notes that explain what slowed recovery and which actions will be taken to reduce the gap next time. Use direct and honest language.<br><br>**Phase 2 ‚Äì Evidence:** Include simple before and after metrics for services where improvements have already reduced gaps so users can see progress over time.<br><br>**Phase 3 ‚Äì Replicate:** Use successful gap reduction stories to inspire similar actions on other services and share them across teams. | - Shows users and stakeholders that long recoveries are not being ignored and that concrete work is planned to improve them.<br><br>- Builds confidence because improvements are backed by numbers instead of only promises.<br><br>- Helps spread effective practices quickly because teams can see real examples of what worked elsewhere.<br><br> | **Complaint recurrence decrease** as gap shrinks. | Chart deltas show tangible improvement. |
| Monthly reliability brief | **Phase 1 ‚Äì Summarize:** Each month, produce a short briefing that highlights total over target minutes, the services most affected, and the main actions taken to address them. Present this in simple visuals such as the RTO gap chart.<br><br>**Phase 2 ‚Äì Forecast:** Outline expected risks and focus areas for the coming month based on trends and planned work so stakeholders know what is being watched closely.<br><br>**Phase 3 ‚Äì Commit:** Record specific owner commitments and timelines for the next improvements and review them in the following brief. | - Provides a regular, predictable channel for reliability information so stakeholders do not need to chase fragmented updates.<br><br>- Supports better planning because technology and business teams have a shared view of recovery performance and future risk.<br><br>- Demonstrates ongoing commitment to improving RTO performance rather than treating it as a one off clean up exercise.<br><br> | **Ticket deflection √ó handling cost**; **Retention uplift** where communication is valued. | Bars trend neatly month to month for execs. |
"""
            }

            render_cio_tables("CIO ‚Äì Recovery Time vs RTO Target", cio_c)
