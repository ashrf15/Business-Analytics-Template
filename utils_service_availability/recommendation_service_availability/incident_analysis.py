# utils_service_availability/recommendation_service_availability/incident_analysis.py

import streamlit as st
import plotly.express as px
import pandas as pd
import numpy as np
import uuid  # ‚úÖ Added to generate unique keys

# ============================================================
# Helper: Generate unique chart keys
# ============================================================
def unique_key(prefix: str):
    """Generates a unique Streamlit key using a prefix + random UUID."""
    return f"{prefix}_{uuid.uuid4().hex[:8]}"


# ============================================================
# Helper: CIO Table Renderer
# ============================================================
def render_cio_tables(title, cio_data):
    st.subheader(title)
    with st.expander(" Cost Reduction"):
        st.markdown(cio_data["cost"], unsafe_allow_html=True)
    with st.expander(" Performance Improvement"):
        st.markdown(cio_data["performance"], unsafe_allow_html=True)
    with st.expander(" Customer Satisfaction Improvement"):
        st.markdown(cio_data["satisfaction"], unsafe_allow_html=True)


# ========== Mesiniaga Theme ==========
MES_BLUE = ["#004C99", "#007ACC", "#3399FF", "#66B2FF", "#9BD1FF"]
try:
    import plotly.express as px
    px.defaults.template = "plotly_white"
except Exception:
    pass


# ============================================================
# INCIDENT ANALYSIS DASHBOARD SECTION
# ============================================================
def incident_analysis(df: pd.DataFrame):

    # ============================================================
    # 5a. Summary of Incidents or Outages
    # ============================================================
    with st.expander("üìå Summary of Incidents or Outages Affecting Service Availability"):
        required = {"report_date", "service_name", "incident_count"}
        if required.issubset(df.columns):
            df["report_date"] = pd.to_datetime(df["report_date"], errors="coerce")
            trend = (
                df.groupby(["report_date", "service_name"], as_index=False)["incident_count"]
                .sum()
                .sort_values("report_date")
            )

            fig = px.line(
                trend,
                x="report_date",
                y="incident_count",
                color="service_name",
                markers=True,
                title="Incident Trends Over Time by Service",
                labels={"report_date": "Date", "incident_count": "Number of Incidents"},
                color_discrete_sequence=MES_BLUE,
                template="plotly_white",
            )
            st.plotly_chart(fig, use_container_width=True, key=unique_key("incident_summary_chart"))

            total_incidents = df["incident_count"].sum()
            daily = df.groupby("report_date")["incident_count"].sum().reset_index(name="daily_incidents")
            avg_daily = daily["daily_incidents"].mean() if not daily.empty else 0.0
            max_day_row = daily.loc[daily["daily_incidents"].idxmax()] if not daily.empty else None

            st.markdown("### Analysis ‚Äî Incident Summary")
            if max_day_row is not None:
                st.write(
f"""**What this graph is:** A **multi-line time series** showing **daily incident counts** split by **service**.  
- **X-axis:** Calendar date.  
- **Y-axis:** Number of incidents per day (higher values indicate more disruption pressure).

**What it shows in your data:**  
- **Total incidents recorded:** **{int(total_incidents):,}**.  
- **Average daily incidents:** **{avg_daily:.1f}** per day.  
- **Peak day:** **{max_day_row['report_date'].strftime('%d/%m/%Y')}** with **{int(max_day_row['daily_incidents'])} incidents** across services.

**How to read it operationally:**  
1) **Peaks:** Trigger surge playbooks (on-call expansion, triage desk, fast routing) on peak-like days.  
2) **Plateaus:** If consistently high, reassess change cadence and capacity; recurring elevation implies structural issues.  
3) **Downswings:** Validate which interventions (maintenance, release freezes, automation) caused the reduction and standardize them.  
4) **Mix:** Compare services that spike together to detect shared dependencies (e.g., platform, network, or authentication).

**Why this matters:** Incident volume is an **early cost signal**‚Äîmore incidents drive more recovery time and higher user disruption. Stabilizing the curve lowers overtime, prevents SLA breaches, and improves customer confidence."""
                )
            else:
                st.info("No incident rows available for analysis narrative.")

            # ---------------- CIO table 5a ----------------
            peak_date = max_day_row["report_date"].strftime('%d/%m/%Y') if max_day_row is not None else "N/A"
            peak_val = int(max_day_row["daily_incidents"]) if max_day_row is not None else 0
            gap_vs_mean = max(peak_val - avg_daily, 0)

            cio_5a = {
                "cost": f"""
| Recommendation | Explanation (Phase) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| **Target surge periods to cut overtime** | **Phase 1 ‚Äì Identify:** Mark peak days such as **{peak_date}** where the number of incidents climbs to **{peak_val}** and compare this level with the normal daily mean of **{avg_daily:.1f}** so that you clearly see which days put abnormal pressure on the team.<br><br>**Phase 2 ‚Äì Staff:** For those identified high-pressure days, pre-authorize surge shifts and adjust rosters in advance so that enough people are on duty and overtime becomes planned rather than last-minute and chaotic.<br><br>**Phase 3 ‚Äì Review:** After the next similar spike period, compare the planned surge staffing cost with the avoided overtime and backlog so that you know whether this targeted staffing strategy is financially and operationally effective. | - Focuses staffing on the exact days where the team faces abnormal incident volume so that overtime is used strategically instead of reactively.<br><br>- Reduces fatigue and error rates because employees are not constantly pushed into last-minute extra hours during unplanned spikes.<br><br>- Prevents incident queues from growing uncontrollably on peak days so that resolution times stay closer to normal and SLA penalties are less likely.<br><br> | **Savings ‚âà (Overtime hrs avoided √ó hourly rate)**, sized by **excess incidents ‚âà {gap_vs_mean:.1f}** over mean. | Time series shows **{peak_date}** at **{peak_val}** incidents versus mean **{avg_daily:.1f}**. |
| **Automate first-touch triage** | **Phase 1 ‚Äì Map:** Use historical spike days to identify the top three most frequent incident categories so that you know which issues are most suitable for automation and standardised handling.<br><br>**Phase 2 ‚Äì Automate:** Build rules or workflows that automatically categorise, assign and prefill these common tickets with known information so that analysts no longer spend time on manual data entry and routing for repeat patterns.<br><br>**Phase 3 ‚Äì Iterate:** Each month, review misrouted or overridden tickets and refine the automation rules so that accuracy and trust in the automated triage continue to improve. | - Reduces the amount of manual work per incident because common information is filled in automatically and tickets go directly to the correct queues.<br><br>- Speeds up how quickly tickets reach the right subject matter experts so that incidents are resolved faster and with fewer handoffs.<br><br>- Frees frontline analysts to focus on complex or unusual issues instead of repetitive categorisation work, which improves overall efficiency and morale.<br><br> | **Hours saved = (Time saved/incident √ó {int(total_incidents):,})** using your dataset‚Äôs incident volume. | Recurrent spikes across services imply heavy triage overhead on peaks. |
| **De-duplicate repeat tickets** | **Phase 1 ‚Äì Detect:** During bursts of activity, detect tickets with very similar descriptions, sources or error patterns so that potential duplicates can be identified in near real time.<br><br>**Phase 2 ‚Äì Notify:** When a likely duplicate is detected, link it to a parent incident and inform the user that the issue is already being tracked so that unnecessary new tickets are discouraged.<br><br>**Phase 3 ‚Äì Track:** Monitor the ratio of duplicate to unique tickets over time so that you can see whether the de-duplication strategy is successfully reducing noise. | - Directly reduces the number of tickets that need to be processed because many of them are duplicates of the same underlying problem.<br><br>- Shrinks queue depth for analysts so that effort is spent on distinct issues instead of repeatedly reading and logging clones of the same symptom.<br><br>- Lowers the risk of SLA breaches because fewer redundant tickets compete for attention while the root problem is being fixed.<br><br> | **Cost avoided = (Duplicates prevented √ó handling cost)** calculated from peak-day duplication vs baseline. | Sharp peaks typically include duplicate submissions; line height evidences burst pressure. |
| **Seasonal freeze on risky changes** | **Phase 1 ‚Äì Find:** Use several months of data to identify calendar periods where incident spikes occur repeatedly so that you can see which weeks or days are seasonally fragile.<br><br>**Phase 2 ‚Äì Freeze:** In these higher-risk windows, block or significantly restrict high-risk changes so that the chance of adding new instability into already sensitive periods is reduced.<br><br>**Phase 3 ‚Äì Canary:** Move risky releases outside the freeze periods and deploy them using canary patterns so that any negative impact is caught early with limited blast radius. | - Prevents new change-related outages from stacking on top of natural seasonal risk periods where the environment is already stressed.<br><br>- Reduces firefighting and emergency recovery work because fewer high-risk changes are introduced when the system is historically most vulnerable.<br><br>- Keeps service levels more stable during critical business cycles, which protects revenue and user confidence.<br><br> | **Upper bound:** Œî incidents between freeze months and prior comparable non-freeze months. | Peaks cluster temporally; series reveals repeat periods. |
| **Problem management on top drivers** | **Phase 1 ‚Äì Cluster:** For days that look like **{peak_date}**, group incidents by service and type to identify a small set of dominant technical or process drivers behind the spike.<br><br>**Phase 2 ‚Äì Fix:** Design and implement structural fixes, such as configuration changes or engineering enhancements, that specifically target the top two drivers so that future incidents of the same type are less likely to occur.<br><br>**Phase 3 ‚Äì Verify:** When similar conditions appear again, compare the new incident volume to previous spikes to confirm that the problem management actions have reduced recurrence. | - Shifts effort from repeatedly fixing symptoms to removing the main root patterns that generate large numbers of incidents.<br><br>- Delivers sustained reduction in incident volume and cost because the underlying weaknesses are addressed rather than treated superficially.<br><br>- Lowers the number of escalations and crises over time because the biggest and most frequent issues are systematically eliminated from the environment.<br><br> | **Run-rate reduction = (Incidents reduced/month √ó handling cost)** post-fix. | Repeating spike shapes signal chronic drivers suitable for PM. |
""",
                "performance": f"""
| Recommendation | Explanation (Phase) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| **Live incident heatmap** | **Phase 1 ‚Äì Build:** Create a near real-time dashboard that shows incidents per minute or per hour, and highlight when the rate is trending above the daily average of **{avg_daily:.1f}** so that spikes are visible as they form rather than only after the day ends.<br><br>**Phase 2 ‚Äì Alert:** Configure paging or alert rules that trigger when the incident rate crosses a defined threshold so that responders are engaged early during developing surges.<br><br>**Phase 3 ‚Äì Tune:** Review threshold behaviour every month and adjust sensitivity so that the heatmap is useful and does not become noisy. | - Enables teams to see incident surges early and react before queues and backlogs become unmanageable.<br><br>- Reduces the time between the start of a spike and the moment when extra support is mobilised, which shortens the duration of high-impact periods.<br><br>- Makes incident management more data driven because decisions about escalation and staffing are based on real-time trends rather than intuition.<br><br> | **MTTR gain = (Old MTTR ‚àí New MTTR) √ó incidents** during spikes. | Sudden rises above mean visible around **{peak_date}**. |
| **Skill-based routing** | **Phase 1 ‚Äì Map:** Define clear mappings between incident categories and the specialist teams or individuals best suited to handle them so that routing decisions are explicit instead of ad hoc.<br><br>**Phase 2 ‚Äì Auto-route:** Configure the service desk tool to route new incidents directly to these specialised queues so that generic queues do not become bottlenecks for complex work.<br><br>**Phase 3 ‚Äì Audit:** Regularly analyse reassignments and first-touch success rates so that routing rules can be refined and misroutes reduced. | - Cuts down on the number of handoffs because incidents are sent directly to the right team from the start.<br><br>- Reduces waiting time for users because tickets do not sit in generic queues while staff figure out who should own them.<br><br>- Improves the quality and speed of resolutions because specialists spend more time on issues they are trained to handle and less time sorting through unrelated tickets.<br><br> | **Hours saved = (Handoffs avoided √ó avg handoff delay)** measured on spike days. | Multi-service spikes stress generic queues; routing removes choke points. |
| **Post-peak retrospectives** | **Phase 1 ‚Äì RCA:** After a major spike like **{peak_date}**, run a retrospective to understand which events, changes or capacity issues contributed to the high incident count.<br><br>**Phase 2 ‚Äì Action:** Translate the key findings into specific improvement tasks such as tuning thresholds, refining automation or adjusting change windows so that future spikes are less severe.<br><br>**Phase 3 ‚Äì Track:** During the next peak-like period, check whether the number and severity of incidents have dropped, and capture lessons that still need to be acted upon. | - Creates a structured learning process from bad days so that each spike leads to improvements rather than being forgotten.<br><br>- Reduces the chance that the same patterns will cause similar or worse spikes in the future because identified gaps are deliberately closed.<br><br>- Improves stability trends over time as post-peak actions accumulate and gradually reduce the environment‚Äôs sensitivity to stress.<br><br> | **Incidents prevented ‚âà (peak ‚àí mean) ‚âà {gap_vs_mean:.1f}** on similar days. | Peak-to-mean gap quantifies avoidable volume. |
| **Rate-limit noisy sources** | **Phase 1 ‚Äì Identify:** Determine which forms, integrations or systems generate a large number of low value or repetitive tickets during spikes so that the main noisy sources are clearly understood.<br><br>**Phase 2 ‚Äì Gate:** Apply throttling, grouping or message suppression to these sources so that repeated submissions are limited and users are instead directed to existing parent incidents or information.<br><br>**Phase 3 ‚Äì Educate:** Provide simple feedback prompts or user messages that explain what is happening so that people understand that their request has already been captured. | - Stabilises the inflow of tickets during stressful periods so that analysts can stay focused on genuine new issues rather than floods of duplicates.<br><br>- Reduces queue oscillations and wild swings in workload that come from a small number of noisy sources sending repeated messages.<br><br>- Improves the predictability of incident management and makes it easier to size and plan staffing for peak times.<br><br> | **Reduction = (Noisy inflow drop √ó handling time)** vs pre-limit baseline. | Steep slopes in the series suggest noisy generators. |
| **Change-incident linking** | **Phase 1 ‚Äì Correlate:** Systematically link incident records to recent change windows so that it is clear which incidents are likely to be change-related rather than random failures.<br><br>**Phase 2 ‚Äì Gate:** Use this correlation to tighten reviews on high-risk changes and require stronger evidence before approving deployments that touch sensitive areas.<br><br>**Phase 3 ‚Äì Canary:** Implement canary releases and rollback rules for changes that have historically been associated with spikes so that impact is smaller and easier to reverse. | - Improves understanding of how much incident volume is driven by changes, which allows more targeted improvements to the change process.<br><br>- Reduces the number of change-induced outages because risky deployments are scrutinised more carefully and rolled out more safely.<br><br>- Increases trust between operations and change teams because everyone has clearer data linking releases to resulting incidents and can act on it together.<br><br> | **Benefit = (Change-linked incidents ‚Üì √ó MTTR)** measured across months. | Temporal proximity of spikes to release days. |
""",
                "satisfaction": f"""
| Recommendation | Explanation (Phase) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| **Proactive user comms on spikes** | **Phase 1 ‚Äì Detect:** As soon as a spike that looks like **{peak_date}** begins, use monitoring to detect that the environment is entering an abnormal state so that communications can start early.<br><br>**Phase 2 ‚Äì Inform:** Quickly send clear messages to users about the nature of the issue, expected timelines and any available workarounds so that they can plan their work rather than waiting in the dark.<br><br>**Phase 3 ‚Äì Close:** After the spike is resolved, send a concise summary of what happened and what will be done to prevent similar events so that users feel informed and respected. | - Reduces the number of inbound calls and follow-ups because users already know what is happening and what they should do while services are degraded.<br><br>- Lowers frustration and anger because stakeholders feel that they are being kept in the loop instead of being ignored during high-impact incidents.<br><br>- Helps users stay productive by giving them workarounds or alternative routes that they can use while the incident is being resolved.<br><br> | **Complaints avoided √ó handling cost**, sized against spike-day call volume vs baseline. | Peak days correlate with user frustration; the curve‚Äôs apex is **{peak_val}**. |
| **VIP shielding during surges** | **Phase 1 ‚Äì Prioritize:** Identify key customers or business units that are most sensitive to downtime so that you can offer them extra support during spikes.<br><br>**Phase 2 ‚Äì Support:** During surge periods, provide these VIPs with dedicated status updates, direct contact channels and faster routing to subject matter experts so that their critical work is protected as much as possible.<br><br>**Phase 3 ‚Äì Review:** After each surge, review feedback from VIPs to check whether the additional support is meeting their expectations and adjust where needed. | - Protects the operations of high-value users, which reduces the risk of losing major customers due to repeated bad experiences.<br><br>- Demonstrates that the organisation understands and honours different levels of criticality across its customer base.<br><br>- Strengthens relationships with important stakeholders because they can see that the support model adjusts to their importance during difficult periods.<br><br> | **Retention value** for VIP cohort exposed on spike months. | Spike intensity maps to VIP pain. |
| **Public incident timeline** | **Phase 1 ‚Äì Status page:** Maintain a simple status page that shows when incidents start, what is being done and current status so that anyone can check the latest information without opening a ticket.<br><br>**Phase 2 ‚Äì Milestones:** Update this page regularly with progress milestones such as ‚Äúidentified root cause‚Äù or ‚Äúrollback in progress‚Äù so that users can see movement during longer events.<br><br>**Phase 3 ‚Äì RCA:** After resolution, link a short root cause explanation so that people can understand why it happened and what will change. | - Reduces the number of users who need to contact support for updates because information is always available in one central place.<br><br>- Creates a perception of control and professionalism because the organisation is visibly managing the incident with clear steps and communication.<br><br>- Supports better CSAT scores because users feel informed and see evidence that learning is taken seriously after issues occur.<br><br> | **Deflection √ó handling cost** vs no-status baseline. | Distinct spikes justify visible communication. |
| **Self-service guidance** | **Phase 1 ‚Äì Build:** Create easy-to-follow guides for the most frequent incident scenarios that appear during spikes so that users know how to diagnose and mitigate simple issues on their own.<br><br>**Phase 2 ‚Äì Surface:** Publish these guides prominently in portals and chatbots so that they are easy to find without needing to talk to support staff.<br><br>**Phase 3 ‚Äì Measure:** Track how many tickets are deflected or solved through self-service to understand where content should be expanded or improved. | - Helps users resolve minor or known issues themselves, which keeps them productive even when the support team is under heavy load.<br><br>- Reduces the number of simple tickets reaching the service desk so that agents can focus on complex, high-impact incidents during peaks.<br><br>- Builds user confidence in the support ecosystem because they feel empowered and supported with clear instructions and options.<br><br> | **Deflected tickets √ó handling cost** on peak-like days. | Repeated patterns imply automatable topics. |
| **Expectation management SLAs** | **Phase 1 ‚Äì Show:** For logged incidents, display visible SLA clocks or expected resolution windows so that users have realistic expectations about timing from the beginning.<br><br>**Phase 2 ‚Äì Update:** Adjust these ETAs and provide progress updates during spikes when timelines shift so that users understand the reasons for delays.<br><br>**Phase 3 ‚Äì Alert:** Notify users proactively if a breach is likely so that they can adjust their plans rather than being surprised at the last minute. | - Lowers anxiety because users can see visible timers and updated expectations instead of wondering what is happening behind the scenes.<br><br>- Reduces escalations because potential delays are communicated early with reasons and revised timeframes.<br><br>- Supports stronger relationships with users since they can make informed decisions about their own work schedules based on transparent information.<br><br> | **Escalations avoided √ó cost** comparing spike vs steady days. | Peak windows carry higher breach risk; visibility mitigates. |
"""
            }

            render_cio_tables("CIO ‚Äì Incident Summary", cio_5a)
        else:
            st.warning(f"‚ö†Ô∏è Missing required columns: {required - set(df.columns)}")

    # ============================================================
    # 5b. Business Impact Analysis
    # ============================================================
    with st.expander("üìå Impact Analysis of Incidents on Users and Business Operations"):
        required = {"service_name", "incident_count", "estimated_cost_downtime", "business_impact"}
        if required.issubset(df.columns):
            impact_df = (
                df.groupby("service_name", as_index=False)
                .agg(total_incidents=("incident_count", "sum"),
                     total_cost=("estimated_cost_downtime", "sum"))
                .sort_values("total_cost", ascending=False)
            )

            fig = px.bar(
                impact_df.head(10),
                x="service_name",
                y="total_cost",
                text="total_cost",
                title="Top 10 Services by Downtime Cost",
                labels={"service_name": "Service Name", "total_cost": "Total Cost of Downtime (RM)"},
                color_discrete_sequence=MES_BLUE,
                template="plotly_white",
            )
            fig.update_traces(texttemplate="RM %{text:,.0f}", textposition="outside")
            st.plotly_chart(fig, use_container_width=True, key=unique_key("business_impact_chart"))

            top = impact_df.iloc[0]
            avg_cost = impact_df["total_cost"].mean()
            low = impact_df.iloc[-1]
            total_cost_all = impact_df["total_cost"].sum()

            st.markdown("### Analysis ‚Äî Business Impact")
            st.write(
f"""**What this graph is:** A **ranked bar chart** of **total downtime cost (RM)** per **service** to pinpoint financial exposure.  
- **X-axis:** Service name.  
- **Y-axis:** Total estimated cost of downtime (RM) across the period.

**What it shows in your data:**  
- **Highest-impact service:** **{top['service_name']}** at **RM {top['total_cost']:,.0f}**.  
- **Lowest of top set:** **{low['service_name']}** at **RM {low['total_cost']:,.0f}**.  
- **Average cost per service:** **RM {avg_cost:,.0f}**; **Total portfolio cost:** **RM {total_cost_all:,.0f}**.

**How to read it operationally:**  
1) **Head of the distribution:** Focus fixes on the tallest bars first; few services drive most of the loss.  
2) **Tail:** Verify if low-cost services are genuinely stable or simply under-reported.  
3) **Gap to average:** Services far above average require priority RCAs and capacity checks.

**Why this matters:** Financial concentration reveals **where each hour of engineering saves the most RM**. Tackling the top services first maximizes ROI and reduces user-visible pain quickly."""
            )

            # ---------------- CIO table 5b ----------------
            top3_cost = impact_df.head(3)["total_cost"].sum() if not impact_df.empty else 0
            median_cost = impact_df["total_cost"].median() if not impact_df.empty else 0

            cio_5b = {
                "cost": f"""
| Recommendation | Explanation (Phase) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| **Fix the #1 cost driver: {top['service_name']}** | **Phase 1 ‚Äì RCA:** Run a structured root cause analysis on **{top['service_name']}** to list the main failure modes and quantify how many minutes and how much cost each one generates so that you know exactly what you are fixing.<br><br>**Phase 2 ‚Äì Engineer:** Design and implement technical improvements such as redundancy, patches or configuration hardening that address those dominant failure modes so that the service becomes more resilient in daily operation.<br><br>**Phase 3 ‚Äì Validate:** In the following reporting cycle, compare the downtime cost for **{top['service_name']}** with the previous period so that you can confirm if the engineering work has meaningfully reduced financial loss. | - Concentrates effort on the single service that is responsible for the highest downtime cost so that investment delivers a strong and visible financial impact.<br><br>- Reduces the risk of future penalties and revenue loss because a major weak point in the environment is addressed at its root.<br><br>- Frees up operational capacity in the long term because fewer large incidents arise from this service once the main causes are fixed.<br><br> | **Upper bound savings = RM {top['total_cost']:,.0f}** (dataset cost for this service). | Tallest bar = **{top['service_name']}**, **RM {top['total_cost']:,.0f}**. |
| **Top-3 remediation sprint** | **Phase 1 ‚Äì Tiger team:** Form a cross-functional squad specifically assigned to focus on the three highest cost services so that decisions and actions are fast and not slowed by competing priorities.<br><br>**Phase 2 ‚Äì Execute:** Plan and run the most impactful fixes for these three services in parallel, such as design changes, capacity increases or configuration clean-up, to reduce calendar time to benefit.<br><br>**Phase 3 ‚Äì Measure:** After the sprint, compare pre and post downtime cost for the Top-3 services to see how much financial improvement the focused effort delivered. | - Maximises return on effort because the limited engineering capacity is directed to the small set of services that drive most of the cost.<br><br>- Compresses cumulative downtime cost faster than spreading small improvements across many low-impact services.<br><br>- Creates a clear success story that can be communicated to stakeholders, showing how targeted engineering work directly reduced financial exposure.<br><br> | **Savings ‚âà RM {top3_cost:,.0f}** (sum of Top-3 bars). | Cost curve is head-heavy in the rank chart. |
| **Cost-aware maintenance windows** | **Phase 1 ‚Äì Map:** Identify the business hours when the top cost services generate the most revenue or handle the most critical workloads so that you know which times are most sensitive to downtime.<br><br>**Phase 2 ‚Äì Shift:** Move maintenance activities for these services to lower-risk windows and ensure that maintenance durations are as short as possible so that the overlap with important business operations is minimised.<br><br>**Phase 3 ‚Äì Canary:** Use canary approaches and technical checks after maintenance to confirm that the service returns to normal performance quickly. | - Reduces the financial impact of planned downtime because maintenance is scheduled away from periods where outages are most expensive.<br><br>- Leads to fewer customer complaints and escalations because disruptive activities are aligned with quieter business windows.<br><br>- Protects revenue streams and business-critical processes by ensuring that the most valuable periods are kept as stable as possible.<br><br> | **Avoided cost = planned minutes √ó cost/min** (use your dataset‚Äôs minutes if available). | High bars indicate high sensitivity to downtime timing. |
| **Kill single points of failure (SPOF)** | **Phase 1 ‚Äì Discover:** For the highest cost services, identify components such as servers, network links or applications where failure would cause a complete outage because there is no backup path.<br><br>**Phase 2 ‚Äì Mitigate:** Introduce high availability configurations, failover mechanisms or alternative routes so that the service can continue to operate even if one component fails.<br><br>**Phase 3 ‚Äì Drill:** Periodically test switchover and failover scenarios so that you know the design works and the team knows how to execute it. | - Prevents catastrophic outages where one small failure brings down a whole service and creates very expensive downtime events.<br><br>- Increases the resilience of critical services so that they can tolerate component failures with minimal user impact.<br><br>- Reduces the uncertainty and fear around large incidents because there is a known, tested ability to fail over to alternative components or paths.<br><br> | **Benefit = avoided outage minutes √ó cost/min** on historically impacted services. | High total cost often tied to rare, severe outages. |
| **Contractual penalty review** | **Phase 1 ‚Äì Map:** For the top cost services, map any contractual penalty clauses to their current performance so that you know where the biggest penalty risks sit.<br><br>**Phase 2 ‚Äì Adjust:** For services whose actual capability is far from the contractual SLA, either strengthen the service reliability or negotiate a more realistic SLA so that expectations match reality.<br><br>**Phase 3 ‚Äì Monitor:** Track breach counts, penalty payments and service performance each month so that contractual risk stays visible and actively managed. | - Reduces unexpected penalty payments because you either improve performance to meet the SLA or reset the SLA to a level that the service can consistently achieve.<br><br>- Aligns commercial commitments with operational realities so that both customers and providers have clear and realistic expectations.<br><br>- Reduces financial volatility in monthly bills and supports better budgeting because penalty risk is understood and controlled rather than left to chance.<br><br> | **Savings = (Breach count √ó penalty rate)** using your breach data. | High-cost leaders typically overlap with breach-risk zones. |
""",
                "performance": f"""
| Recommendation | Explanation (Phase) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| **Fast-track incident automation** | **Phase 1 ‚Äì Detect:** Use historical data on the top cost services to identify incident patterns that happen frequently and follow predictable steps so that they can be candidates for automation.<br><br>**Phase 2 ‚Äì Remediate:** Implement scripts or runbooks that automatically perform known fixes or checks for these recurring incidents so that resolution starts immediately when they are detected.<br><br>**Phase 3 ‚Äì Audit:** Monitor the success and rollback rates of these automations and refine them where they fail or need manual correction. | - Shortens outage duration for recurring issues because remediation starts as soon as the incident pattern is recognised.<br><br>- Reduces the variability in fix quality and speed because automated steps run the same way every time and do not depend on who is on shift.<br><br>- Allows engineers to focus on novel or complex incidents while routine issues are handled reliably by automation in the background.<br><br> | **MTTR gain √ó incident count** for affected services. | High bars imply long restores that automation will compress. |
| **Capacity right-sizing** | **Phase 1 ‚Äì Verify:** For each top cost service, check CPU, memory and network utilisation around incident times so that you can see if saturation is driving instability.<br><br>**Phase 2 ‚Äì Tune/Scale:** Where resources are consistently overused, optimise configurations or increase capacity so that the service has enough headroom to handle real workload peaks.<br><br>**Phase 3 ‚Äì Re-test:** Before peak business periods, stress test the service again to ensure that capacity changes have reduced the likelihood of new incidents. | - Prevents avoidable incidents where services fail simply because they run out of basic resources under load.<br><br>- Stabilises performance during busy periods so that users experience fewer slowdowns and fewer crashes when they need the service most.<br><br>- Helps ensure that money spent on extra capacity is targeted at services where lack of resources clearly contributes to downtime cost.<br><br> | **Benefit = minutes avoided from capacity faults √ó cost/min.** | Financial leaders often resource-bound under load. |
| **Owner accountability scorecards** | **Phase 1 ‚Äì Publish:** Create simple dashboards that show downtime cost and performance indicators for each service and its named owner so that responsibility is clearly visible.<br><br>**Phase 2 ‚Äì Review:** Hold monthly review sessions where owners of high-cost services explain their improvement plans and progress so that gaps are surfaced and support can be offered.<br><br>**Phase 3 ‚Äì Recognize/coach:** Recognise owners who achieve strong improvements and coach those who fall behind so that accountability is balanced with support. | - Builds sustained reliability improvement because owners know they will regularly discuss their service‚Äôs cost and performance in front of peers and leaders.<br><br>- Makes it easier for leadership to understand where ownership is strong and where additional coaching or resources are needed.<br><br>- Encourages a culture where teams are proud of improving their services and are motivated to keep cost and incident trends moving in the right direction.<br><br> | **Run-rate Œî RM** month-over-month on top services. | Persistent tall bars reveal ownership gaps. |
| **Change hygiene for costly services** | **Phase 1 ‚Äì Gate:** For the highest cost services, require additional technical and business review steps before approving changes so that risky releases are examined more carefully.<br><br>**Phase 2 ‚Äì Canary/rollback:** Implement canary deployments and clear rollback plans for these services so that any negative impact from a change is quickly discovered and reversed.<br><br>**Phase 3 ‚Äì Post-release audit:** After each major release, check incident and performance data to confirm whether the change improved or harmed stability and adjust practices accordingly. | - Lowers the chance that new releases will cause outages in services that already generate significant downtime cost.<br><br>- Ensures that when changes do cause problems, impact is limited and can be rolled back quickly so that users experience shorter disruptions.<br><br>- Makes the release process more disciplined and predictable, which builds trust between development, operations and business stakeholders.<br><br> | **Incidents avoided √ó MTTR** for change-linked periods. | Many peaks follow release windows. |
| **RTO tuning** | **Phase 1 ‚Äì Baseline:** Measure the real mean time to restore (MTTR) for top cost services and compare it against the stated recovery time objectives so that you know the gap between promise and reality.<br><br>**Phase 2 ‚Äì Close:** Introduce tools, automation and process changes to bring actual restoration times closer to the target so that users experience faster recovery during incidents.<br><br>**Phase 3 ‚Äì Re-baseline:** After several months, repeat the measurement and adjust targets if they are unrealistic or if higher ambitions are now achievable. | - Reduces the probability that incidents will exceed expected downtime windows and cause contractual or reputational damage.<br><br>- Improves user experience because services come back online more quickly and consistently when failures occur.<br><br>- Helps the organisation negotiate realistic but ambitious recovery targets that reflect actual capabilities rather than historical habits.<br><br> | **Benefit = (Old ‚àí New restore mins) √ó cost/min** for top services. | High-cost cluster suggests RTO gaps. |
""",
                "satisfaction": f"""
| Recommendation | Explanation (Phase) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| **Stakeholder briefings for top services** | **Phase 1 ‚Äì Explain:** Regularly brief key stakeholders on the risks and status of the top cost services using clear, non-technical language so that they understand what is at stake.<br><br>**Phase 2 ‚Äì Plan:** Share the mitigation and improvement plans for these services along with timelines so that stakeholders know what will change and when.<br><br>**Phase 3 ‚Äì Track:** Monitor customer satisfaction and ticket deflection for these services so that you can see whether the briefings and actions are improving the experience. | - Builds trust because business users see that the most critical and expensive services are receiving focused attention and clear planning.<br><br>- Reduces escalations and conflict because stakeholders are regularly updated and do not feel ignored or surprised when issues occur.<br><br>- Aligns expectations with reality so that business planning takes into account the known risks and improvement timelines for key services.<br><br> | **CSAT uplift** among affected users vs baseline; **deflection √ó handling cost**. | Top bars are most user-visible pain points. |
| **Premium uptime options** | **Phase 1 ‚Äì Offer:** Develop higher-tier SLA options for customers who run critical workloads so that they can choose stronger protection levels if they need them.<br><br>**Phase 2 ‚Äì Price:** Price these options based on the additional risk, capacity and operational effort required so that trade-offs are clear and sustainable.<br><br>**Phase 3 ‚Äì Monitor:** Track experience, satisfaction and retention for customers on premium tiers to ensure the offerings deliver real value. | - Allows customers with critical operations to secure enhanced support and uptime guarantees that match their business needs.<br><br>- Can improve retention by giving high-value clients a structured way to get more protection instead of feeling they are treated the same as low-impact users.<br><br>- Makes the trade-off between cost and reliability explicit, which helps both sides have more constructive discussions about service levels and pricing.<br><br> | **Retention value** from reduced churn for premium tier. | Concentrated cost indicates where premium protection matters. |
| **Status page with service granularity** | **Phase 1 ‚Äì Publish:** Create a status page that shows the health of individual services so that users can quickly see what is affected without contacting support.<br><br>**Phase 2 ‚Äì Timeline:** Include simple incident histories and planned maintenance windows per service so that patterns and expectations are transparent.<br><br>**Phase 3 ‚Äì RCA:** Link short root cause notes for major incidents so that users can see how problems are being addressed. | - Reduces the number of support interactions needed just to find out whether a service is down or degraded because users can self-check status.<br><br>- Improves the sense of control and reliability because the environment is not a black box and issues are openly visible.<br><br>- Fosters more mature conversations with customers because both sides share the same factual view of service health and history.<br><br> | **Deflection √ó handling cost** reduction after launch. | Rank chart guides which services to surface first. |
| **Workaround libraries** | **Phase 1 ‚Äì Produce:** For the highest cost services, create short, practical workaround documents that explain how users can keep working when these services are partially unavailable.<br><br>**Phase 2 ‚Äì Train:** Provide these guides to Level 1 support and key users so that they can apply them quickly when incidents occur.<br><br>**Phase 3 ‚Äì Measure:** Track how often workarounds are used and how they affect user productivity and ticket volumes. | - Keeps users productive even when services are suffering problems because they have clear, pre-tested alternative ways of working.<br><br>- Reduces pressure on higher-level support teams because many common issues can be handled using documented steps at Level 1 or by users themselves.<br><br>- Improves user satisfaction as they feel supported with practical guidance instead of simply being told to wait for a fix.<br><br> | **Deflected tickets √ó handling cost** on those services. | High bars justify priority content. |
| **Recognition after improvements** | **Phase 1 ‚Äì Announce:** When a previously high-cost service shows sustained cost reductions, communicate this improvement and show before-and-after trends so that users see the progress.<br><br>**Phase 2 ‚Äì Credit:** Publicly recognise the teams and owners who drove the improvements so that positive behaviour is highlighted and rewarded.<br><br>**Phase 3 ‚Äì Replicate:** Capture the practices that led to success and apply them to other services where similar gains are possible. | - Improves perception of the IT organisation because users can see real examples where reliability and cost have improved over time.<br><br>- Motivates teams to continue investing in reliability work because they see that their efforts are visible and valued by leadership and customers.<br><br>- Encourages re-use of good practices across services so that more parts of the portfolio benefit from proven approaches.<br><br> | **CSAT trend** improvement post-communication. | Visible post-action bar compression validates success. |
"""
            }

            render_cio_tables("CIO ‚Äì Business Impact", cio_5b)
        else:
            st.warning(f"‚ö†Ô∏è Missing required columns: {required - set(df.columns)}")

    # ============================================================
    # 5c. Root Cause Analysis
    # ============================================================
    with st.expander("üìå Root Cause Analysis for Major Incidents"):
        required = {"root_cause", "incident_count", "estimated_cost_downtime"}
        if required.issubset(df.columns):
            rc = (
                df.groupby("root_cause", as_index=False)
                .agg(total_incidents=("incident_count", "sum"),
                     total_cost=("estimated_cost_downtime", "sum"))
                .sort_values("total_cost", ascending=False)
            )

            fig = px.bar(
                rc,
                x="root_cause",
                y="total_cost",
                text="total_cost",
                title="Incident Cost by Root Cause",
                labels={"root_cause": "Root Cause", "total_cost": "Total Downtime Cost (RM)"},
                color_discrete_sequence=MES_BLUE,
                template="plotly_white",
            )
            fig.update_traces(texttemplate="RM %{text:,.0f}", textposition="outside")
            st.plotly_chart(fig, use_container_width=True, key=unique_key("root_cause_chart"))

            peak = rc.iloc[0]
            avg_rc_cost = rc["total_cost"].mean()
            total_rc_cost = rc["total_cost"].sum()

            st.markdown("### Analysis ‚Äî Root Cause")
            st.write(
f"""**What this graph is:** A **bar chart** ranking **total downtime cost (RM)** by **root cause** to expose systemic drivers.  
- **X-axis:** Root cause category.  
- **Y-axis:** Aggregated downtime cost (RM).

**What it shows in your data:**  
- **Top cost driver:** **{peak['root_cause']}** with **RM {peak['total_cost']:,.0f}**.  
- **Average cost per cause:** **RM {avg_rc_cost:,.0f}**; **Total cost across causes:** **RM {total_rc_cost:,.0f}**.  
- A small number of causes contribute disproportionately to financial impact.

**How to read it operationally:**  
1) **Top cause focus:** Run deep-dive RCA and implement design/config changes first on **{peak['root_cause']}**.  
2) **Middle cluster:** Standardize fixes across mid-tier causes to avoid fragmentation.  
3) **Long tail:** Monitor, but avoid over-engineering‚Äîprove recurrence before investing.

**Why this matters:** Fixing a few systemic drivers removes **repeat incident run-rate**, lowering costs and stabilizing user experience portfolio-wide."""
            )

            # ---------------- CIO table 5c ----------------
            cio_5c = {
                "cost": f"""
| Recommendation | Explanation (Phase) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| **Eradicate top root cause: {peak['root_cause']}** | **Phase 1 ‚Äì RCA:** Run a focused root cause analysis workshop, ideally within a short time window, to map out sub-causes of **{peak['root_cause']}**, the minutes of downtime each sub-cause drives and how frequently they repeat so that you see the full picture of this driver.<br><br>**Phase 2 ‚Äì Engineer:** Based on the RCA, implement targeted changes such as patches, configuration adjustments or redesign of weak components so that the technical conditions that cause this root issue are removed or greatly reduced.<br><br>**Phase 3 ‚Äì Verify:** In the next reporting cycle, check that **{peak['root_cause']}** does not recur or recurs at a much lower level so that you can confirm the financial and operational benefits of the remediation. | - Removes the largest single stream of repeat downtime cost so that overall financial exposure drops significantly over time.<br><br>- Lowers on-call and support workload because incidents linked to this root cause stop appearing regularly on dashboards and queues.<br><br>- Stabilises reliability trends and makes the environment easier to manage because a major systemic weakness is no longer generating unexpected disruptions.<br><br> | **Upper bound savings = RM {peak['total_cost']:,.0f}** (dataset for this cause). | Highest bar is **{peak['root_cause']}** at **RM {peak['total_cost']:,.0f}**. |
| **Preventive health checks** | **Phase 1 ‚Äì Instrument:** Add monitoring, alerts and telemetry that are designed specifically to detect early symptoms related to your top root causes so that you can act before full incidents happen.<br><br>**Phase 2 ‚Äì Threshold:** Set thresholds that trigger alerts before performance fully degrades into an outage so that there is enough time to intervene with smaller corrective actions.<br><br>**Phase 3 ‚Äì Tune:** Review these thresholds and their effectiveness quarterly so that alerts stay meaningful and are aligned with how systems actually behave. | - Converts large, unplanned outages into smaller, controlled interventions that can be scheduled at better times or handled with minimal user impact.<br><br>- Reduces the total number of incidents related to top root causes because issues are caught earlier when the fix is simpler and less disruptive.<br><br>- Improves the quality of monitoring by making it tightly tied to real business and technical risks rather than generic system checks.<br><br> | **Avoided minutes √ó cost/min** for top-cause windows. | Concentration across a few causes implies high ROI monitoring. |
| **Standard fix kits** | **Phase 1 ‚Äì SOP:** Create clear, step-by-step playbooks that describe how to diagnose and fix incidents driven by each major root cause so that engineers have a consistent approach to resolving them.<br><br>**Phase 2 ‚Äì Train:** Run drills and training sessions for Level 1 and Level 2 support teams so that they are comfortable using these playbooks in real incidents.<br><br>**Phase 3 ‚Äì Audit:** Review recurrence trends and fix quality to refine the playbooks so that they evolve as systems and behaviours change. | - Makes incident handling faster and more consistent because staff can follow proven instructions instead of improvising every time.<br><br>- Reduces the number of escalations to senior engineers because lower-level teams can handle a larger proportion of incidents using the documented fix kits.<br><br>- Improves handover quality and reduces knowledge loss when staff change roles because critical know-how is captured in reusable documents.<br><br> | **Repair time saved √ó incident count** for those causes. | Repetition confirms reusability of SOPs. |
| **Supplier/contract fixes** | **Phase 1 ‚Äì Engage:** For root causes that point to vendor-managed components, engage suppliers with clear evidence of incident and cost impact so that they understand the severity of the problem.<br><br>**Phase 2 ‚Äì Commit:** Agree on concrete response actions such as firmware updates, hardware replacements or patch delivery timelines so that responsibilities and expectations are explicit.<br><br>**Phase 3 ‚Äì Enforce:** Make sure penalty clauses or service credits are applied when commitments are missed so that there is a financial incentive for timely remediation. | - Shifts some of the cost and responsibility for recurring issues onto suppliers whose products or services contribute to incidents.<br><br>- Accelerates fixes for vendor-related problems because there is a clear agreement on actions and timelines supported by contractual obligations.<br><br>- Prevents internal teams from spending too much time compensating for external weaknesses that should be addressed by the supplier.<br><br> | **Penalty savings** from breach reductions tied to vendor domains. | Several causes map to vendor components. |
| **Kill noisy alarms** | **Phase 1 ‚Äì Clean:** Identify alarms and alerts that do not lead to any meaningful action and retire or consolidate them so that they no longer distract engineers during actual incidents.<br><br>**Phase 2 ‚Äì Correlate:** Group related alerts under single incidents or service-level symptoms so that teams see the big picture rather than thousands of tiny signals.<br><br>**Phase 3 ‚Äì Review:** Revisit alarm quality each month to ensure new noise has not crept in and that meaningful alarms continue to stand out. | - Reduces the volume of alerts that engineers must review, which lowers cognitive load and helps them focus on real issues that need attention.<br><br>- Improves response quality because teams are less likely to miss important alerts buried inside a flood of noisy notifications.<br><br>- Speeds up detection and triage of serious incidents because the monitoring environment is cleaner and more aligned with real business impact.<br><br> | **Tickets avoided √ó handling cost** after de-noising. | Alert storms inflate incident volume and costs. |
""",
                "performance": f"""
| Recommendation | Explanation (Phase) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| **RCA after every major** | **Phase 1 ‚Äì Within 48h:** After any major incident, perform a structured RCA within a fixed short timeframe so that memories are fresh and evidence is easy to capture in detail.<br><br>**Phase 2 ‚Äì Track:** Assign each identified action to a clear owner and due date so that follow-through is monitored instead of forgotten once the incident is closed.<br><br>**Phase 3 ‚Äì Close:** Revisit the actions and confirm that they have been implemented and have reduced the likelihood or impact of similar incidents. | - Builds a continuous improvement loop where every major event contributes directly to stronger systems and processes.<br><br>- Reduces repeat incidents because lessons learned are turned into concrete actions rather than staying as undocumented discussions.<br><br>- Shortens incident duration over time because recurring failure patterns are systematically addressed and prevented.<br><br> | **ŒîMTTR and Œîincident rate** post-action vs baseline. | Concentrated causes indicate learnable patterns. |
| **Chaos drills for top causes** | **Phase 1 ‚Äì Simulate:** Organise controlled game-day exercises that simulate failures related to **{peak['root_cause']}** so that teams can practice detecting and handling them in a safe environment.<br><br>**Phase 2 ‚Äì Validate:** Use these drills to test runbooks, tools and communication channels to see whether they work as expected when pressure is applied.<br><br>**Phase 3 ‚Äì Fix:** Address any gaps discovered during the drills and repeat them periodically so that the organisation becomes more confident in its ability to handle real incidents. | - Improves the precision and speed of incident response because teams have rehearsed what to do rather than figuring everything out during a live crisis.<br><br>- Reduces the variability in recovery performance because procedures and tools are proven and refined through realistic practice.<br><br>- Increases confidence among engineers and stakeholders that the environment can withstand serious issues without catastrophic impact.<br><br> | **Minutes saved/major √ó frequency** of those causes. | High-cost cause warrants rehearsal priority. |
| **Observability enrichment** | **Phase 1 ‚Äì Add:** Enhance logging, metrics and traces so that incidents linked to major root causes have clear technical indicators and can be diagnosed quickly.<br><br>**Phase 2 ‚Äì SLO alerts:** Tie these indicators to service level objectives and error budget alerts so that breaches are detected early and automatically.<br><br>**Phase 3 ‚Äì Tune:** Adjust thresholds and signals over time so that alerts remain accurate and relevant as the system evolves. | - Shortens the time spent hunting for the root of a problem because key data is already collected and easy to interpret.<br><br>- Improves the quality of troubleshooting, which reduces trial-and-error and lowers the risk of making incorrect changes during incidents.<br><br>- Enables proactive detection of issues before they escalate to full outages when error budgets are approached or breached.<br><br> | **Detection time‚Üì √ó incident count** for those causes. | High bars imply detection lag historically. |
| **Ownership clarity** | **Phase 1 ‚Äì Assign:** Allocate each major root cause to a clearly identified owner or owning team so that there is no confusion about responsibility during incidents or improvement work.<br><br>**Phase 2 ‚Äì Scorecard:** Track progress, recurrence and cost associated with each owned cause in simple scorecards so that performance is visible over time.<br><br>**Phase 3 ‚Äì Coach:** Offer support or coaching to owners whose causes continue to generate high incident volume or cost so that they can improve their approach. | - Reduces the number of tickets that get stuck or bounced around because it is obvious who should handle issues related to each cause.<br><br>- Increases accountability and transparency because each owner can see how their actions affect incident trends and cost over time.<br><br>- Makes it easier for leaders to direct resources such as time, training or tooling improvements where they will have the most impact.<br><br> | **Reassignment cuts √ó delay cost** after RACI clarity. | Cause clusters expose process gaps. |
| **Change guardrails** | **Phase 1 ‚Äì Extra reviews:** Put additional review and test requirements on changes that affect components linked to major root causes so that the risk of triggering new incidents is reduced.<br><br>**Phase 2 ‚Äì Canary/rollback:** Deploy changes in small stages with quick rollback options for these high-risk areas so that any negative effects can be limited and reversed.*<br><br>**Phase 3 ‚Äì Audit:** Regularly audit exceptions to these guardrails to make sure they are rare and justified, and that they do not compromise system stability. | - Reduces the frequency of incidents that are directly caused by changes touching fragile components in the system.<br><br>- Limits the blast radius of mistakes so that if a change does introduce a problem, it can be contained quickly and with less user impact.<br><br>- Strengthens the overall release discipline and encourages teams to think carefully about how their changes interact with known risk areas.<br><br> | **Incidents avoided √ó MTTR** related to change windows. | Root causes often flare during changes. |
""",
                "satisfaction": f"""
| Recommendation | Explanation (Phase) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| **Share RCA outcomes** | **Phase 1 ‚Äì Summarize:** After significant incidents, write a clear non-technical summary of the root cause and contributing factors so that business users can understand what went wrong.<br><br>**Phase 2 ‚Äì Commit:** Communicate the planned fixes, timelines and owners so that users know what you are doing to prevent similar issues in the future.<br><br>**Phase 3 ‚Äì Update:** Provide follow-up updates when fixes are implemented and when results are visible in incident trends. | - Builds trust because users can see that you are open about problems and serious about addressing them rather than hiding details.<br><br>- Lowers the number of chaser calls and emails because stakeholders understand both the cause and the plan without needing repeated explanations.<br><br>- Improves perception of control and professionalism because issues are handled in a structured and transparent way from cause to remediation.<br><br> | **Complaints avoided √ó handling cost** post-RCA sharing. | Users care that the **top driver** is being fixed first. |
| **User-friendly workarounds** | **Phase 1 ‚Äì Publish:** For the most impactful root causes such as **{peak['root_cause']}**, develop user-facing guidance with simple and practical steps that people can follow while a permanent fix is in progress.<br><br>**Phase 2 ‚Äì Surface:** Make this guidance easy to find within portals, FAQs and chatbots so that users can quickly see what options they have when issues appear.<br><br>**Phase 3 ‚Äì Measure:** Track how often these workarounds are used and how they influence ticket volume so that you know which ones deliver real value. | - Helps users stay productive during periods when underlying technical issues have not yet been completely resolved.<br><br>- Reduces demand on front-line support because many common questions can be answered by following the published guidance.<br><br>- Shows users that you are proactively taking steps to mitigate impact rather than leaving them without options while long-term fixes are being developed.<br><br> | **Deflected tickets √ó handling cost** attributable to that cause. | Largest cause benefits most from guided workarounds. |
| **Expectation setting** | **Phase 1 ‚Äì Clocks:** Show visible SLA or ETA timers for open incidents tied to major root causes so that users know how long resolution is expected to take.<br><br>**Phase 2 ‚Äì Updates:** Provide progress updates as work moves through investigation, fix and validation so that users can see that their incidents are not stalled.<br><br>**Phase 3 ‚Äì Survey:** After restoration, collect feedback on whether communication was clear and helpful so that expectations can be improved next time. | - Reduces anxiety and confusion because users can track progress and understand when their issues are likely to be resolved.<br><br>- Lowers the number of escalations that happen purely due to lack of communication rather than actual technical delay.<br><br>- Gives insight into how communication style and timing affect satisfaction so that you can tune your messaging approach for future incidents.<br><br> | **Escalations avoided √ó cost** during high-impact events. | High-impact causes are user-visible pain points. |
| **VIP communications** | **Phase 1 ‚Äì Notify:** Proactively contact VIP customers or key business units when incidents related to major root causes occur so that they hear about the situation directly from you first.<br><br>**Phase 2 ‚Äì SME hotline:** Offer a direct line to subject matter experts for VIPs during these events so that their most critical questions can be answered quickly and accurately.<br><br>**Phase 3 ‚Äì Review:** After the incident, review VIP feedback to understand whether the extra communication and access reduced their risk and frustration. | - Protects critical relationships with high-value customers by ensuring they feel prioritised and well-informed during difficult incidents.<br><br>- Reduces the likelihood of sudden contract cancellations or severe escalations from these customers because they feel supported even when the service is struggling.<br><br>- Creates a reputation for strong customer care among the most important segments of your user base.<br><br> | **Retention value** for VIPs tied to those causes. | Top causes correlate with VIP disruption. |
| **Celebrate elimination** | **Phase 1 ‚Äì Announce:** When a major root cause has shown zero recurrence for a meaningful period, communicate this success and show how incident trends have changed so that users can see real progress.<br><br>**Phase 2 ‚Äì Credit:** Acknowledge the teams and individuals who worked on the fix so that their contributions are visible across the organisation.<br><br>**Phase 3 ‚Äì Replicate:** Capture the approach used and apply it to other similar causes so that the success is repeated across the portfolio. | - Builds a positive narrative around reliability work and shows that investment in root cause fixes leads to tangible, measurable results.<br><br>- Strengthens morale in engineering and operations teams because their hard work in improving stability is recognised publicly.<br><br>- Encourages broader adoption of effective methods because other teams can see which strategies led to successful elimination of high-impact causes.<br><br> | **CSAT trend uplift** around announcements. | Post-action bar shrinkage validates gains. |
"""
            }

            render_cio_tables("CIO ‚Äì Root Cause Analysis", cio_5c)
        else:
            st.warning(f"‚ö†Ô∏è Missing required columns: {required - set(df.columns)}")

    # ============================================================
    # 5d. Preventive Actions
    # ============================================================
    with st.expander("üìå Actions Taken to Prevent Recurring Incidents"):
        required = {"root_cause", "improvement_action", "incident_count"}
        if required.issubset(df.columns):
            action_df = (
                df.groupby(["root_cause", "improvement_action"], as_index=False)
                .agg(total_incidents=("incident_count", "sum"))
            )

            fig = px.treemap(
                action_df,
                path=["root_cause", "improvement_action"],
                values="total_incidents",
                title="Preventive Actions Implemented per Root Cause",
                color_discrete_sequence=MES_BLUE,
                template="plotly_white",
            )
            st.plotly_chart(fig, use_container_width=True, key=unique_key("preventive_action_chart"))

            top_row = action_df.iloc[action_df["total_incidents"].idxmax()] if not action_df.empty else None
            total_actions = int(action_df["total_incidents"].sum()) if not action_df.empty else 0
            avg_actions = float(action_df["total_incidents"].mean()) if not action_df.empty else 0.0

            st.markdown("### Analysis ‚Äî Preventive Measures")
            if top_row is not None:
                st.write(
f"""**What this graph is:** A **treemap** showing the **volume of preventive actions** grouped by **root cause ‚Üí improvement action**.  
- **Blocks:** Action categories; **size:** number of incidents addressed by that action.

**What it shows in your data:**  
- **Most applied action:** **{top_row['improvement_action']}** under **{top_row['root_cause']}** (covers **{int(top_row['total_incidents'])} incidents**).  
- **Total actions mapped:** **{total_actions}** across all causes; **Average per (cause, action)**: **{avg_actions:.1f}**.

**How to read it operationally:**  
1) **Large blocks:** Standardize and scale these actions‚Äîprove they work, then make them default.  
2) **Tiny, scattered blocks:** Consolidate or retire low-impact actions to avoid fragmentation.  
3) **Cause coverage gaps:** Where causes have few actions, invest in new playbooks and monitoring.

**Why this matters:** A disciplined prevention program converts **unplanned outages into planned, short interventions**. Scaling effective actions lowers incident run-rate, MTTR, and customer pain."""
                )
            else:
                st.info("No preventive action records available for a detailed narrative.")

            top_action = top_row['improvement_action'] if top_row is not None else "N/A"
            top_cause = top_row['root_cause'] if top_row is not None else "N/A"
            top_cov = int(top_row['total_incidents']) if top_row is not None else 0

            # ---------------- CIO table 5d ----------------
            cio_5d = {
                "cost": f"""
| Recommendation | Explanation (Phase) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| **Scale the most effective action** | **Phase 1 ‚Äì Validate:** Confirm that the largest block in the treemap, which is **{top_action}** under **{top_cause}** covering **{top_cov}** incidents, is genuinely reducing incident frequency or impact so that you are not scaling a weak solution.<br><br>**Phase 2 ‚Äì Rollout:** Once validated, make this action the default response for all incidents with matching signatures so that more of the environment benefits from a proven intervention.<br><br>**Phase 3 ‚Äì Track:** Each month, measure cost and incident trends for the areas where the action is now used so that you can quantify financial savings and adjust the rollout plan. | - Focuses investment on actions that already show strong evidence of working, which increases the chance of achieving visible cost reduction quickly.<br><br>- Turns isolated good practices into standard operating procedures so that improvements are repeated instead of staying limited to a few teams or services.<br><br>- Gives leadership concrete data on how a particular action drives financial and operational gains, which supports further funding for similar initiatives.<br><br> | **Run-rate cut = (Incidents affected √ó handling cost)** using **{top_cov}** as coverage anchor. | Largest treemap block shows proven impact. |
| **Retire low-impact actions** | **Phase 1 ‚Äì Identify:** Find actions that address fewer incidents than the average of **{avg_actions:.1f}** so that you can see which activities deliver relatively little coverage.<br><br>**Phase 2 ‚Äì Merge/retire:** Consolidate overlapping actions or retire those that show minimal or no measurable impact so that effort is not wasted on complex but ineffective steps.<br><br>**Phase 3 ‚Äì Reinvest:** Redirect time and budget from these low-impact actions into strengthening and scaling higher-ROI actions that have better evidence and coverage. | - Stops money and effort being tied up in activities that do not meaningfully change incident outcomes or cost for the business.<br><br>- Simplifies playbook libraries and processes, making it easier for staff to pick effective responses quickly instead of navigating many similar options.<br><br>- Increases the overall value of prevention work because more of the resource is concentrated on actions that actually reduce incidents and downtime cost.<br><br> | **Savings = (Retired actions √ó avg action cost)** estimated from sub-mean actions. | Many tiny blocks signal fragmentation/low ROI. |
| **Preventive maintenance calendar** | **Phase 1 ‚Äì Schedule:** Use the treemap and incident history to schedule preventive actions for high-risk causes into controlled maintenance windows so that interventions are planned rather than reactive.<br><br>**Phase 2 ‚Äì Control:** During these windows, apply pre and post checks to verify that maintenance does not introduce new problems and that risk is actually reduced.<br><br>**Phase 3 ‚Äì Review:** On a quarterly basis, review whether the calendarised actions are lowering incident volume and cost for the targeted causes. | - Shifts work away from emergency fixes into predictable, planned activities that are easier and cheaper to manage.<br><br>- Reduces overtime and last-minute firefighting because many issues are dealt with before they evolve into full-blown incidents.<br><br>- Gives business stakeholders more visibility and predictability around when risk is being actively managed and when small planned disruptions may occur.<br><br> | **Avoided minutes √ó cost/min** for targeted causes during calendarized windows. | Treemap highlights which causes/actions to prioritize. |
| **Central action library** | **Phase 1 ‚Äì Document:** Create a single, well-organised library of preventive SOPs and link each action to evidence from the data so that teams can see which actions work best for each cause.<br><br>**Phase 2 ‚Äì Train:** Train operations and support teams to use this library as their primary reference so that they know where to find the correct action for a given scenario.<br><br>**Phase 3 ‚Äì Audit:** Periodically review how often actions are reused and whether steps are being followed, and update the library to reflect new learning. | - Speeds up the delivery of preventive actions because staff have a clear, central place to find the right steps and do not need to reinvent approaches.<br><br>- Reduces inconsistency between teams and shifts because everyone is working from the same documented playbooks rather than personal memory.<br><br>- Preserves institutional knowledge as staff come and go because documented actions and their evidence remain accessible to new team members.<br><br> | **Time saved = (Rework avoided √ó incidents)** leveraging repeated actions. | Repeated actions across causes indicate reuse potential. |
| **ROI gating for new actions** | **Phase 1 ‚Äì Define:** Establish a simple minimum ROI threshold or success metric that any new preventive action should meet before it is scaled widely so that expectations are clear.<br><br>**Phase 2 ‚Äì Pilot:** Test new actions on a small, controlled set of incidents or services, and measure their effect on incident volume and cost so that decisions are based on data.<br><br>**Phase 3 ‚Äì Scale:** Only roll out actions that meet or exceed the threshold and retire or redesign those that do not, so that the library remains lean and effective. | - Keeps the prevention programme focused on delivering real improvements rather than accumulating many unproven or cosmetic actions.<br><br>- Encourages an experimental mindset where teams try new ideas but are required to measure results before standardising them across the organisation.<br><br>- Prevents process and playbook bloat, which makes it easier for staff to navigate and choose from a set of genuinely useful actions.<br><br> | **Benefit = (Low-ROI spend avoided)** vs prior cycle. | Action volumes enable apples-to-apples ROI comparisons. |
""",
                "performance": f"""
| Recommendation | Explanation (Phase) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| **Continuous improvement loop** | **Phase 1 ‚Äì Quarterly:** Every quarter, review the performance of preventive actions by looking at how they changed incident counts and MTTR so that successes and failures are visible.<br><br>**Phase 2 ‚Äì Update:** Use this information to refine SOPs, retire weak actions and improve strong ones so that the library stays aligned with real-world behaviour.<br><br>**Phase 3 ‚Äì Retrain:** Share the updated playbooks and train the teams again so that everyone is aware of the latest best practices. | - Keeps performance gains alive instead of letting them fade as systems, workloads and teams change over time.<br><br>- Ensures that preventive actions stay relevant and effective by being updated in response to actual data rather than assumptions.<br><br>- Creates a culture where preventive work is an ongoing cycle rather than a one-time project, which supports long-term reliability improvements.<br><br> | **Œîincident rate** and **ŒîMTTR** after each cycle. | Treemap consolidation over time reflects maturing practice. |
| **Coverage mapping by cause** | **Phase 1 ‚Äì Gap scan:** Review the treemap to identify root causes with very few associated preventive actions so that you can spot areas with weak coverage.<br><br>**Phase 2 ‚Äì Design:** Create new playbooks or enhance existing ones for these under-served causes so that future incidents can be prevented or reduced more effectively.<br><br>**Phase 3 ‚Äì Rollout:** Deploy these new actions and track adoption and impact so that you know if the coverage gap is closing. | - Reduces blind spots where certain causes continue to generate incidents simply because there are no strong preventive actions in place.<br><br>- Helps balance the reliability posture across services and causes so that no single area lags behind due to lack of attention.<br><br>- Supports more consistent performance improvements across the portfolio instead of having only a few well-protected causes and many neglected ones.<br><br> | **Incidents prevented** in previously under-served causes. | Sparse blocks reveal low-coverage causes. |
| **Instrumentation before action** | **Phase 1 ‚Äì Add:** Before rolling out significant preventive actions, add or refine monitoring and logging where those actions will apply so that their impact can be measured.<br><br>**Phase 2 ‚Äì Thresholds:** Set meaningful SLO-based thresholds and alerts tied to these metrics so that early signs of failure or degradation trigger preventive action at the right time.<br><br>**Phase 3 ‚Äì Validate:** After actions are applied, confirm that the signals accurately reflect real issues and adjust them as needed to improve quality. | - Makes preventive actions more effective because they are activated or evaluated based on accurate and timely operational data.<br><br>- Reduces guesswork and subjective judgments about whether an action helped or not because metrics provide evidence of change.<br><br>- Enables more efficient use of engineering time because focus stays on actions that show measurable impact on the monitored signals.<br><br> | **Detection time‚Üì √ó affected incidents** for those actions. | Actions work better with good observability. |
| **Owner assignment per action** | **Phase 1 ‚Äì Assign:** Assign a named owner for each significant preventive action or action family so that someone is clearly accountable for its maintenance and effectiveness.<br><br>**Phase 2 ‚Äì Scorecards:** Track deadlines, adoption rates and incident trends related to each action in simple scorecards to keep progress visible.<br><br>**Phase 3 ‚Äì Coach:** Provide additional support and guidance to owners whose actions are not yielding the expected improvements so that they can refine their approach. | - Reduces delays and confusion in implementing preventive actions because someone is explicitly responsible for moving each action forward.<br><br>- Makes it easier to identify which actions are falling behind and need attention or redesign so that effort is not wasted on ineffective work.<br><br>- Encourages stronger ownership behaviours where individuals and teams feel accountable for the preventive health of their areas.<br><br> | **Cycle time‚Üì** from owner clarity vs prior cycles. | Diverse actions require clear ownership to land. |
| **Playbook A/B testing** | **Phase 1 ‚Äì Trial:** For common incident types, test two different preventive approaches or playbook versions on similar incident groups so that results can be compared fairly.<br><br>**Phase 2 ‚Äì Select:** Analyse which version delivers better reductions in incident rate or MTTR and choose that version as the standard for future use.<br><br>**Phase 3 ‚Äì Standardize:** Roll out the winning playbook across all relevant services and retire the weaker variant so that everyone benefits from the best approach. | - Uses data rather than opinion to decide which preventive methods work best, which makes the programme more effective.<br><br>- Avoids locking into weak approaches simply because they were the first ones implemented or are familiar to certain teams.<br><br>- Progressively improves preventive actions over time as better variants replace weaker ones across the portfolio.<br><br> | **Minutes saved/incident √ó volume** for the winning variant. | Treemap co-horts provide test beds. |
""",
                "satisfaction": f"""
| Recommendation | Explanation (Phase) | Benefits | Cost Calculation | Evidence & Graph Interpretation |
|---|---|---|---|---|
| **Publish prevention roadmap** | **Phase 1 ‚Äì Plan:** Build and share a simple roadmap that shows which root causes are being targeted with which actions and when so that users know improvement is actively being pursued.<br><br>**Phase 2 ‚Äì Track:** Keep this roadmap up to date with progress markers showing which actions are in planning, in progress or completed so that status is transparent.<br><br>**Phase 3 ‚Äì Report:** Periodically report the results and how incident trends are changing so that users can see the effect of the roadmap in their daily experience. | - Increases trust because users see a concrete plan rather than hearing vague promises about future stability improvements.<br><br>- Reduces uncertainty and repetitive follow-ups because the roadmap answers many questions about what is being done and when.<br><br>- Makes it easier to secure support and patience from business stakeholders as they can see how preventive work connects to their pain points and timelines.<br><br> | **Complaints avoided √ó handling cost** compared to no-roadmap baseline. | Users see proactive steps against top causes. |
| **User validation loops** | **Phase 1 ‚Äì Pilot:** Try new preventive actions with selected user groups who regularly experience issues so that feedback is gathered from those most affected.<br><br>**Phase 2 ‚Äì Feedback:** Collect and analyse user feedback on how practical and helpful the actions were so that adjustments can be made before full rollout.<br><br>**Phase 3 ‚Äì Scale:** Expand actions that users found effective and user-friendly, and adjust or drop those that did not fit real workflows. | - Ensures that preventive actions align with how people actually work rather than being designed only from a technical point of view.<br><br>- Increases user satisfaction because people feel included in shaping the solutions that affect their daily tools and processes.<br><br>- Reduces the risk that new actions will create additional friction or unintended side effects in business workflows.<br><br> | **CSAT uplift** in pilot cohorts vs control. | Actions tied to real user pain move the needle fastest. |
| **Self-help content for top causes** | **Phase 1 ‚Äì Author:** Create clear, step-by-step self-help content for top-root-causes such as **{top_cause}**, where **{top_cov}** incidents are involved, so that users have ready guidance for the most common problems.<br><br>**Phase 2 ‚Äì Surface:** Make this content prominent on portals and in chatbots so that users can quickly access it at the moment they face an issue.<br><br>**Phase 3 ‚Äì Measure:** Measure how many tickets are deflected or shortened because users followed the content before or during their interaction with support. | - Keeps users operational even when underlying issues are not fully resolved because they can apply practical workarounds or checks on their own.<br><br>- Reduces load on support teams as many basic questions can be answered or partially resolved through self-help content rather than full tickets.<br><br>- Demonstrates responsiveness to user needs as the help provided clearly targets the most frequent and impactful causes visible in the data.<br><br> | **Deflected tickets √ó handling cost** using treemap volumes. | Largest cause benefits most from guidance. |
| **Status badges on actions** | **Phase 1 ‚Äì Label:** For each preventive action and root cause, show simple status badges such as Planned, In-Progress and Done so that users can quickly see where things stand.<br><br>**Phase 2 ‚Äì Share:** Expose these badges on dashboards or status pages that stakeholders already use so that the information is easy to find and interpret.<br><br>**Phase 3 ‚Äì Survey:** Gather feedback on whether this visibility helps users feel more informed and adjust presentation if necessary. | - Reduces anxiety and speculation because users can see at a glance which improvements are moving forward and which are still in planning.<br><br>- Reduces the number of manual status requests because the information is already available in a clear and standardised format.<br><br>- Increases perceived reliability and professionalism because preventive work is visible, trackable and part of normal communications rather than hidden in internal systems.<br><br> | **Escalations avoided √ó cost** post-visibility. | Visibility reduces uncertainty for users. |
| **Recognition of effective actions** | **Phase 1 ‚Äì Celebrate:** Highlight actions and teams that have significantly reduced incidents or cost so that people see what success looks like in prevention work.<br><br>**Phase 2 ‚Äì Replicate:** Use these success stories to promote adoption of the same actions in similar services or teams so that wins are scaled up.<br><br>**Phase 3 ‚Äì Track:** Monitor satisfaction and retention in areas where celebrated actions have been applied to understand their longer-term effect. | - Creates a positive feedback loop where teams are motivated to contribute to and improve preventive work because it is publicly recognised.<br><br>- Helps spread good practices quickly across the organisation as other teams copy the actions that are known to work well.<br><br>- Supports stronger user loyalty and confidence as they see that improvements are happening and are acknowledged at organisational level.<br><br> | **Retention value & CSAT trend** post-recognition. | Big blocks that shrink incident counts deserve visibility. |
"""
            }

            render_cio_tables("CIO ‚Äì Preventive Actions", cio_5d)
        else:
            st.warning(f"‚ö†Ô∏è Missing required columns: {required - set(df.columns)}")
