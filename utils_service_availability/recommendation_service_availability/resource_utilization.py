import streamlit as st
import plotly.express as px
import pandas as pd
import numpy as np

# ============================
# Mesiniaga visual theme
# ============================
px.defaults.template = "plotly_white"
PRIMARY_BLUE   = "#004C99"
SECONDARY_BLUE = "#007ACC"
ACCENT_BLUE_1  = "#6FA8DC"
ACCENT_BLUE_2  = "#A8C7F0"
MES_COLOR_SEQ  = [PRIMARY_BLUE, SECONDARY_BLUE, ACCENT_BLUE_1, ACCENT_BLUE_2]

# ============================================================
# Helper for CIO tables
# ============================================================
def render_cio_tables(title, cio):
    st.subheader(title)
    with st.expander(" Cost Reduction"):
        st.markdown(cio["cost"], unsafe_allow_html=True)
    with st.expander(" Performance Improvement"):
        st.markdown(cio["performance"], unsafe_allow_html=True)
    with st.expander(" Customer Satisfaction Improvement"):
        st.markdown(cio["satisfaction"], unsafe_allow_html=True)

def _to_num(x):
    return pd.to_numeric(x, errors="coerce")


# ============================================================
# Target 10 ‚Äì Resource Utilization & Scalability
# ============================================================
def resource_utilization(df: pd.DataFrame):

    # --------------------------------------------------------
    # 10a. Resource Utilization Overview (CPU/MEM/DISK/NET)
    # --------------------------------------------------------
    with st.expander("üìå Resource Utilization by Service (CPU / Memory / Disk / Network)"):
        need = ["service_name", "cpu_utilization", "memory_utilization",
                "disk_utilization", "network_utilization"]
        if not set(need).issubset(df.columns):
            st.warning(f"‚ö†Ô∏è Missing required columns: {set(need) - set(df.columns)}")
        else:
            work = df.copy()
            for c in ["cpu_utilization","memory_utilization","disk_utilization","network_utilization"]:
                work[c] = _to_num(work[c])

            df_util = work.groupby("service_name", as_index=False).agg({
                "cpu_utilization": "mean",
                "memory_utilization": "mean",
                "disk_utilization": "mean",
                "network_utilization": "mean"
            }).round(1)

            df_long = df_util.melt(id_vars="service_name",
                                   var_name="Resource",
                                   value_name="Utilization (%)")

            fig = px.bar(
                df_long,
                x="service_name",
                y="Utilization (%)",
                color="Resource",
                barmode="group",
                title="Average Resource Utilization by Service",
                color_discrete_sequence=MES_COLOR_SEQ
            )
            fig.update_traces(cliponaxis=False)
            fig.update_layout(xaxis_tickangle=-15)
            st.plotly_chart(fig, use_container_width=True)

            # Dynamic evidence values
            peaks = df_long.loc[df_long.groupby("Resource")["Utilization (%)"].idxmax()].reset_index(drop=True)
            lows  = df_long.loc[df_long.groupby("Resource")["Utilization (%)"].idxmin()].reset_index(drop=True)
            peaks_text = "; ".join([f"{r['Resource']} ‚Üí {r['service_name']} ({r['Utilization (%)']:.1f}%)" for _, r in peaks.iterrows()])
            lows_text  = "; ".join([f"{r['Resource']} ‚Üí {r['service_name']} ({r['Utilization (%)']:.1f}%)" for _, r in lows.iterrows()])
            overall_avg = df_long["Utilization (%)"].mean()

            st.markdown("### Analysis ‚Äì Average Resource Utilization")
            st.write(
f"""**What this graph is:** A **grouped bar chart** showing **average CPU, Memory, Disk, and Network utilization** for each service. 
**X-axis:** Service name. 
**Y-axis:** Average utilization (%). 
**Bars:** One per resource type for each service.

**What it shows in your data:** 
**Highest loads:** {peaks_text} 
**Lowest loads:** {lows_text} 
**Portfolio average utilization:** {overall_avg:.1f}% across all resources/services.

**How to read it operationally:** 
**Saturation risk:** Bars ‚â• 85‚Äì90% need scale/optimization before incident rates spike. 
**Over-provisioning:** Very low bars point to rightsizing candidates. 
**Bottleneck type:** CPU vs Memory pattern separates compute-bound from memory-bound services; Disk/Network bars expose I/O chokepoints.

**Why this matters:** Running hot increases outage/SLA risk; running cold wastes spend. Tuning capacity to demand improves **cost, performance, and user satisfaction**."""
            )

            # Pull concrete values to inject into tables
            # Example: most CPU-bound service, most Memory-bound service, etc.
            # (Used in "Evidence" and in cost formulas as real numbers from the chart)
            def _pick(resource, fn):
                sub = df_long[df_long["Resource"].str.lower() == resource]
                if sub.empty:
                    return "N/A", 0.0
                idx = fn(sub["Utilization (%)"].values)
                row = sub.iloc[idx]
                return row["service_name"], float(row["Utilization (%)"])

            cpu_hot_name, cpu_hot_val   = _pick("cpu_utilization",   np.argmax)
            cpu_cold_name, cpu_cold_val = _pick("cpu_utilization",   np.argmin)
            mem_hot_name, mem_hot_val   = _pick("memory_utilization",np.argmax)
            disk_hot_name, disk_hot_val = _pick("disk_utilization",  np.argmax)
            net_hot_name, net_hot_val   = _pick("network_utilization",np.argmax)

            target_low_util  = 40.0  # target lower bound for rightsizing calculus (only used to produce a numeric delta with real bar values)
            target_hot_guard = 85.0  # alert/guard threshold for hot resources

            cio_a = {
                "cost": f"""
| Recommendation | Explanation (Phased, Detailed) | Benefits (Expanded) | Cost Calculation (Uses Real Chart Values) | Evidence & Graph Interpretation (Real Values) |
|---|---|---|---|---|
| Rightsize low-utilization servers | **Phase 1 ‚Äì Identify:** Select services where the average CPU utilization is below the target of approximately {target_low_util:.0f} percent and confirm that this low usage is consistent over several reporting periods instead of being a one time anomaly.<br><br>**Phase 2 ‚Äì Action:** Work with application and infrastructure owners to reduce virtual CPU and memory tiers or to consolidate multiple lightly loaded services onto fewer hosts while making sure that functional and performance requirements remain satisfied.<br><br>**Phase 3 ‚Äì Validate:** After resizing or consolidation, continuously monitor the infrastructure bill, CPU and memory utilization, and key service level objectives such as latency and error rate to confirm that cost has decreased while reliability and performance remain stable.<br><br> | - eliminates standing waste by reducing unused capacity that is not contributing to user experience or business value.<br><br>- reduces baseline infrastructure spending while keeping service performance within the agreed thresholds so users do not experience slower responses.<br><br>- releases additional budget headroom that can be redirected into resilience work, modernization efforts, or new features instead of paying for idle resources.<br><br>- simplifies the platform footprint over time which makes operations, patching, and monitoring easier to manage.<br><br> | **Unused CPU% (example)** for **{cpu_cold_name}** = {max(0.0, target_low_util - cpu_cold_val):.1f} pp (target {target_low_util:.0f}% ‚àí actual {cpu_cold_val:.1f}%). Multiply by **provisioned capacity** and **unit cost** to quantify RM saved. | Lowest CPU bar: **{cpu_cold_name}** at **{cpu_cold_val:.1f}%**. Multiple very low bars in the chart confirm rightsizing potential: {lows_text}. |
| Targeted scale-up only where hot | **Phase 1 ‚Äì Pinpoint:** Use the grouped bar chart to highlight only those services and resources whose utilization is above the guard threshold of about {target_hot_guard:.0f} percent and confirm that these high values align with business critical workloads or peak periods.<br><br>**Phase 2 ‚Äì Optimize:** For these hot spots, first remove obvious waste such as inefficient queries, poor caching, or oversized batch jobs and only then add capacity where the business case is clear and justified.<br><br>**Phase 3 ‚Äì Recheck:** After changes, compare the new utilization bars against the guard threshold and confirm that they have moved into a safer band without shifting the bottleneck to another resource or service.<br><br> | - avoids expensive blanket upgrades across the entire environment and directs spend only to the services and resources that actually need more headroom.<br><br>- reduces the probability of outages and performance incidents on the most heavily used services because capacity and optimization are focused where risk is highest.<br><br>- improves financial discipline by linking scaling decisions directly to measured hot spots in the chart instead of to assumptions or fear of failure.<br><br>- helps maintain a better balance between cost, risk, and performance across the portfolio as demand evolves over time.<br><br> | **Hot margin (example)** for **{cpu_hot_name}** = {max(0.0, cpu_hot_val - target_hot_guard):.1f} pp (actual {cpu_hot_val:.1f}% ‚àí guard {target_hot_guard:.0f}%). Convert reduced pp to avoided incidents √ó downtime minutes √ó RM/min if you capture those. | Highest CPU bar: **{cpu_hot_name}** at **{cpu_hot_val:.1f}%**; other hot spots include: {peaks_text}. |
| Schedule heavy jobs off-peak | **Phase 1 ‚Äì Map:** Identify backup jobs, reporting tasks, large data loads, and integration runs that coincide with high CPU, Disk, or Network bars and document the time windows when they are executed today.<br><br>**Phase 2 ‚Äì Shift:** Move these heavy jobs toward off peak hours, or redesign them into smaller batches so that they no longer compete directly with interactive user traffic during business critical periods.<br><br>**Phase 3 ‚Äì Monitor:** After rescheduling, track the new peak utilization values, latency, and incident trends to confirm that peaks have flattened and performance during business hours has become more stable.<br><br> | - lowers contention between background processes and real time user requests which reduces slowdowns and timeouts during important business windows.<br><br>- delays or avoids additional hardware and cloud spend because a smoother utilization curve often delivers more capacity from the same footprint.<br><br>- makes user performance more predictable so business teams experience fewer unexplained slow periods during their normal working day.<br><br>- improves the effectiveness of monitoring and alerting because spikes are more clearly linked to genuine issues rather than scheduled batch noise.<br><br> | **Peak reduction (pp)** = pre-shift peak ‚àí post-shift peak (use the charted bars for before/after). Translate **pp** reduction into fewer breach minutes √ó RM/min. | Hot I/O patterns: **Disk {disk_hot_name}** at **{disk_hot_val:.1f}%**, **Network {net_hot_name}** at **{net_hot_val:.1f}%** indicate contention windows suitable for off-peak. |
| Consolidate underused workloads | **Phase 1 ‚Äì Pack:** Look across the services that consistently show low CPU and memory utilization and design a consolidation plan that moves these workloads onto a smaller number of hosts or instances.<br><br>**Phase 2 ‚Äì Power down:** After consolidation, decommission or power down the now redundant nodes, and ensure that monitoring, backup, and inventory systems reflect the new footprint.<br><br>**Phase 3 ‚Äì SLO check:** Continuously validate that latency, throughput, and error rate service levels remain acceptable for the consolidated workloads and adjust if any regression is detected.<br><br> | - reduces the total number of machines or instances that need to be paid for, patched, backed up, and monitored which lowers operational cost and complexity.<br><br>- improves the utilization of remaining infrastructure so more of what is paid for is actually used to deliver business value.<br><br>- simplifies capacity planning because there are fewer moving parts to track and fewer edge cases that can cause misconfigurations.<br><br>- can improve hardware efficiency and environmental impact because fewer physical resources are required to run the same logical workloads.<br><br> | **Hosts removed √ó monthly RM/host**; drive consolidation targets using the share of services under **{target_low_util:.0f}%** CPU (actual coldest: **{cpu_cold_name} {cpu_cold_val:.1f}%**). | Visible cluster of low bars (e.g., {lows_text}) supports safe consolidation. |
| Tiered storage policies | **Phase 1 ‚Äì Classify:** For each service, identify which data sets are hot and accessed frequently and which data sets are cold and only rarely read, and document these classifications in a way that storage teams can act on.<br><br>**Phase 2 ‚Äì Move:** Migrate cold or archival data from expensive high performance storage tiers to cheaper storage classes while keeping hot transaction logs, indexes, and real time analytics data on fast media.<br><br>**Phase 3 ‚Äì Audit:** On a regular schedule, review access logs and storage usage to ensure that data has been placed on the right tier and that no new cold data is quietly accumulating on premium storage.<br><br> | - lowers ongoing storage cost by matching the performance level of the storage to how the data is actually used instead of treating all data as equally critical.<br><br>- keeps critical data and indexes on fast storage which helps avoid performance bottlenecks when users interact with the system under load.<br><br>- reduces surprise spikes in storage bills because growth is better controlled through tiering policies and regular audits.<br><br>- makes it easier to justify storage investments to business stakeholders because there is a clear link between data type, storage class, and cost.<br><br> | **GB moved √ó (RM/GB_hot ‚àí RM/GB_cold)**; prioritize services with high **Disk** utilization (e.g., **{disk_hot_name} {disk_hot_val:.1f}%**). | Disk bars highlight hot datasets; **{disk_hot_name}** is a lead candidate at **{disk_hot_val:.1f}%**. |
""",
                "performance": f"""
| Recommendation | Explanation (Phased, Detailed) | Benefits (Expanded) | Cost Calculation (Uses Real Chart Values) | Evidence & Graph Interpretation (Real Values) |
|---|---|---|---|---|
| Auto-scale on thresholds | **Phase 1 ‚Äì Policy:** Define clear scaling rules so that when a resource such as CPU or memory crosses about {target_hot_guard:.0f} percent, additional capacity is added and when it returns to a steady state the extra capacity is removed gradually.<br><br>**Phase 2 ‚Äì Implement:** Configure horizontal pod autoscalers, auto scaling groups, or equivalent mechanisms per service and link them to real utilization metrics so scaling decisions are data driven rather than manual.<br><br>**Phase 3 ‚Äì Tune:** Monitor scaling events and adjust thresholds, step sizes, and cooldown periods so that the system scales quickly enough to protect performance without constantly oscillating up and down.<br><br> | - preserves performance during sudden load bursts by injecting capacity at the right time instead of waiting for manual reaction.<br><br>- reduces the number of user visible slowdowns and timeouts because services are less likely to run at or beyond their safe operating limits.<br><br>- improves resource efficiency over the long term because capacity can scale back down when traffic drops instead of remaining permanently overprovisioned.<br><br>- makes resilience less dependent on human attention, which is especially valuable during nights, weekends, or large incident storms.<br><br> | **Breaches avoided √ó avg outage min**; trigger counts tied to bars above {target_hot_guard:.0f}% (e.g., **{cpu_hot_name} {cpu_hot_val:.1f}%**). | Multiple bars ‚â• guard (e.g., {peaks_text}) indicate imminent saturation without autoscaling. |
| Resource-aware alerting | **Phase 1 ‚Äì Thresholds:** Set early warning thresholds at around eighty to eighty five percent utilization and critical thresholds at around {target_hot_guard:.0f} percent, and document clearly what actions are expected at each level.<br><br>**Phase 2 ‚Äì Playbooks:** Build straightforward runbooks that describe how engineers should respond when these thresholds trigger so that scaling, optimization, or investigation steps are consistent.<br><br>**Phase 3 ‚Äì Review:** Regularly review alerts that fired and tune thresholds or playbooks where alerts were noisy or unhelpful so that attention is focused only on meaningful signals.<br><br> | - drives earlier intervention before utilization becomes dangerous which reduces the likelihood of sudden service degradation.<br><br>- prevents alert fatigue by linking each threshold to a specific and useful action rather than creating noise with no follow through.<br><br>- improves recovery speed when problems do occur because responders have clear instructions and can act quickly instead of debating what to do.<br><br>- strengthens operational discipline and supports better on call experience because alerts feel relevant and actionable rather than random.<br><br> | **Breaches avoided √ó overage minutes**; use bar heights over thresholds to parameterize alerts. | Bars near or over the guard directly justify tighter alerting. |
| Load distribution & sharding | **Phase 1 ‚Äì Hot partitioning:** Analyze traffic and resource usage to find shards, queues, or partitions that are consistently heavier than others and document these imbalances clearly.<br><br>**Phase 2 ‚Äì Connection limits:** Apply connection limits, rebalance partitions, or adjust hashing strategies so that load is spread more evenly across nodes and no single node carries a disproportionate share.<br><br>**Phase 3 ‚Äì Validate:** After changes, measure tail latency and error rates to confirm that the distribution is more balanced and that no new hot spot has been created.<br><br> | - reduces the risk that one overloaded node will slow down or fail while others sit underused which directly improves reliability.<br><br>- improves average and tail response times because work is spread more evenly and resources are used more efficiently.<br><br>- makes capacity planning simpler because every node behaves more similarly which makes forecasting more accurate.<br><br>- reduces the number of incidents caused by random imbalance which often appear as intermittent or hard to reproduce performance issues.<br><br> | **ŒîMTTR √ó incidents** and **P95 latency drop** post-rebalance. | One service often leads per-resource peaks (e.g., **{cpu_hot_name}**, **{mem_hot_name}**). |
| Tune GC, caching, pools | **Phase 1 ‚Äì Profile:** Examine garbage collection behaviour, cache hit ratios, and thread or connection pool usage for services with high CPU or memory bars and identify patterns that cause stalls or wasted work.<br><br>**Phase 2 ‚Äì Adjust:** Change cache sizes, eviction policies, pool limits, and garbage collection parameters so that they better match real traffic patterns and remove obvious inefficiencies.<br><br>**Phase 3 ‚Äì Measure:** Monitor latency, throughput, and error budgets after tuning to confirm that performance has improved and that CPU and memory utilization have moved closer to the portfolio average of {overall_avg:.1f} percent.<br><br> | - increases throughput without necessarily adding more hardware because the system does less unnecessary work for each request.<br><br>- reduces pauses and spikes caused by garbage collection or blocking resource pools which improves user experience during peak periods.<br><br>- makes performance more predictable since tuned components behave more consistently instead of swinging between idle and overloaded states.<br><br>- often unlocks hidden capacity which delays the need for expensive infrastructure expansion projects.<br><br> | **Perf gain (%) √ó revenue/throughput**; improvements confirmed when bars fall from {cpu_hot_val:.1f}% toward average {overall_avg:.1f}%. | CPU and Memory mismatch (e.g., **{mem_hot_name} {mem_hot_val:.1f}%** vs CPU peers) shows tuning opportunity. |
| I/O pipeline optimization | **Phase 1 ‚Äì Batch:** Review how the system performs disk and network operations and introduce batching, asynchronous I/O, and smarter buffering where many small operations are creating overhead.<br><br>**Phase 2 ‚Äì Queue:** Implement robust queues with back pressure and retry behaviour so downstream systems are not overwhelmed and upstream services degrade gracefully instead of failing outright.<br><br>**Phase 3 ‚Äì Observe:** Track p95 and p99 latency, error rates, and I/O utilization after these changes to confirm that the improvements are effective and stable under real traffic conditions.<br><br> | - decreases the likelihood of bottlenecks on disk and network resources which reduces user facing slowdowns during busy times.<br><br>- helps prevent cascading failures where one overloaded component causes a chain reaction across multiple services.<br><br>- improves the resilience of key workflows that depend heavily on storage and network operations which is critical for data intensive applications.<br><br>- allows more traffic to be served from the same infrastructure footprint because each I/O pipeline becomes more efficient.<br><br> | **P95 latency drop √ó affected user minutes** in services with high Disk or Network bars. | **Disk {disk_hot_name} {disk_hot_val:.1f}%** and **Network {net_hot_name} {net_hot_val:.1f}%** indicate the I/O bound path. |
""",
                "satisfaction": f"""
| Recommendation | Explanation (Phased, Detailed) | Benefits (Expanded) | Cost Calculation (Uses Real Chart Values) | Evidence & Graph Interpretation (Real Values) |
|---|---|---|---|---|
| Pre-empt UX slowdowns | **Phase 1 ‚Äì Capacity guard:** Use utilization thresholds such as {target_hot_guard:.0f} percent to define when services are too close to saturation and treat crossing these thresholds as a user experience risk rather than just a technical number.<br><br>**Phase 2 ‚Äì Throttle and inform:** When thresholds are crossed, temporarily apply controlled throttling or rate limiting and communicate clearly to users that capacity is being protected and what they can expect.<br><br>**Phase 3 ‚Äì Normalize:** Remove throttles once utilization falls back into a healthy band and confirm that user experience metrics such as page load time and transaction success rate have stabilized.<br><br> | - reduces sudden and unexplained slowdowns from the user perspective because the platform protects itself before it becomes overloaded.<br><br>- keeps more interactions within acceptable response time ranges which directly improves satisfaction and reduces frustration.<br><br>- shows users that the provider is actively managing capacity and communicating transparently rather than allowing the system to silently degrade.<br><br>- creates predictable behaviour during peak load which makes it easier for business teams to plan their own campaigns and activities.<br><br> | **Tickets avoided √ó handling cost** tied to periods when bars exceeded {target_hot_guard:.0f}%. | Peaks (e.g., {peaks_text}) map to user-visible lag windows. |
| Priority lanes for critical journeys | **Phase 1 ‚Äì Reserve:** Identify the most important user journeys such as checkout, claims, or VIP access and reserve a portion of capacity specifically for these flows even when the overall system is busy.<br><br>**Phase 2 ‚Äì Enforce:** Implement quality of service rules, routing, or rate limiting so non critical workloads do not starve these critical journeys during load spikes.<br><br>**Phase 3 ‚Äì Review:** Periodically review performance and satisfaction metrics for these journeys to ensure that the reserved capacity is actually protecting them as intended.<br><br> | - keeps the most valuable transactions running smoothly when the platform is under stress which directly protects revenue and key relationships.<br><br>- ensures that important customers and business processes experience fewer errors and slowdowns compared to low priority workloads.<br><br>- allows the organisation to absorb traffic spikes more gracefully since not all traffic is treated equally when resources are tight.<br><br>- strengthens trust with high value users who see that their use cases are clearly prioritized in design and operations.<br><br> | **Churn avoided √ó ACV** with inputs from hot-bar services. | Highest bars (CPU, Disk, Network) pinpoint where user pain concentrates. |
| Guided maintenance windows | **Phase 1 ‚Äì Pick:** Use the utilization charts to choose change windows that align with naturally lower bars so maintenance work runs when there is more spare capacity and fewer active users.<br><br>**Phase 2 ‚Äì Communicate:** Inform users and stakeholders early about the chosen windows and set expectations on what impact, if any, they might experience during the maintenance period.<br><br>**Phase 3 ‚Äì Verify:** After each maintenance event, validate that user experience and service levels remained acceptable and that the window selection logic is still valid.<br><br> | - reduces the number of users who feel the impact of planned work because it happens when fewer people depend on the system.<br><br>- makes maintenance more predictable and less stressful for both users and operations because the risk of overload during change is lower.<br><br>- helps reduce the volume of complaints and confusion related to planned downtime since communication is proactive and aligned with actual utilization patterns.<br><br>- supports smoother rollouts which reduces the chance that changes will be rolled back purely due to user pressure rather than technical reasons.<br><br> | **Peak minutes avoided √ó RM/min** referencing before and after bar heights. | Low-bar services and time windows are clearly visible in the grouped view: {lows_text}. |
| Internal performance dashboards | **Phase 1 ‚Äì Publish:** Provide internal teams with a simple dashboard that shows the same grouped bar view of CPU, memory, disk, and network utilization so they can see how their services behave over time.<br><br>**Phase 2 ‚Äì Drill:** Enable drill down for outlier services so owners can investigate why their patterns differ from the rest and take targeted actions to improve them.<br><br>**Phase 3 ‚Äì Act:** Use this dashboard in regular service reviews to agree on specific SLO improvement or capacity actions and then track follow through until bar patterns improve.<br><br> | - increases transparency across engineering, operations, and leadership by giving everyone a common view of resource health.<br><br>- speeds up decision making because teams no longer need to manually gather and reconcile separate reports before agreeing on priorities.<br><br>- encourages a culture of ownership where service teams take responsibility for the shape of their bars and work to improve them over time.<br><br>- reduces the need for reactive crisis discussions because potential issues are visible earlier and can be addressed proactively.<br><br> | **Time saved in war rooms**; measurable when hot bars reduce over successive weeks. | The grouped bar view is trackable and comparable week on week. |
| Post-change UX validation | **Phase 1 ‚Äì Define:** Agree on p95 and p99 latency and error thresholds that represent acceptable user experience for each key service and document them clearly.<br><br>**Phase 2 ‚Äì Test:** After significant changes or capacity adjustments, use synthetic tests and real user monitoring to check whether these thresholds are still met under realistic load conditions.<br><br>**Phase 3 ‚Äì Rollback or tune:** If user experience degrades beyond agreed thresholds, either roll back the change or tune the configuration until the bars and UX metrics return to acceptable levels.<br><br> | - ensures that technical tuning and capacity changes translate into real improvements or at least do not harm user experience in unexpected ways.<br><br>- provides early detection when a change that looks efficient at the infrastructure level actually makes the product feel slower or less reliable to users.<br><br>- builds confidence in the change process for both users and stakeholders because there is a clear safety net focused on user experience, not just system metrics.<br><br>- reduces the risk of deploying capacity optimizations that save money but secretly damage satisfaction, which can be more expensive in the long run.<br><br> | **Complaint recurrence decrease** aligned to services where bars fell and UX metrics improved. | Before and after movements in the grouped bars validate that UX gains are connected to the underlying resource changes. |
"""
            }

            render_cio_tables("CIO ‚Äì Resource Utilization Overview", cio_a)

    # --------------------------------------------------------
    # 10b. Utilization vs Incidents (Correlation Scatter)
    # --------------------------------------------------------
    with st.expander("üìå Utilization vs Incident Volume (Impact Relationship)"):
        need = ["service_name", "cpu_utilization", "incident_count", "downtime_minutes"]
        if not set(need).issubset(df.columns):
            st.warning(f"‚ö†Ô∏è Missing required columns: {set(need) - set(df.columns)}")
        else:
            work = df.copy()
            work["cpu_utilization"]  = _to_num(work["cpu_utilization"])
            work["incident_count"]   = _to_num(work["incident_count"])
            work["downtime_minutes"] = _to_num(work["downtime_minutes"])

            df_corr = work.groupby("service_name", as_index=False).agg({
                "cpu_utilization": "mean",
                "incident_count": "sum",
                "downtime_minutes": "sum"
            })

            fig2 = px.scatter(
                df_corr,
                x="cpu_utilization",
                y="incident_count",
                size="downtime_minutes",
                hover_name="service_name",
                title="CPU Utilization vs Incident Volume (Bubble = Downtime)",
                labels={"cpu_utilization": "CPU Utilization (%)", "incident_count": "Incident Count"},
                color_discrete_sequence=[PRIMARY_BLUE]
            )
            st.plotly_chart(fig2, use_container_width=True)

            # Analysis evidence
            max_inc = df_corr.loc[df_corr["incident_count"].idxmax()]
            min_inc = df_corr.loc[df_corr["incident_count"].idxmin()]
            max_bubble = df_corr.loc[df_corr["downtime_minutes"].idxmax()]
            avg_cpu = df_corr["cpu_utilization"].mean()
            avg_inc = df_corr["incident_count"].mean()
            # Pearson correlation (defensive)
            corr = float(np.corrcoef(df_corr["cpu_utilization"], df_corr["incident_count"])[0,1]) if len(df_corr) > 1 else np.nan

            st.markdown("### Analysis ‚Äì CPU Utilization vs Incidents")
            st.write(
f"""**What this graph is:** A **scatter plot** relating **CPU utilization** (x) to **incident volume** (y), with **bubble size** representing **total downtime minutes**. 
**X-axis:** Average CPU utilization (%). 
**Y-axis:** Incident count. 
**Bubble size:** Downtime minutes.

**What it shows in your data:** 
**Highest incident load:** {max_inc['service_name']} ‚Äî {int(max_inc['incident_count'])} incidents at {max_inc['cpu_utilization']:.1f}% CPU. 
**Largest downtime bubble:** {max_bubble['service_name']} ‚Äî {int(max_bubble['downtime_minutes'])} minutes. 
**Portfolio averages:** {avg_cpu:.1f}% CPU and {avg_inc:.1f} incidents per service. 
**Correlation (CPU vs incidents):** {('+' if corr >= 0 else '')}{corr:.2f} ({'positive' if corr>=0 else 'negative'} relationship).

**How to read it operationally:** 
**Hotspot quadrant:** Upper-right points (high CPU, many incidents) need headroom and code/DB optimization. 
**Large bubbles:** Regardless of CPU, big bubbles flag long restores‚Äîtighten runbooks and rollback. 
**Low-CPU, high incidents:** Likely software/process defects instead of capacity limits.

**Why this matters:** Incidents rise non-linearly as saturation approaches. Fixing hotspots and shortening restores cuts **failures and downtime minutes**, protecting **SLA and cost**."""
            )

            cio_b = {
                "cost": f"""
| Recommendation | Explanation (Phased, Detailed) | Benefits (Expanded) | Cost Calculation (Uses Real Chart Values) | Evidence & Graph Interpretation (Real Values) |
|---|---|---|---|---|
| Add headroom to hotspot services | **Phase 1 ‚Äì Identify:** Use the scatter plot to highlight services that sit in the upper right area with CPU utilization above the portfolio average of {avg_cpu:.1f} percent and incident counts above the average of {avg_inc:.1f} incidents, and list them out clearly for stakeholders.<br><br>**Phase 2 ‚Äì Scale or optimize:** For those hotspot services, either add targeted capacity or implement performance optimizations such as query tuning and caching to reduce stress and bring incidents down.<br><br>**Phase 3 ‚Äì Reassess:** After these actions, review the scatter again to confirm that the points for these services have moved closer to the centre or lower left, showing fewer incidents and safer utilization.<br><br> | - reduces the probability of expensive incidents on the services that are currently under the highest operational stress and generate the most risk.<br><br>- protects revenue generating or customer facing journeys that are often represented by these hotspot services on the chart.<br><br>- makes the incident pattern more balanced so that a few services no longer dominate operational workload and cost.<br><br>- gives leadership a clear narrative that investments are targeted at the riskiest parts of the estate instead of spread thinly everywhere.<br><br> | **Downtime avoided (minutes)** for hotspots √ó **RM/min**; anchor with current bubbles such as **{max_bubble['service_name']} {int(max_bubble['downtime_minutes'])} minutes**. | Hotspot example: **{max_inc['service_name']}** with **{int(max_inc['incident_count'])} incidents** at **{max_inc['cpu_utilization']:.1f}%** CPU. |
| Optimize workloads before hardware | **Phase 1 ‚Äì Profile:** For services with high incident counts even at moderate CPU usage, profile the application to find inefficient code paths, repeated calls, or poor data access patterns that are driving incidents without a clear capacity limit.<br><br>**Phase 2 ‚Äì Patch:** Implement code fixes, better caching, or architectural tweaks that remove these inefficiencies while coordinating with testing and release teams to deploy safely.<br><br>**Phase 3 ‚Äì Validate:** Check whether incident counts for these services drop in subsequent reporting periods while CPU utilization stays in a similar range, showing that stability improved without adding hardware.<br><br> | - avoids premature or unnecessary hardware expansion by fixing inefficiencies that are purely within the application or configuration layer.<br><br>- improves stability at lower ongoing cost because issues are removed at the source instead of being masked by more capacity.<br><br>- creates cleaner and more predictable performance because erratic behaviour from inefficient code is eliminated.<br><br>- increases confidence that future capacity investments are made only after software level opportunities have been exhausted.<br><br> | **Cost avoided = Planned hardware cost ‚àí Post optimization hardware cost**, plus **Œîincidents √ó average downtime minutes**. | Services with many incidents at moderate CPU indicate inefficiency and are visible as points high on the Y axis but not far on the X axis. |
| Queue and rate-limit bursts | **Phase 1 ‚Äì Identify:** Find services on the scatter that show high incident counts during known burst periods and understand which user actions or integrations cause these peaks.<br><br>**Phase 2 ‚Äì Apply:** Introduce robust queues, rate limiting, and back pressure so that surges are smoothed and downstream components are not hammered beyond their safe limits.<br><br>**Phase 3 ‚Äì Monitor:** Track how incident counts and downtime bubbles change for these services in later periods to confirm that burst related failures have decreased.<br><br> | - reduces incident frequency and duration during high traffic events because the system absorbs spikes more gracefully instead of failing abruptly.<br><br>- keeps core services available for more users even when demand temporarily exceeds normal levels.<br><br>- reduces operational firefighting during predictable bursts such as campaigns or month end processing.<br><br>- gives the business more freedom to run promotions or drive traffic without fear that the platform will collapse under pressure.<br><br> | **Incidents avoided √ó average downtime minutes** using current average of {avg_inc:.1f} incidents per service and the observed bubble sizes. | Upper right clustering and large bubbles expose burst sensitivity on specific services. |
| Separate noisy neighbors | **Phase 1 ‚Äì Detect:** Examine services whose incidents and downtime seem to spike when other co located workloads are busy which suggests noisy neighbor effects on shared infrastructure.<br><br>**Phase 2 ‚Äì Isolate:** Move critical or sensitive services onto dedicated or better isolated resource pools so that they are not impacted by unrelated workloads on the same host or cluster.<br><br>**Phase 3 ‚Äì Verify:** After isolation, watch the scatter and incident records for these services to confirm that their points move downwards in incident count even if CPU usage stays similar.<br><br> | - reduces random and hard to diagnose incidents that occur only when other unrelated workloads become busy on the same platform.<br><br>- improves stability for key services without needing to redesign their application code, simply by placing them on cleaner capacity.<br><br>- reduces time spent in war rooms chasing cross service interference that is difficult to reproduce in lower environments.<br><br>- gives more predictable quality of service for high value applications that cannot tolerate unexplained performance swings.<br><br> | **Outage minutes avoided √ó RM/min**; compare bubble minutes before and after isolation for affected services. | Large bubbles at mixed CPU levels suggest contention beyond raw CPU, which is consistent with noisy neighbor behaviour. |
| Right-size recovery SLAs for hotspots | **Phase 1 ‚Äì Allocate:** Use the scatter to determine which services contribute most to incident volume and downtime and assign stronger on call coverage, faster response expectations, and more senior owners to those services.<br><br>**Phase 2 ‚Äì Playbooks:** Develop focused runbooks and rollback procedures specifically for these hotspots so that teams can act quickly when issues occur.<br><br>**Phase 3 ‚Äì Review:** Regularly review recovery performance metrics and adjust SLAs and support depth as hotspots move or as improvements reduce their risk profile.<br><br> | - ensures that the services with the greatest operational and business impact do not wait in the same queue as minor components during an incident.<br><br>- speeds up recovery for the small set of services that drive a disproportionate share of downtime cost and user pain.<br><br>- clarifies priorities for on call staff so they know where to focus their energy when multiple incidents compete for attention.<br><br>- supports stronger SLA compliance because the most impactful services receive the most capable support and the fastest responses.<br><br> | **ŒîMTTR √ó hotspot incidents** using per service incident counts from the scatter plot. | Hotspot services are explicit in the scatter, for example **{max_inc['service_name']}** with high incidents and meaningful CPU load. |
""",
                "performance": f"""
| Recommendation | Explanation (Phased, Detailed) | Benefits (Expanded) | Cost Calculation (Uses Real Chart Values) | Evidence & Graph Interpretation (Real Values) |
|---|---|---|---|---|
| Stress-test to find failure knee | **Phase 1 ‚Äì Test:** Run controlled load tests on representative services to find the utilization level at which incidents or errors start increasing sharply and document this as the failure knee for each service.<br><br>**Phase 2 ‚Äì Guard:** Set operational policies and autoscaling thresholds that keep real production usage comfortably below this knee so that normal variance does not push the system into an unstable zone.<br><br>**Phase 3 ‚Äì Track:** Monitor how often real traffic approaches this knee and use SLO alarms to trigger action before the system crosses into the risky region.<br><br> | - prevents the platform from repeatedly entering a range where a small increase in load causes a large increase in incidents and downtime.<br><br>- gives teams a concrete and evidence based utilization target rather than a generic rule of thumb, which improves decision quality.<br><br>- helps shape capacity planning around real system behaviour so investments are sized to stay below the danger point.<br><br>- reduces the number of crisis events driven by unexpected non linear behaviour under load, which stabilizes operations.<br><br> | **SLA uplift** from fewer incidents near the measured knee which is consistent with the observed correlation of {('+' if corr>=0 else '')}{corr:.2f} between CPU and incidents. | Rising incident trend beyond around eighty to eighty five percent CPU is visible as more points moving into the upper right quadrant. |
| Load distribution and autoscaling | **Phase 1 ‚Äì Rebalance:** Use the scatter and supporting metrics to identify services that carry many incidents relative to peers and check whether load distribution across nodes is uneven or insufficient.<br><br>**Phase 2 ‚Äì Autoscale:** Combine better load balancing with autoscaling rules so that capacity increases when demand rises for these services instead of forcing a few nodes to absorb the entire load.<br><br>**Phase 3 ‚Äì Validate:** After these changes, evaluate incident counts and saturation levels to confirm that incidents per unit of traffic have gone down for these services.<br><br> | - improves system throughput because the same traffic is spread more fairly across capacity while also being backed by dynamic scaling.<br><br>- reduces the number of incidents triggered by short term traffic spikes because the system has more flexibility to absorb them.<br><br>- improves user experience during busy times which is often when the business cares most about reliability and performance.<br><br>- gives operations staff a simpler narrative that combines load distribution and scaling into a single, visible control mechanism.<br><br> | **Ticket drop √ó handle time** using pre and post incident counts and average handling duration based on a baseline of {avg_inc:.1f} incidents per service. | The scatter shows how a small group of services dominate the incident axis, which are prime candidates for better load distribution and autoscaling. |
| Faster rollback for large bubbles | **Phase 1 ‚Äì Stage:** For services with the largest downtime bubbles, prepare clean rollback images, database snapshots, or configuration bundles that allow quick reversion when a change goes wrong.<br><br>**Phase 2 ‚Äì Timebox:** Define clear limits for how long teams will attempt to push a risky fix forward before aborting and rolling back to a known good state.<br><br>**Phase 3 ‚Äì Execute:** Ensure responders are trained to follow the timebox rule and to choose rollback decisively rather than letting downtime continue to grow without progress.<br><br> | - caps the length of outages that would otherwise grow into long and painful incidents for services with historically large downtime bubbles.<br><br>- turns some complex recovery efforts into simpler restore operations, which can be executed more quickly and reliably.<br><br>- reduces the amount of time users spend unable to use critical services when a change behaves badly in production.<br><br>- encourages safer experimentation because there is a known and tested way back to stability when something goes wrong.<br><br> | **ŒîDowntime minutes from bubble size √ó RM/min** using a baseline such as **{max_bubble['service_name']} {int(max_bubble['downtime_minutes'])} minutes**. | The largest bubbles on the scatter clearly identify which services suffer the longest restores and therefore benefit most from fast rollback. |
| Circuit breakers and timeouts | **Phase 1 ‚Äì Fail fast:** Implement timeouts and circuit breakers on calls to unstable or slow dependencies so that the calling service fails quickly and predictably instead of hanging or cascading the problem onward.<br><br>**Phase 2 ‚Äì Degrade gracefully:** Design degraded modes that can keep partial functionality available when dependencies are unhealthy so users can still complete some essential tasks.<br><br>**Phase 3 ‚Äì Heal:** Configure automatic recovery of circuits when upstream services become healthy again and monitor that the system returns cleanly to normal behaviour.<br><br> | - prevents widespread failures across multiple services when a single dependency becomes slow or unavailable which reduces overall incident count.<br><br>- improves user experience because even during outages some functions remain available rather than everything failing at once.<br><br>- shortens recovery time since systems recover automatically as dependencies heal rather than requiring manual restarts everywhere.<br><br>- encourages better architecture design where failure is treated as an expected condition rather than as a rare exception.<br><br> | **ŒîError budget burn** and **incidents avoided** relative to current incident counts shown in the scatter. | Upper right points and big bubbles reflect scenarios where lack of isolation between services leads to large failures under pressure. |
| Golden signals dashboard | **Phase 1 ‚Äì Instrument:** Ensure that every key service exposes standardized metrics for latency, traffic, errors, and saturation and that these are collected and viewable in one consistent dashboard.<br><br>**Phase 2 ‚Äì Gate:** Use these metrics as gates for rollouts and other risky operations so that actions only proceed when golden signals are within healthy limits.<br><br>**Phase 3 ‚Äì Review:** Conduct weekly or regular reviews of golden signal trends to prioritize reliability work before issues become full incidents.<br><br> | - reduces blind spots by giving teams a shared and consistent view of their most important health indicators.<br><br>- helps prevent bad deployments or risky operations from being executed when the system is already fragile, which reduces the chance of new incidents.<br><br>- makes identifying root causes faster because teams can see which signals deviated first when an incident began.<br><br>- supports a more systematic and data driven reliability culture instead of relying solely on ad hoc experience or intuition.<br><br> | **Breaches avoided** compared to periods where changes were made without enforcing golden signal gates. | The relationship between bubble size, incidents, and CPU utilization provides concrete evidence of how saturation and errors interact, which the golden signals dashboard can make visible earlier. |
""",
                "satisfaction": f"""
| Recommendation | Explanation (Phased, Detailed) | Benefits (Expanded) | Cost Calculation (Uses Real Chart Values) | Evidence & Graph Interpretation (Real Values) |
|---|---|---|---|---|
| Proactive comms during hotspot hours | **Phase 1 ‚Äì Notify:** Use knowledge of hotspot services and peak periods from the scatter to proactively inform users when the platform is operating near known risk thresholds and explain which functions might be slower.<br><br>**Phase 2 ‚Äì Provide:** Offer clear workarounds, alternative flows, or suggested off peak times to complete non urgent tasks so users can plan their work better.<br><br>**Phase 3 ‚Äì Close:** After the high risk window passes, communicate what happened, how the platform behaved, and what will be improved for the next similar period.<br><br> | - reduces surprise and frustration because users understand that high load periods are known and monitored rather than being random failures.<br><br>- reduces the volume of support contacts and escalations because many questions are answered by proactive communication instead of reactive tickets.<br><br>- builds trust since users see that the organisation is transparent about limitations and is actively managing them.<br><br>- helps users adjust their own behaviour so they can avoid peak times for non critical actions which further reduces stress on the system.<br><br> | **Complaints avoided √ó handling cost** correlated to hotspot points and known busy periods. | Hotspots and large bubbles on the scatter map directly to the times and services where users feel the most pain. |
| Prioritize UX for impacted services | **Phase 1 ‚Äì Identify:** Use the scatter to select services with both high incident counts and significant downtime and treat them as UX critical from a customer standpoint.<br><br>**Phase 2 ‚Äì Guard:** Apply priority lanes, strict SLOs, and additional monitoring for these services so user experience is protected even when the wider platform is under load.<br><br>**Phase 3 ‚Äì Measure:** Track user satisfaction, churn indicators, and complaint patterns for these services to confirm that the extra investment is delivering visible improvements.<br><br> | - concentrates UX improvements on the services that directly impact retention and revenue instead of spreading effort thinly across all components.<br><br>- improves the lived experience of customers using those services because they become more stable and responsive over time.<br><br>- provides a clear story to leadership about why certain services receive more attention and investment from the reliability and UX teams.<br><br>- allows the organisation to protect its most important digital journeys even during periods when the rest of the system is less stable.<br><br> | **Retention uplift √ó user base** of hotspot services identified on the scatter, including **{max_inc['service_name']}** with **{int(max_inc['incident_count'])} incidents**. | {max_inc['service_name']} leads incident load and is an obvious candidate for focused UX protection. |
| Capacity banner on status page | **Phase 1 ‚Äì Expose:** Show a simple capacity or headroom indicator on the status page for key services so users can see when the system is operating close to its limits.<br><br>**Phase 2 ‚Äì Educate:** Explain what the indicator means and how users can adjust their behaviour, such as by avoiding heavy operations during very high load windows when possible.<br><br>**Phase 3 ‚Äì Retire:** Remove any warnings once capacity is increased or load drops back into normal range and update the page with a short note on what changed.<br><br> | - gives users a real time view of platform health which makes performance variations easier to understand and accept.<br><br>- reduces repeat queries and speculation about whether the system is working since the capacity posture is clearly visible.<br><br>- signals operational maturity because the organisation is willing to share informative metrics rather than hide behind generic ‚Äúservice unavailable‚Äù messages.<br><br>- helps align expectations between technical teams and users by visualizing the same constraints both groups must work within.<br><br> | **Repeat contacts decrease √ó cost per ticket** during periods when capacity banners are visible compared to similar periods without transparency. | The scatter offers a strong narrative about capacity posture which can be simplified into the user facing status indicator. |
| Post-incident user brief | **Phase 1 ‚Äì Explain:** After significant incidents, share a user facing explanation that summarises in plain language what caused the issue, how it was fixed, and how long recovery took compared to targets.<br><br>**Phase 2 ‚Äì Timeline:** Include a simple timeline that shows when the issue started, when key actions were taken, and when full service was restored so users have a factual reference.<br><br>**Phase 3 ‚Äì Next steps:** Describe practical prevention steps or capacity changes that will be made to reduce the chance of the same problem happening again and indicate expected timelines.<br><br> | - helps rebuild trust by showing that the organisation understands the problem and is taking specific steps to prevent recurrence.<br><br>- reduces rumours and conflicting stories about what happened because there is a clear and official account of the incident.<br><br>- provides useful material for internal stakeholders such as account managers to use when communicating with customers.<br><br>- makes it easier to learn from incidents because user facing communication becomes part of the standard incident closure process.<br><br> | **Churn avoided** on services with the biggest downtime bubbles where communication quality strongly influences customer decisions. | The largest bubble, **{max_bubble['service_name']} {int(max_bubble['downtime_minutes'])} minutes**, highlights the type of event that benefits most from good user briefs. |
| Targeted help-center guides | **Phase 1 ‚Äì Document:** Create help center articles that address the most common symptoms seen on hotspot services during high utilization periods and write them in simple, user friendly language.<br><br>**Phase 2 ‚Äì Surface:** Make these guides easy to find through search, chatbots, and support agent tools so that users quickly land on the relevant information when issues arise.<br><br>**Phase 3 ‚Äì Update:** Regularly update the guides based on new incident patterns and feedback from users and support teams to keep them accurate and helpful.<br><br> | - enables many users to self diagnose and resolve minor issues without opening support tickets which saves time for both users and agents.<br><br>- reduces frustration because users can get immediate guidance when they experience slowdowns or transient errors in hotspot services.<br><br>- improves consistency of advice from support teams because they can point to a single, standard explanation rather than improvising.<br><br>- scales support capacity more effectively because well written guides can serve many users at once without additional staffing.<br><br> | **Deflected tickets √ó cost per ticket** during high load phases where users would otherwise contact support for common symptoms. | Recurrent patterns visible in the scatter distribution indicate which services and behaviours should be prioritised in help center content. |
"""
            }

            render_cio_tables("CIO ‚Äì Utilization vs Incidents", cio_b)

    # --------------------------------------------------------
    # 10c. Capacity Status Distribution
    # --------------------------------------------------------
    with st.expander("üìå Capacity Planning and Scalability Status"):
        need = ["service_name", "capacity_status"]
        if not set(need).issubset(df.columns):
            st.warning(f"‚ö†Ô∏è Missing required columns: {set(need) - set(df.columns)}")
        else:
            work = df.copy()
            work["capacity_status"] = work["capacity_status"].astype(str)

            cap = work.groupby(["service_name", "capacity_status"], as_index=False).size()

            fig3 = px.bar(
                cap,
                x="service_name",
                y="size",
                color="capacity_status",
                title="Capacity Status by Service",
                labels={"size": "Record Count", "capacity_status": "Capacity Status"},
                color_discrete_sequence=MES_COLOR_SEQ
            )
            fig3.update_layout(xaxis_tickangle=-15)
            st.plotly_chart(fig3, use_container_width=True)

            # Evidence numbers
            at_risk  = cap[cap["capacity_status"].str.lower() == "at risk"].sort_values("size", ascending=False)
            overutil = cap[cap["capacity_status"].str.lower() == "overutilized"].sort_values("size", ascending=False)
            stable   = cap[cap["capacity_status"].str.lower() == "stable"].sort_values("size", ascending=False)

            risk_name  = at_risk.iloc[0]["service_name"] if not at_risk.empty else "N/A"
            risk_count = int(at_risk.iloc[0]["size"])    if not at_risk.empty else 0
            over_name  = overutil.iloc[0]["service_name"] if not overutil.empty else "N/A"
            over_count = int(overutil.iloc[0]["size"])     if not overutil.empty else 0
            stable_name  = stable.iloc[0]["service_name"] if not stable.empty else "N/A"
            stable_count = int(stable.iloc[0]["size"])    if not stable.empty else 0

            total_records = int(cap["size"].sum())
            share_over = (cap[cap["capacity_status"].str.lower()=="overutilized"]["size"].sum() / total_records * 100) if total_records else 0
            share_risk = (cap[cap["capacity_status"].str.lower()=="at risk"]["size"].sum() / total_records * 100) if total_records else 0
            share_stable = (cap[cap["capacity_status"].str.lower()=="stable"]["size"].sum() / total_records * 100) if total_records else 0

            st.markdown("### Analysis ‚Äì Capacity Status")
            st.write(
f"""**What this graph is:** A **stacked/grouped bar chart** showing how often each service is labeled **Stable**, **At Risk**, or **Overutilized**. 
**X-axis:** Service name. 
**Y-axis:** Record count (how many times a status occurred). 
**Color:** Capacity status category.

**What it shows in your data:** 
**Most ‚ÄúAt Risk‚Äù service:** {risk_name} ({risk_count} records). 
**Most ‚ÄúOverutilized‚Äù service:** {over_name} ({over_count} records). 
**Portfolio mix:** Overutilized {share_over:.1f}%, At Risk {share_risk:.1f}%, Stable {share_stable:.1f}% of total {total_records} records.

**How to read it operationally:** 
**Expand where it‚Äôs real:** Many ‚ÄúOverutilized‚Äù counts ‚Üí scale or shed load. 
**Pre-empt risk:** Frequent ‚ÄúAt Risk‚Äù ‚Üí predictive scaling and pre-fixes. 
**Trim safely:** ‚ÄúStable‚Äù dominance ‚Üí rightsizing opportunities without jeopardizing SLOs.

**Why this matters:** Clear status patterns let you **invest precisely**‚Äîexpand where risk is high and trim where headroom is idle‚Äîimproving **cost, uptime, and user experience**."""
            )

            cio_c = {
                "cost": f"""
| Recommendation | Explanation (Phased, Detailed) | Benefits (Expanded) | Cost Calculation (Uses Real Chart Values) | Evidence & Graph Interpretation (Real Values) |
|---|---|---|---|---|
| Automate scale-down for stable services | **Phase 1 ‚Äì Detect:** Use the capacity status chart to identify services that are labeled as ‚ÄúStable‚Äù most of the time and confirm that they are not approaching capacity limits during known busy periods.<br><br>**Phase 2 ‚Äì Rightsize:** Reduce instance sizes or node counts for these stable services or move them to more cost effective infrastructure while keeping resilience requirements in mind.<br><br>**Phase 3 ‚Äì Verify:** Monitor service levels and status tags after the change to confirm that the services remain mostly in the ‚ÄúStable‚Äù state and that there is no hidden risk introduced by the cost reduction.<br><br> | - decreases ongoing infrastructure, power, and licensing costs by trimming capacity where the chart shows consistent stability and low risk.<br><br>- reduces waste because capacity that is rarely stressed is no longer overprovisioned for unlikely scenarios.<br><br>- demonstrates disciplined stewardship of resources which is important for budgeting and sustainability discussions.<br><br>- makes capacity planning clearer because the environment is less padded with unnecessary headroom that hides the true demand patterns.<br><br> | **Savings = Nodes removed √ó RM per node per month**; prioritize where ‚ÄúStable‚Äù counts are highest such as **{stable_name} {stable_count} records**. | Stable mix is {share_stable:.1f}% of the portfolio; the top stable service is **{stable_name}**, indicating strong opportunities for safe reduction. |
| Target upgrades only for ‚ÄúAt Risk‚Äù | **Phase 1 ‚Äì Focus:** Isolate services with high counts of ‚ÄúAt Risk‚Äù status and confirm that these readings are not just momentary glitches but recurring warnings across multiple periods.<br><br>**Phase 2 ‚Äì Action:** Decide whether to add capacity, optimize workloads, or reduce noisy neighbour interference for these services so they move from ‚ÄúAt Risk‚Äù toward ‚ÄúStable‚Äù.<br><br>**Phase 3 ‚Äì Measure:** Track how the status mix changes after interventions to confirm that risk is shrinking for the targeted services and not moving elsewhere unnoticed.<br><br> | - avoids expensive upgrades for services that are already stable and channels investment where capacity risk is genuinely high.<br><br>- reduces the likelihood that ‚ÄúAt Risk‚Äù services will degrade into ‚ÄúOverutilized‚Äù or outage situations which would be more costly to fix later.<br><br>- provides a clear explanation to finance and leadership about why certain services are receiving upgrades while others are not.<br><br>- helps build a feedback loop where each cycle of upgrades produces visible improvements in the status chart which reinforces confidence in the approach.<br><br> | **Return on investment = Avoided incident loss √∑ Upgrade cost** using per service ‚ÄúAt Risk‚Äù counts such as **{risk_name} {risk_count}**. | Repeated ‚ÄúAt Risk‚Äù tags on **{risk_name}** clearly show a pre failure posture that justifies targeted investment. |
| Spot-buy vs reserved mix | **Phase 1 ‚Äì Classify:** Use capacity status patterns to classify services as bursty or steady by examining how frequently they move between ‚ÄúStable‚Äù, ‚ÄúAt Risk‚Äù, and ‚ÄúOverutilized‚Äù.<br><br>**Phase 2 ‚Äì Buy:** For bursty services, favour spot or flexible capacity, while for steady services, choose reserved or committed capacity agreements to reduce unit costs.<br><br>**Phase 3 ‚Äì Review:** On a quarterly basis, revisit these classifications and adjust purchasing strategies if usage or status patterns change significantly.<br><br> | - reduces overall compute costs by matching the commercial model to how each service actually behaves instead of treating all services the same.<br><br>- improves predictability of spending because steady services are backed by cheaper long term commitments while spike prone services use flexible capacity that can be released when not needed.<br><br>- provides a stronger narrative to procurement and finance teams about why different buying strategies are used for different workloads.<br><br>- encourages better alignment between technical planning and commercial planning which reduces the risk of over commitment or under utilisation of reserved capacity.<br><br> | **Change in unit price √ó usage hours** parameterized by the status frequencies where Stable and Overutilized shares are {share_stable:.1f}% and {share_over:.1f}% respectively. | The distribution of statuses across services highlights which workloads are steady and which are volatile, guiding smarter purchasing. |
| Consolidate non-peak workloads | **Phase 1 ‚Äì Pack:** Identify services that frequently show ‚ÄúStable‚Äù status and are not business critical during peak times and plan to consolidate them onto shared nodes or clusters.<br><br>**Phase 2 ‚Äì Retire:** After successful consolidation, retire or repurpose the freed nodes so they no longer incur direct cost or operational overhead.<br><br>**Phase 3 ‚Äì SLO check:** Confirm through monitoring and user feedback that performance and availability for consolidated workloads remain within acceptable ranges.<br><br> | - reduces infrastructure footprint and costs by ensuring that lightly used workloads share capacity instead of owning entire nodes.<br><br>- simplifies the environment because there are fewer hosts to manage, patch, and secure which saves operational effort.<br><br>- frees up resources that can be applied to more demanding or strategic services that are capacity constrained.<br><br>- supports greener operations by reducing energy consumption associated with running underused hardware.<br><br> | **Nodes removed √ó RM per node** with candidate services selected based on high Stable counts and low business criticality. | Multiple services with Stable dominance in the chart support consolidation decisions with minimal risk. |
| Archive cold logs/data | **Phase 1 ‚Äì Identify:** Look for services that remain Stable even when holding very large log or data volumes which suggests that much of the data is cold and seldom accessed.<br><br>**Phase 2 ‚Äì Move:** Transition older or rarely accessed data for these services to cheaper archival storage while ensuring regulatory and retention requirements are still met.<br><br>**Phase 3 ‚Äì Audit:** Periodically audit the archived data usage to confirm that it is still rarely touched and to move any newly cold data from primary storage to archival tiers.<br><br> | - reduces storage costs by putting long lived but rarely accessed data on lower cost storage options without affecting day to day operations.<br><br>- keeps primary storage lean and responsive which helps maintain Stable status even as data grows over time.<br><br>- simplifies backup and restore strategies because primary datasets become smaller and more focused on active data.<br><br>- improves compliance and data hygiene because retention policies are implemented more consciously instead of letting data accumulate everywhere.<br><br> | **Gigabytes moved √ó difference in cost between hot and cold storage** with priority on services that show persistent Stable status. | Stable records suggest that moving older data for those services into cheaper storage can be done with limited risk to performance. |
""",
                "performance": f"""
| Recommendation | Explanation (Phased, Detailed) | Benefits (Expanded) | Cost Calculation (Uses Real Chart Values) | Evidence & Graph Interpretation (Real Values) |
|---|---|---|---|---|
| Predictive scaling for ‚ÄúAt Risk‚Äù | **Phase 1 ‚Äì Forecast:** Use historical load and capacity status trends to forecast when ‚ÄúAt Risk‚Äù services are likely to become ‚ÄúOverutilized‚Äù and document these predictions for planning purposes.<br><br>**Phase 2 ‚Äì Add:** Before those predicted peaks occur, add capacity or adjust limits so that services have more headroom and can stay Stable through the expected demand.<br><br>**Phase 3 ‚Äì Track:** After each forecasted event, check whether the service remained Stable and refine the forecasting approach based on what actually happened.<br><br> | - reduces the number of incidents that occur during predictable growth or seasonal peaks because capacity is added proactively rather than reactively.<br><br>- helps maintain reliable performance for important services at times when the business is most sensitive to disruption.<br><br>- demonstrates strategic capacity planning which can improve confidence from both technical and business stakeholders.<br><br>- limits the need for emergency spending or urgent capacity changes during crises because the work has already been done ahead of time.<br><br> | **Incidents avoided √ó average duration** using ‚ÄúAt Risk‚Äù counts such as **{risk_name} {risk_count}** to estimate where predictive scaling has the largest impact. | Frequent ‚ÄúAt Risk‚Äù tags signal approaching saturation and show where predictive scaling will deliver the most benefit. |
| Hotspot playbooks for ‚ÄúOverutilized‚Äù | **Phase 1 ‚Äì Throttle or shed:** For services frequently labeled as ‚ÄúOverutilized‚Äù, define clear actions to temporarily throttle non critical requests or shed optional workloads to protect core functions.<br><br>**Phase 2 ‚Äì Burst:** When safe and cost effective, add short term burst capacity specifically for these services during peak events to reduce pressure and avoid performance collapse.<br><br>**Phase 3 ‚Äì Recover:** Once load drops, remove extra capacity and return throttling settings to normal, then review the event to refine the playbook for next time.<br><br> | - maintains key service levels even when load temporarily exceeds normal capacity by giving operators a structured response to extreme conditions.<br><br>- reduces the number of prolonged ‚ÄúOverutilized‚Äù periods that degrade user experience or trigger incidents.<br><br>- improves operational confidence because responders know exactly what controls to use when they see repeated ‚ÄúOverutilized‚Äù states.<br><br>- provides a foundation for automating some responses in the future once the playbook has proven effective.<br><br> | **Change in SLO breaches √ó penalty** anchored to ‚ÄúOverutilized‚Äù records such as **{over_name} {over_count}**. | Overutilized bars highlight recurring stress points that warrant dedicated hotspot playbooks. |
| Weekly capacity snapshots to CIO | **Phase 1 ‚Äì Summarize:** Generate a simple weekly snapshot that shows the mix of Stable, At Risk, and Overutilized statuses by service and highlight any changes since the previous week.<br><br>**Phase 2 ‚Äì Decide:** Use this snapshot in leadership or operational reviews to prioritise which capacity or optimization actions should be funded and executed next.<br><br>**Phase 3 ‚Äì Iterate:** Track how the status mix evolves over time and adjust plans when new risks or opportunities emerge in the charts.<br><br> | - gives leaders a concise and repeatable view of capacity health so they can make quicker and better informed decisions.<br><br>- reduces last minute escalations because capacity risks are surfaced earlier and handled in a structured forum.<br><br>- supports alignment between technical teams and executives on where budget should be focused for the greatest stability impact.<br><br>- turns capacity management into a continuous process rather than a once a year budgeting exercise disconnected from reality.<br><br> | **Time saved in war rooms** compared to situations where capacity issues are only discovered during major incidents. | Status bars trend cleanly week over week and provide an easy narrative for CIO level reporting. |
| Dependency mapping | **Phase 1 ‚Äì Trace:** For services that are often At Risk or Overutilized, map their upstream and downstream dependencies to find shared components that may be causing or amplifying stress across multiple services.<br><br>**Phase 2 ‚Äì Fix:** Work with owners of those shared components to improve their performance, add redundancy, or change usage patterns so they no longer create bottlenecks.<br><br>**Phase 3 ‚Äì Validate:** After improvements, check whether capacity statuses for the dependent services move toward Stable more often than before.<br><br> | - reduces the chance that a single weak dependency can push many services into At Risk or Overutilized states at the same time.<br><br>- improves the resilience of the overall system architecture by strengthening critical shared components instead of only tuning edge services.<br><br>- helps prioritise cross team work where it will deliver the most widespread reliability gains.<br><br>- shortens incident resolution for capacity related problems because dependencies and their influence are already documented and understood.<br><br> | **ŒîMTTR √ó incidents** on stacks affected by shared constraints as shown by clusters of At Risk or Overutilized statuses. | Clusters of At Risk and Overutilized tags around certain services suggest shared constraints that need deeper dependency analysis. |
| Synthetic load monitors | **Phase 1 ‚Äì Probe:** Set up synthetic load tests or probes that continuously exercise key paths for services with mixed statuses so you can detect stress and degradation before users are impacted.<br><br>**Phase 2 ‚Äì Alert:** Configure these probes to raise early alerts when performance or success rates fall outside normal ranges even if capacity status has not yet updated.<br><br>**Phase 3 ‚Äì Tune:** Adjust the probe intensity and thresholds based on real world outcomes so that they remain sensitive enough to be useful without creating constant noise.<br><br> | - reveals emerging capacity issues earlier than waiting for real user traffic to generate visible failures.<br><br>- reduces the number of surprise incidents because teams can respond to probe signals before full saturation or overload occurs.<br><br>- provides an extra layer of defence in environments where status tagging alone may not capture subtle degradation patterns.<br><br>- builds confidence that critical user journeys are being continuously validated rather than only checked during scheduled tests.<br><br> | **Breaches avoided** compared to historical incident levels when only reactive monitoring of capacity status was used. | Flip flop patterns in capacity statuses are early warning signals that synthetic probes can help stabilize. |
""",
                "satisfaction": f"""
| Recommendation | Explanation (Phased, Detailed) | Benefits (Expanded) | Cost Calculation (Uses Real Chart Values) | Evidence & Graph Interpretation (Real Values) |
|---|---|---|---|---|
| Communicate scaling roadmaps | **Phase 1 ‚Äì Publish:** Create a simple roadmap that explains which At Risk and Overutilized services will receive capacity and optimization work over the coming months and share it with key stakeholders.<br><br>**Phase 2 ‚Äì Update:** Provide brief regular updates on progress against this roadmap including improvements delivered and any changes in priority based on updated status data.<br><br>**Phase 3 ‚Äì Close:** When specific actions are completed and statuses improve, communicate this clearly so users and leaders see the connection between plans and outcomes.<br><br> | - increases stakeholder confidence because they can see that capacity risks are understood and being addressed in an organised way.<br><br>- reduces ad hoc queries and pressure for unplanned upgrades since there is a visible and evolving plan already in place.<br><br>- creates a more constructive dialogue between technical and business teams about trade offs and timing for capacity improvements.<br><br>- helps everyone see where progress is being made which supports continued investment in capacity and optimization work.<br><br> | **Ticket deflection √ó cost** during high risk periods as more questions are answered by roadmap updates rather than individual requests. | At Risk and Overutilized services such as **{risk_name}** and **{over_name}** are central to the roadmap story and visible in the chart. |
| Guard critical user journeys | **Phase 1 ‚Äì Reserve:** Identify critical user journeys mapped to services with frequent At Risk or Overutilized statuses and reserve dedicated capacity or higher priority scheduling for those journeys.<br><br>**Phase 2 ‚Äì Enforce:** Apply quality of service rules or routing preferences so that these journeys continue to function even when shared infrastructure is under pressure.<br><br>**Phase 3 ‚Äì Review:** Evaluate user experience and churn indicators for these critical journeys to confirm that they remain protected during peak usage or incident conditions.<br><br> | - ensures that business critical workflows remain accessible and responsive even when less important activities may slow down or be paused.<br><br>- protects revenue and key customer relationships by shielding high value interactions from the worst effects of capacity constraints.<br><br>- provides a tangible demonstration to business stakeholders that their most important journeys are being prioritised in technical design and operations.<br><br>- allows for more flexible management of lower priority traffic which can be throttled or delayed without compromising essential services.<br><br> | **Churn avoided √ó annual contract value** for services that show high Overutilized counts and support critical user journeys. | Overutilized bars in the chart map directly to the most fragile journeys that need special protection. |
| Maintenance during stable windows | **Phase 1 ‚Äì Align:** Use the capacity status history to schedule maintenance for each service in time windows where the service is most often Stable and user demand is lower.<br><br>**Phase 2 ‚Äì Notify:** Communicate these planned windows to users and stakeholders in advance and explain why those periods were chosen based on observed stability.<br><br>**Phase 3 ‚Äì Validate:** After maintenance, confirm that the activity did not push services into At Risk or Overutilized status and gather user feedback on perceived impact.<br><br> | - reduces disruption to users because planned work is carried out when the service is already operating with comfortable headroom.<br><br>- lowers the risk that maintenance will accidentally tip a service into performance problems since it is not performed during stressed periods.<br><br>- makes maintenance communications more credible because they are backed by data about when the environment is genuinely quiet.<br><br>- helps keep trust high while still allowing needed upgrades and fixes to go ahead on a regular cadence.<br><br> | **Peak minutes avoided √ó RM per minute** with Stable share of {share_stable:.1f}% indicating the size of safe windows available. | High Stable counts highlight low risk periods that are ideal for planned maintenance without harming user experience. |
| UX monitoring and alerts | **Phase 1 ‚Äì Set:** Define front end or user facing metrics such as page load time, error messages, and transaction failure rates for key services and connect these to the capacity status view.<br><br>**Phase 2 ‚Äì Alert:** Trigger alerts when UX metrics degrade, especially if they coincide with At Risk or Overutilized statuses, so teams can respond quickly with capacity or tuning actions.<br><br>**Phase 3 ‚Äì Learn:** After incidents, review which UX signals were most predictive and adjust thresholds so that future alerts are more accurate and more helpful.<br><br> | - ensures that capacity issues are evaluated not just from the system perspective but also from the user perspective which is what ultimately matters.<br><br>- reduces the time between a user experiencing a slow or failing interaction and the operations team becoming aware and acting on it.<br><br>- improves the quality of incident investigations by linking back end capacity data and front end experience metrics in a single narrative.<br><br>- supports continuous improvement of UX because alerts become a source of structured data about when and how users are impacted by capacity constraints.<br><br> | **Repeat contacts decrease √ó cost per ticket** around status changes because teams react earlier to UX changes tied to capacity shifts. | Status transitions from Stable to At Risk or Overutilized often precede UX complaints, so linking these views helps close the loop quickly. |
| Publish monthly reliability notes | **Phase 1 ‚Äì Summarize:** Build a short monthly note that summarises changes in capacity status mix, major actions taken, and any observed impact on uptime and user experience.<br><br>**Phase 2 ‚Äì Highlight:** Call out significant wins such as services moving from Overutilized to Stable and any new risks that appeared during the month.<br><br>**Phase 3 ‚Äì Plan:** Use these notes to outline the next planned steps for capacity and performance work so stakeholders understand the forward path.<br><br> | - keeps stakeholders continuously informed about reliability and capacity without requiring them to interpret raw charts on their own.<br><br>- reduces anxiety and speculation because there is a regular, authoritative update on how the platform is performing and where it is going next.<br><br>- reinforces accountability since teams must show progress or explain obstacles in a clear and consistent format.<br><br>- makes it easier to secure ongoing support for reliability initiatives because the story is refreshed each month with evidence and outcomes.<br><br> | **Customer satisfaction uplift** and lower complaint recurrence as communication becomes more consistent and data driven. | The capacity status bars provide a visually intuitive basis for the monthly narrative that non technical stakeholders can understand. |
"""
            }
            render_cio_tables("CIO ‚Äì Capacity Planning Recommendations", cio_c)
